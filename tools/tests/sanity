#!/bin/bash
#
# Copyright (c) 2015 EMC Corporation
# All Rights Reserved
#

# ==============================================================
# Uncomment the below line to get tracing output for this script
#set -x

#Configuration file to be used
BASEDIR=$(dirname $0)
CONFIG_FILE=${1}

Usage()
{
    echo 'Usage: sanity <configuration_file> <bourne_ip>|<[bourne_ipv6]> {all | snapvx | isilon | vplex | vplexexport | vplexsnap | vnxblock | ingestblock | vnxblock_flex_varray | netapp | netappc | recoverpoint | xtremio | srdf |
 vnxfile | datadomainfile | vnxfile_flex_varray | vmaxblock | hds | vmaxblock_flex_varray | combined_block | blocksnapshot | blockmirror | full_copy | blockconsistencygroup | init | quick |
 errorhandling | vnxe | security | syssvc | audit | monitor | vdc | catalog | recovery | backuprestore backupserver-uri username password | dr | ipsec | ceph} [local|ldap] [<timeslot>] | sanzoning | application | unity'
    echo 'E.g.:  sanity conf/sanity.conf 137.69.169.21 all'
    echo 'E.g.:  sanity ~/.sanity/sanity.conf 137.69.169.21 isilon'
    echo 'Note: IPv6 needs to be square bracket surrounded. E.g.:  sanity [2620:0:170:2842::180] security'
    echo 'If the env variable DISCOVER_SAN=1 is set: discover and use SAN networking.'
    echo 'E.g.:  sanity conf/sanity.conf 137.69.169.21 audit ldap 2006-03-23T12[:23]'
    exit 2
}

toLower()
{
    echo "$(echo ${1} | tr '[:upper:]' '[:lower:]')"
}

toUpper()
{
    echo "$(echo ${1} | tr '[:lower:]' '[:upper:]')"
}

if [ -f $CONFIG_FILE ]; then
    echo "Using Configuration file $CONFIG_FILE for sanity"
    source ${1}
else
    echo "Configuration file $CONFIG_FILE not found"
    Usage
fi



[ $# -ge 3 ] || Usage
date

init_env_var()
{
    [[ -n "${TEST_APPLIANCE}" ]] && return
    remote_ip="${1}"
    [[ "${remote_ip}" == "localhost" ]] && export TEST_APPLIANCE=no && return 
    local_ip=`ifconfig ${ethdev} 2>/dev/null|awk '/inet addr:/ {print $2}' | sed 's/addr://'`
    [[ "${local_ip}" == "${remote_ip}" ]] && export TEST_APPLIANCE=no && return
    local_fqdn=$(nslookup ${local_ip} | sed -n  's/.*arpa.*name = \(.*\)/\1/p')
    [[ "${local_fqdn%.*}" == "${remote_ip}" ]] && export TEST_APPLIANCE=no && return    
}
init_env_var "${2}"

# Use the environment variable DISCOVER_SAN to determine if 
# SAN Networking should be discovered, thereby allowing zoning to be
# done for the exports.
DISCOVER_SAN=${DISCOVER_SAN:-0}
echo "DISCOVER_SAN: " $DISCOVER_SAN

# By default, we're not running quick sanity
QUICK=${QUICK:-0}

# The token file name will have a suffix which is this shell's PID
# It will allow to run the sanity in parallel
export BOURNE_TOKEN_FILE=${BOURNE_TOKEN_FILE:-"/tmp/token$$.txt"}
BOURNE_SAVED_TOKEN_FILE="/tmp/token_saved.txt"

PATH=$BASEDIR:/bin:/usr/bin:/usr/local/bin
OBJECTORFILENAME="/tmp/testipc.txt"
BOURNE_IPS=${2:-$BOURNE_IPADDR}
HOSTNAME=$BOURNE_IPADDR
IFS=',' read -ra BOURNE_IP_ARRAY <<< "$BOURNE_IPS"
BOURNE_IP=${BOURNE_IP_ARRAY[0]}
IP_INDEX=0

# By default, we're not running quick sanity
QUICK=0

SS=${3}
if [ ${SS} = "vplex" ]; then
    VPLEX_QUICK_PARAM=${4:-'normal'}
    AUTH=${5:-ldap}
    EXTRA_PARAM=${6}
elif [ ${SS} = "recoverpoint" ]; then
    RP_QUICK_PARAM=${4:-'quick'}
    AUTH=${5:-ldap}
    EXTRA_PARAM=${6}
elif [ ${SS} = "xtremio" ]; then
    XIO_QUICK_PARAM=${4:-'quick'}
    AUTH=${5:-ldap}
    EXTRA_PARAM=${6}
elif [ ${SS} = "vdc" ]; then
    VDC_DEFAULT_ENDPOINT=vdc_dummy_endpoint
    VDC_ENDPOINT_B=${4:-$VDC_DEFAULT_ENDPOINT}
    VDC_ENDPOINT_C=${5:-$VDC_DEFAULT_ENDPOINT}
    if [ -n "${BOURNE_IPV6_MODE}" ]; then
	AUTH=ipv6
    else
	AUTH=ldap
    fi
    echo AUTH=$AUTH
elif [ ${SS} = "dr" ]; then
    DR_SITE_B_IP=${4}
    DR_SITE_C_IP=${5}
    if [ -n "${BOURNE_IPV6_MODE}" ]; then
	AUTH=ipv6
    else
	AUTH=ldap
    fi
    echo AUTH=$AUTH
elif [ ${SS} = "backuprestore" ]; then
    if [[ $# -ne 6 ]]; then
	echo "Invalid backuprestore parameters."
	echo "usage: backuprestore backup-server-uri username password"
	exit 1
    fi
    
    BACKUP_SERVER_URL=$4
    BACKUP_SERVER_USERNAME=$5
    BACKUP_SERVER_PASSWORD=$6

    if [ -n "${BOURNE_IPV6_MODE}" ]; then
	AUTH=ipv6
    else
	AUTH=ldap
    fi
    echo AUTH=$AUTH
else
    # By default, we're not running quick sanity
    if [ ${SS} = "quick" ]; then
	QUICK=1
	RP_INGESTTESTS=0
    else
        QUICK=0
    fi
    echo "QUICK: " $QUICK

    AUTH=${4:-ldap}
    EXTRA_PARAM=${5}
fi

[ -z "${AUTH}" ] && AUTH=${4:-ldap}

DO_SETUP_ONLY=0
for a in $* 
do
    if [ "$a"x = "-setup_onlyx" ]; then
        DO_SETUP_ONLY=1
    fi
done

case $SS in
    all|snapvx|isilon|vplex|vplexexport|vplexsnap|netapp|netappc|recoverpoint|srdf|vnxfile|datadomainfile|vnxfile_flex_varray|vnxblock|vnxblock_flex_varray|hds|xtremio|vmaxblock|ingestblock|vmaxblock_flex_varray|combined_block|syssvc|blocksnapshot|blockmirror|full_copy|blockconsistencygroup|security|monitor|audit|ui|init|quick|errorhandling|vdc|vnxe|recovery|backuprestore|sanzoning|dr|ipsec|application|driversystem|ceph|unity)
    ;;

    # These tests are handled by sanity integration tests written with the code
    catalog)
    echo "Running external sanity: ./gradlew -q :sanity:catalog -DSANITY_IP=${BOURNE_IP}"
    (cd $(dirname $0)/../../..; ./gradlew -q :sanity:catalog -DSANITY_IP=${BOURNE_IP})
    exit 0
    ;;

    *)
    Usage
esac

# Webstorage parameters taken from the environment if set.
WS_SETUP_FILESHARE_COUNT=${WS_SETUP_FILESHARE_COUNT:-1}
WS_SETUP_ISILON_FS_SIZE=${WS_SETUP_ISILON_FS_SIZE:-104857600000}
WS_SETUP_VNX_FS_SIZE=${WS_SETUP_VNX_FS_SIZE:-104857600000}
WS_SETUP_NETAPP_FS_SIZE=${WS_SETUP_NETAPP_FS_SIZE:-53687091200}
WS_SETUP_NFS_FS_SIZE=${WS_SETP_NFS_FS_SIZE:-10240000}
WS_SETUP_MODE=0
WS_SETUP_COS=""
WS_SETUP_BUCKETS=""
WS_SETUP_GEO_REPGROUP=${WS_SETUP_GEO_REPGROUP:-""}
if [ ${WS_SETUP+x} ] ; then
    WS_SETUP_MODE=1
    echo "WS_SETUP mode is enabled, output file=${WS_SETUP}"
fi

case $AUTH in
    local|ldap)
    ;;
    ipv6)
        export BOURNE_IPV6_MODE="YES"
	ISI_IP="2620:0:170:2842::6199"
	ISI_SN="6805ca0e0736b7cf7b52af25f8f67c8e6d35"
	VNXF_IP="2620:0:170:2860::60"
	VNXDM_IP="2620:0:170:2860::60"
	VNXF_SN="APM00112400154"
	NETAPPF_IP="2620:0:170:2842:2a0:98ff:fe3d:bab4"
	NETAPPDM_IP="2620:0:170:2842:2a0:98ff:fe3d:bab4"
	NETAPPF_SN="700001322607"
	VMAX_SMIS_IP="2620:0:170:2842:250:56ff:fe91:56af"
	VNX_SMIS_IP="2620:0:170:2842:250:56ff:fe91:56af"
	VNXB_SN="APM00112900837"
	VMAX_SN="000195701430"
	VPLEX_VMAX_SMIS_IP="2620:0:170:2842:250:56ff:fe91:56af"
	VPLEX_VNX1_SMIS_IP="2620:0:170:2842:250:56ff:fe91:56af"
	VPLEX_VMAX_NATIVEGUID="000195701430"
	VPLEX_VNX1_NATIVEGUID="APM00112900837"
	VPLEX_IP="2620:0:170:2842::208"
	VPLEX_GUID="VPLEX+FNM00130900344:FNM00131100242"
	HDS_PROVIDER="lglap095.lss.emc.com"
	HDS_PROVIDER_IP="2620:0:170:2858::95";;
    *)
    Usage
esac

ethdev=`/sbin/ifconfig | grep Ethernet | head -1 | awk '{print $1}'`
if [ "$ethdev" = "" ] ; then
   ethdev=eth0
fi
# COP-16713: Sometimes a linux ethernet interface is more than 9 characters
#            However, ifconfig will only show the first 9.  If the name is 
#            long, run "ip link show", which will give us the full name.
if [ ${#ethdev} -gt 8 ] ; then
    ethdev=`ip link show | grep ${ethdev} | awk '{print $2}' | awk -F: '{print $1}'`
fi

macaddr=`/sbin/ifconfig ${ethdev} | /usr/bin/awk '/HWaddr/ { print $5 }'`
if [ "$macaddr" = "" ] ; then
    macaddr=`/sbin/ifconfig en0 | /usr/bin/awk '/ether/ { print $2 }'`
fi

seed=`date "+%H%M%S%N"`
seed2b=`printf "%02X" $$ | cut -b1-2`
hostseed=`echo ${macaddr} | awk -F: '{print $5$6}'`
hostbase=host${hostseed}
export BOURNE_API_SYNC_TIMEOUT=700

#
# Zone configuration
#
NH=nh
NH2=nh2
NH3=nh3
FC_ZONE_A=fctz_a
FC_ZONE_B=fctz_b
IP_ZONE=iptz
IP_ZONE2=iptz2
IP_ZONE3=iptz3

FCTZ_A=$NH/$FC_ZONE_A
FCTZ_B=$NH/$FC_ZONE_B

#
# Vdc Configuration
#
VDC_ENDPOINT_B_NAME=vdc_name_B_$$
VDC_ENDPOINT_B_SECRETKEY=
VDC_ENDPOINT_B_CERTCHAIN=
VDC_ENDPOINT_B_ID=
VDC_TEST_PROJECT=vdc_project_$$
VDC_TEST_DISCONN_RECONN_PROJECT=disrecon_vdc_project_$$
VDC_TEST_REMOVE_PROJECT=testremove_vdc_project_$$

#
# Disaster Recovery Configuration
#
DR_SITE_B_NAME=dr_site_B_$$
DR_SITE_B_DESCRIPTION="Site B description"
DR_SITE_C_NAME=dr_site_C_$$
DR_SITE_C_DESCRIPTION="Site C description"
DR_SITE_A_NAME="Active"
#
# Recovery Configuration
#
RECOVERY_CORRUPTED_NODE_IP=

#
# bourne tenant & project configuration
#
SHORTENED_HOST=`hostname`
: ${TENANT=$SHORTENED_HOST}
: ${PROJECT=sanity}
: ${SRDF_PROJECT=dontuseme}

SVCUSER=svcuser

#Local ldap server config.
LOCAL_LDAP_AUTHN_NAME='Local_Ldap_Provider'
LOCAL_LDAP_AUTHN_MODE='ldap'
LOCAL_LDAP_AUTHN_URLS='ldap://'${LOCAL_LDAP_SERVER_IP}
LOCAL_LDAP_AUTHN_DOMAINS=VIPRSANITY.COM
LOCAL_LDAP_AUTHN_SEARCH_BASE='ou=ViPR,dc=viprsanity,dc=com'
LOCAL_LDAP_AUTHN_MANAGER_DN='cn=manager,dc=viprsanity,dc=com'
LOCAL_LDAP_AUTHN_MANAGER_PWD=secret
LOCAL_LDAP_AUTHN_SEARCH_FILTER='uid=%U'
LOCAL_LDAP_AUTHN_AUTHN_GROUP_ATTR=CN
LOCAL_LDAP_AUTHN_PROVIDER_NEWNAME='Local_Ldap_Provider_Updated_Name'
LOCAL_LDAP_AUTHN_WHITELIST='ldapViPR*'
LOCAL_LDAP_AUTHN_SEARCH_SCOPE=SUBTREE
LOCAL_LDAP_AUTHN_GROUP_OBJECT_CLASSES='groupOfNames,groupOfUniqueNames,posixGroup,organizationalRole'
LOCAL_LDAP_AUTHN_GROUP_MEMBER_ATTRIBUTES='member,uniqueMember,memberUid,roleOccupant'

#Local ldap server user and group config.
LOCAL_LDAP_VIPR_USER_GROUP='  ldapViPRGroup_RootTenantOuter    '
LOCAL_LDAP_TENANT_ATTRIBUTE_KEY=title
LOCAL_LDAP_TENANT_ATTRIBUTE_ROOT_TENANT_VALUE=RootTenantUser
LOCAL_LDAP_TENANT_ATTRIBUTE_ROOT_SUBTENANT1_VALUE=SubTenant1User
LOCAL_LDAP_TENANT_ATTRIBUTE_ROOT_SUBTENANT2_VALUE=SubTenant2User
LOCAL_LDAP_SUPERUSER_USERNAME=ldapViPRUser1@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_SUPERUSER_PASSWORD=secret
LOCAL_LDAP_GROUPUSER_USERNAME=ldapViPRUser2@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_GROUPUSER_PASSWORD=secret
LOCAL_LDAP_TENANTADMIN_USERNAME=ldapViPRUser3@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_TENANTADMIN_PASSWORD=secret
LOCAL_LDAP_TENANT_USERNAME=ldapViPRUser4@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_TENANT_PASSWORD=secret
LOCAL_LDAP_TENANT_PROJECT_ADMINS_GROUP=ldapViPRGroup_RootTenantProjectAdmins@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_PROJECT_ADMIN_USERNAME=ldapViPRUser5@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_PROJECT_ADMIN_PASSWORD=secret
LOCAL_LDAP_MAXGROUPSUSER_USERNAME=ldapViPRUser5@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_MAXGROUPSUSER_PASSWORD=secret
LOCAL_LDAP_USER_USERNAME_1=ldapViPRUser6@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_USER_PASSWORD_1=secret
LOCAL_LDAP_USER_USERNAME_2=ldapViPRUser7@${LOCAL_LDAP_AUTHN_DOMAINS}
LOCAL_LDAP_USER_PASSWORD_2=secret


#Secure ldap server config.
LOCAL_SECURE_LDAP_AUTHN_NAME='Local_Secure_Ldap_Provider'
LOCAL_SECURE_LDAP_AUTHN_URLS='ldaps://'${LOCAL_LDAP_SERVER_IP}
LOCAL_SECURE_LDAP_AUTHN_DOMAINS=SECURE.VIPRSANITY.COM
LOCAL_SECURE_LDAP_AUTHN_SEARCH_BASE='dc=secure,dc=viprsanity,dc=com'
LOCAL_SECURE_LDAP_AUTHN_MANAGER_DN='uid=secureManagerDN,dc=secure,dc=viprsanity,dc=com'
LOCAL_SECURE_LDAP_AUTHN_MANAGER_PWD=secret
LOCAL_SECURE_LDAP_AUTHN_WHITELIST='secureViPRGroup*'

#Secure ldap server user and group config.
LOCAL_SECURE_LDAP_TENANT_ATTRIBUTE_KEY=title
LOCAL_SECURE_LDAP_TENANT_ATTRIBUTE_VALUE=SecureViPRUser
LOCAL_SECURE_LDAP_USER_USERNAME=secureLdapViPRUser1@${LOCAL_SECURE_LDAP_AUTHN_DOMAINS}
LOCAL_SECURE_LDAP_USER_PASSWORD=secret
LOCAL_SECURE_LDAP_USER_USERNAME_WITH_SPACES='       secureLdapViPRUser1@'${LOCAL_SECURE_LDAP_AUTHN_DOMAINS}'       '


#
# cos configuration
#
COS_VNXFILE=cosvnxf
COS_VNXE=cosvnxe
COS_UNITY=cosunity
COS_ISIFILE=cosisi
COS_NETAPP=cosnetappf
COS_NETAPPC=cosnetappcf
COS_DDFILE=cosdatadomainf
COS_VNXBLOCK=cosvnxb
COS_VNXBLOCK_FC=cosvnxb_fc
COS_VNXBLOCK_ISCSI=cosvnxb_iscsi
COS_VNXBLOCK_THIN=cosvnxb_thin
COS_VNXBLOCK_THICK=cosvnxb_thick
COS_VMAXBLOCK=cosvmaxb
COS_VMAXBLOCK_FC=cosvmaxb_fc
COS_VMAXBLOCK_ISCSI=cosvmaxb_iscsi
COS_VMAXBLOCK_THIN=cosvmaxb_thin
COS_VMAXBLOCK_THICK=cosvmaxb_thick
COS_VMAXBLOCK_SRDF_SOURCE=vpool_srdf_source
COS_VMAXBLOCK_SRDF_TARGET=vpool_srdf_target
COS_VMAXBLOCK_V3_SRDF_SOURCE=vpool_v3_srdf_source
COS_VMAXBLOCK_V3_SRDF_TARGET=vpool_v3_srdf_target
COS_RP=cos_rp
COS_RP_BASE=cos_rp_base
COS_HDS=coshds
COS_VNXEBLOCK_CG=cosvnxe_cg
COS_VNXEBLOCK_FC=cosvnxe_fc
COS_VNXEBLOCK_ISCSI=cosvnxe_iscsi
COS_ECS=ecs_vpool
COS_UNITYBLOCK_CG=cosunity_cg

#
# Isilon configuration
#
ISI_DEV=isilon_device
ISI_SMARTCONNECT_IP=${ISI_IP}
ISI_NATIVEGUID=ISILON+$ISI_SN
ISI_SMBFILESHARE1=smbfileshare1$(date +%Y%m%d%H%M%S) 
ISI_SMBFILESHARE2=smbfileshare2$(date +%Y%m%d%H%M%S)
ISI_SMBSNAPSHARE1=smbsnapshare1$(date +%Y%m%d%H%M%S)
ISI_SMBSNAPSHARE2=smbsnapshare2$(date +%Y%m%d%H%M%S) 

#
# ECS configuration
#
ECS_DEV=ecs_device
ECS_NATIVEGUID=ECS+$ECS_SN
ECS_SOFT_QUOTA=1024
ECS_HARD_QUOTA=2048
ECS_BUCKET=bucket-${RANDOM}

#
# VNX file device configuration
#
VNXF_DEV=vnxf_device
VNXF_NATIVEGUID=CELERRA+$VNXF_SN
VNXF_SMBFILESHARE1=smbfileshare1$(date +%Y%m%d%H%M%S) 

#
# VMAX3 configuration
#
VMAX3_SMIS_DEV=VMAX3_SMIS_DEV
COS_VMAX3BLOCK_FC=COS_VMAX3BLOCK_FC

#
# XtremIO config
#

XTREMIO=xtremio4
XTREMIOVOL=volXtremIO
XTREMIO_3X_NATIVEGUID=XTREMIO+$XTREMIO_3X_SN
XTREMIO_4X_NATIVEGUID=XTREMIO+$XTREMIO_4X_SN
XTREMIO_3X_COS_FC=xtremioTest3
XTREMIO_4X_COS_FC=xtremioTest4
XTREMIO_INGEST_HOST=ingesthost.lss.emc.com
XTREMIO_INGEST_HOST_LABEL=ingesthost

# XtremIO 2nd setup
XTREMIO2=xtremio2
XTREMIO2_NATIVEGUID=XTREMIO+$XTREMIO2_SN

#
# Netapp file device configuration
#
NETAPPF_DEV=netapp_device
NETAPPF_NATIVEGUID=NETAPP+$NETAPPF_SN
NETAPPF_SMBFILESHARE1=smbfileshare1$(date +%Y%m%d%H%M%S) 
NETAPPF_SMBFILESHARE2=smbfileshare2$(date +%Y%m%d%H%M%S)
NETAPPF_SMBSNAPSHARE1=smbsnapshare1$(date +%Y%m%d%H%M%S)
NETAPPF_SMBSNAPSHARE2=smbsnapshare2$(date +%Y%m%d%H%M%S)  

#
# NetApp Cluster Mode device configuration
#
NETAPPCF_DEV=netappc_device
NETAPPCF_NATIVEGUID=NETAPPC+$NETAPPCF_SN
NETAPPCF_SMBFILESHARE1=smbfileshare1$(date +%Y%m%d%H%M%S)
NETAPPCF_SMBFILESHARE2=smbfileshare2$(date +%Y%m%d%H%M%S)
NETAPPCF_SMBSNAPSHARE1=smbsnapshare1$(date +%Y%m%d%H%M%S)
NETAPPCF_SMBSNAPSHARE2=smbsnapshare2$(date +%Y%m%d%H%M%S)

#
# VNXE  device configuration
#
VNXE_DEV=vnxe_device
VNXE_NATIVEGUID=VNXE+$VNXE_SN
VNXE_SMBFILESHARE1=cifs1$(date +%Y%m%d%H%M%S) 

#
# Ceph cluster configuration
#
CEPH_PROVIDER=ceph_test_cluster
CEPH_CLIENT_HOST_NAME=ceph_client
CEPH_COS=ceph_vpool
CEPH_VOLNAME=EXPCephTest${RANDOM}
CEPH_EXPORT_GROUP_NAME=EXPCephGroup${RANDOM}

# UNITY  device configuration
#
UNITY_DEV=unity_device
UNITY_NATIVEGUID=UNITY+$UNITY_SN

#
# DataDomain file device configuration
#
DATADOMAINF_DEV=datadomainf_device
DATADOMAINF_PROVIDER=datadomain_provider
DATADOMAINF_PROVIDER_INTERFACE=ddmc
DATADOMAINF_NATIVEGUID=$DATADOMAINF_ID
DATADOMAINF_SMBFILESHARE1=smbfileshare1$(date +%Y%m%d%H%M%S)
DATADOMAINF_SMBFILESHARE2=smbfileshare2$(date +%Y%m%d%H%M%S)

#
# SRDF Configuration
#
SRDF_VOLUME=srdfSanity-${HOSTNAME}-${RANDOM}

#
# RecoverPoint configuration
#
RP_SYSTEM_TYPE=rp
RP_REMOVE_BAD_CHARS_HOSTNAME=${HOSTNAME//[^a-zA-Z0-9]/}
RP_MODIFIED_HOSTNAME=${RP_REMOVE_BAD_CHARS_HOSTNAME:0:8}
RP_SANITY_RANDOM=$((RANDOM % 999))

RP_VOLUME=rpSanity-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-vol
RP_CONSISTENCY_GROUP=rpSanity-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-cg

RP_VPLEX_VOLUME=rpvpSanity-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-vol
RP_VPLEX_CONSISTENCY_GROUP=rpvplexSanity-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-cg

RP_METROPOINT_VOLUME=rpmpSanity-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-vol
RP_METROPOINT_CONSISTENCY_GROUP=rpmpSanity-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-cg

RP_XIO_VOLUME=rpxioSanity-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-vol
RP_XIO_CONSISTENCY_GROUP=rpxioSanity-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}-cg

RP_EXPORT_GROUP=rpSanity-${RP_MODIFIED_HOSTNAME}-${RP_SANITY_RANDOM}
RP_EXPORT_GROUP_HOST=host${hostseed}.sanity.com

#
# Configuration for Simulator
#
RP_PROVIDER_SIMULATOR_IP=$SIMULATOR_IP
SIMULATOR_CISCO_MDS_IP=$SIMULATOR_IP
RP_SIMULATOR_IP=$SIMULATOR_IP
RP_SIMULATOR=rp-sim
PROVIDER_SIMULATOR=provider-sim
FABRIC_SIMULATOR=fabric-sim
SIMULATOR_VSAN_11=VSAN_11
SIMULATOR_VSAN_12=VSAN_12
VPLEX_PROVIDER_SIMULATOR=$PROVIDER_SIMULATOR
VPLEX_PROVIDER_SIMULATOR_IP=$SIMULATOR_IP
VPLEX_SIMULATOR=vplex-sim

#
# Full copy configuration
#
FULL_COPY_VOLUME=full-copy-test-${HOSTNAME}-${RANDOM}

#
# MIRROR (VMAX) block device configuration
#
COS_MIRROR=cosmirror
COS_MIRROR_WITH_OPTIONAL=cosmirror_with_optional
COS_MIRROR_WITH_2_MIRRORS=cosmirror_max_2
COS_MIRROR_BEFORE_CHANGE=cosmirror_before_change
COS_MIRROR_AFTER_CHANGE=cosmirror_after_change
COS_MIRROR_VNX=cosmirror_vnx
COS_VMAX_CG_MIRROR=cosvmax_cg_mirror

#
# Export group configuration
#
VNX_VOLUME=VnxSanityVol-${HOSTNAME}-${RANDOM}
VNX_META_VOLUME=VnxSanityMeta-${HOSTNAME}-${RANDOM}
VNXEXPORT_GROUP=VnxExp${RANDOM}
VNXEXPORT_GROUP_HOST=host.vnx.export${seed}
VMAX_VOLUME=VmaxSanity-${HOSTNAME}-${RANDOM}
VMAX_META_VOLUME=VmaxSanityMeta-${HOSTNAME}-${RANDOM}
VMAXEXPORT_GROUP=VmaxExp${RANDOM}
VMAXEXPORT_GROUP_HOST=host.vnx.export${seed}
VMAX_VNXEXPORT_GROUP=VmaxVnxExp-${RANDOM}
VMAX_VNXEXPORT_GROUP_HOST=vmax.vnx.export${seed}
XTREMIOEXPORT_GROUP_HOST=host.xtremio.export${seed}
XTREMIOEXPORT_GROUP=XtremIOExp${RANDOM}
BLOCKEXPORT_GROUP=BlockSanityExportGroup-${HOSTNAME}-${RANDOM}
MIRROR_VOLUME=VmaxMirror-${HOSTNAME}-${RANDOM}
MIRROR_VOLUME_VNX=VnxMirror-${HOSTNAME}-${RANDOM}
CONSISTENCY_GROUP_SRDF=${RANDOM}
CONSISTENCY_GROUP=consistency-group-`date +%s | cut -c5-10`
CONSISTENCY_GROUP_SNAPSHOT=group-snapshot-`date +%s | cut -c5-10`
VNX_COS_GROUP=vnx-cos-group-`date +%s | cut -c5-10`
VMAX_COS_GROUP=vmax-cos-group-`date +%s | cut -c5-10`

#
# fileshare tests configuration
#
# Min 1GB FS
FS_SIZE=1073741824
FS_SIZEMB=1024MB
# Expand size 1.5GB FS
FS_EXPAND_SIZE=1610612736
FSEXP_RO_EPS="www.ferrari.com www.porsche.com"
FSEXP_RW_EPS="www.lexus.com www.infiniti.com www.acura.com"
FSEXP_SHARED_VARRAY_RW_EPS="client1.emc.com client2.emc.com"
FSEXP_ROOT_EPS="www.honda.com"
FSEXP1="www.ford.com"
FSEXP2="www.gmc.com"
FSEXP3="www.pontiac.com"
FSEXP4="www.kia.com"
FSEXP_DEFAULT_EPS="$FSEXP1 $FSEXP2 $FSEXP3"
SNAPEXP_DEFAULT_EPS="www.emc.com www.abc.com www.amazon.com"
FS_VNXE_SIZE=2000000000
FS_VNXE_EXPAND_SIZE=3000000000
FS_UNITY_SIZE=3000000000
FS_UNITY_EXPAND_SIZE=4000000000


# Alternate configuration file for alternate hardware/variables
ALTERNATE_CONFIG_FILE=myhardware.conf
if [ -f ${ALTERNATE_CONFIG_FILE} ]
then
   source ${ALTERNATE_CONFIG_FILE}
fi

pwwn()
{
    idx=$1
    echo 50:${macaddr}:${idx}
}

nwwn()
{
    idx=$1
    echo 51:${macaddr}:${idx}
}

wwnIdx() {
    i=$1
    j=$2
    k=$(($i - 1))
    k=$((2 * $k))
    k=$(($j + $k))
    echo $k
}

#
# block tests configuration
#
BLK_SIZE=1073741824
BLK_SIZE_EXPAND=2147483648 # 2GB
BLK_SIZE_EXPAND_2=2462056448 # 2GB + 300MB
BLK_SIZE_EXPAND_3=2566914048 # 2GB + 400MB
BLK_LUN1=1
BLK_LUN2=2
BLK_LUN3=3
BLK_CLIENT_FC=`pwwn ${seed2b}`
BLK_CLIENT_FC_NODE=`nwwn ${seed2b}`
HOSTNAME=`hostname`
BLK_CLIENT_iSCSI=iqn.2010-01.com.emc.snafu-${HOSTNAME}_${seed}
BLK_HOSTID=sanity-host-${HOSTNAME}-${seed}

######################### Start of Driver Managed System configuration. ############################
#

#
# Driver managed system setup (simulated)
#
DRIVER_SYSTEM_TYPE=driversystem
DRIVER_SYSTEM_IP=10.20.30.40
DRIVER_SYSTEM_PORT=8080
DRIVER_SYSTEM_USER=user
DRIVER_SYSTEM_PASSWORD=password
######################### End of Driver Managed System configuration. ############################

export BOURNE_IPADDR="$BOURNE_IP"

source sanity_utils

# search configuration
TAG=$(tr -dc A-Za-z0-9_ < /dev/urandom | head -c 4)
SEARCH_PREFIX=$(echo $TAG|head -c 2)

# Place to put command output in case of failure
CMD_OUTPUT=/tmp/output.txt

# General echo output
secho()
{
    echo "*** $*"
}

#
# run commands and check for exit status
# Do not add to the undo list; mostly because the "create" version
# of the command does not match the "delete" in arguments, like 
# some blocksnapshot create or multi-volume create commands
#
run_noundo()
{
    cmd=$*
    echo === $cmd
    rm -f ${CMD_OUTPUT}
    if [ "${HIDE_OUTPUT}" = "" -o "${HIDE_OUTPUT}" = "1" ]; then
	$cmd &> ${CMD_OUTPUT}
    else
	$cmd 2>&1
    fi
}

#
# run commands and check for exit status
#
run()
{
    run_noundo $*
    set_undo $*
}

balance()
{
    cmd=$*
    echo $cmd
    $cmd --ip=${BOURNE_IP_ARRAY[$IP_INDEX]}
    if [ $((IP_INDEX+1)) -ge ${#BOURNE_IP_ARRAY[@]} ]; then 
        IP_INDEX=0
    else
        IP_INDEX=$((IP_INDEX+1))
    fi       
}

#
# Creates a neighborhood
# Sets up one IP transport zone and one FC transport zone
#
zone_setup()
{
    # do this only once
    neighborhood show $NH  &> /dev/null && return $?

    run neighborhood create $NH
    if [ "$EXTRA_PARAM" = "search" ] ; then
        neighborhood search $(echo $NH | head -c 2)
        run neighborhood tag $NH $TAG
        neighborhood search $SEARCH_PREFIX --tag true
    fi

    run neighborhood create $NH2

    run transportzone create $IP_ZONE $NH --type IP
    if [ "$EXTRA_PARAM" = "search" ] ; then
        transportzone search $(echo $IP_ZONE | head -c 2)
        run transportzone tag $NH/$IP_ZONE $TAG
        transportzone search $SEARCH_PREFIX --tag true
    fi

    # Set the object transport zone
    # Cleanup just in case... Ignore errors because grep will exit with non-zero if
    # it doesn't match anything (which is the case if the object transport zone
    # isn't set).
    trap - ERR

    run transportzone add $NH/$IP_ZONE $FSEXP1
    run transportzone add $NH/$IP_ZONE $FSEXP2
    run transportzone add $NH/$IP_ZONE $FSEXP3

    #
    # set up zone for cisco switch simulator
    #
    if [ $QUICK -eq 1 ]; then
        cisco_mds_quick_setup_once
    elif [ $DISCOVER_SAN -eq 1 ]; then
        brocade_setup_once
    else
        run transportzone create $FC_ZONE_A $NH --type FC
        run transportzone create $FC_ZONE_B $NH --type FC
        run transportzone add    $NH/$FC_ZONE_A $BLK_CLIENT_FC
    fi

    for i in A1 A2 A3 A4 A5 A6 A7 A8 C1 C2 C3 C4 C5 C6 C7 C8
    do
        wwn=`pwwn $i`
	run transportzone add $FCTZ_A $wwn
    done

    for i in B1 B2 B3 B4 B5 B6 B7 B8 D1 D2 D3 D4 D5 D6 D7 D8
    do
        wwn=`pwwn $i`
        run transportzone add $FCTZ_B $wwn
    done

    trap '_failure $LINENO' ERR
}

brocade_setup_once() 
{
    # Do once
    nsys=`networksystem list | wc -l`
    [ "$nsys" -gt 0 ] && return;

    #Discover the Brocade SAN switch.
    secho "Discovering brocade ..."
    run networksystem create $BROCADE_NETWORK brocade --smisip $BROCADE_IP --smisport 5988 --smisuser $BROCADE_USER --smispw $BROCADE_PW --smisssl false
    sleep 30

    run transportzone assign FABRIC_losam082-fabric $NH
    run transportzone assign FABRIC_vplex154nbr2 $NH2
    VPLEX_TZ1=$NH/FABRIC_losam082-fabric
    VPLEX_TZ2=$NH2/FABRIC_vplex154nbr2
    FC_ZONE_A=FABRIC_losam082-fabric
    FC_ZONE_B=FABRIC_vplex154nbr2
    FCTZ_A=$NH/$FC_ZONE_A
    FCTZ_B=$NH2/$FC_ZONE_B
}

cisco_mds_quick_setup_once()
{
    # Do once - Discover Cisco MDS simulator switch
    secho "Discover Cisco MDS simulator switch"
    run networksystem create CiscoMdsSimulator  mds --devip $SIMULATOR_CISCO_MDS --devport 22 --username $SIMULATOR_CISCO_MDS_USER --password $SIMULATOR_CISCO_MDS_PW
   
#    VPLEX_TZ1=$NH/FABRIC_losam082-fabric
#    VPLEX_TZ2=$NH2/FABRIC_vplex154nbr2
    FC_ZONE_A=VSAN_11
    FC_ZONE_B=VSAN_12
    FCTZ_A=$NH/$FC_ZONE_A
    FCTZ_B=$NH2/$FC_ZONE_B


    run transportzone assign VSAN_11 $NH
    run transportzone assign VSAN_12 $NH

#   add ports to VSAN_11
    run transportzone add $FCTZ_A 51:00:50:56:9F:01:3B:A1
    run transportzone add $FCTZ_A 51:00:50:56:9F:01:3B:A2
    run transportzone add $FCTZ_A 51:00:50:56:9F:01:3B:A3
    run transportzone add $FCTZ_A 51:00:50:56:9F:01:3B:A4
}


#
# Setup clusters, hosts, etc.
# Setup two clusters with 2 hosts each. Each host has initiators
# in transport zone A and B
#
host_setup()
{
    # do this only once
    cluster show $TENANT/sanityCluster1 &> /dev/null && return $?

    secho "Setup hosts and clusters for $TENANT started"
    proj=$PROJECT
    tenant=$TENANT

    for i in 1 2
    do
        cluster=sanityCluster$i
        run cluster create $cluster $tenant --project $proj
        j=1
        while [ $j -lt 3 ]
        do
            host=$hostbase$tenant$i$j
            k=`wwnIdx $i $j`
            nwwn=`nwwn $i$j`
            pwwn1=`pwwn A$k`
            pwwn2=`pwwn B$k`
            pwwn3=`pwwn C$k`
            pwwn4=`pwwn D$k`
            run hosts create $host $tenant Windows ${host}.lss.emc.com --port 8111 --username user --password 'password' --osversion 1.0 --cluster ${tenant}/${cluster}
            run initiator create $host FC ${pwwn1} --node ${nwwn}
            run initiator create $host FC ${pwwn2} --node ${nwwn}
            run initiator create $host FC ${pwwn3} --node ${nwwn}
            run initiator create $host FC ${pwwn4} --node ${nwwn}

	    #
	    # create host initiator for quick sanity
	    #
	    if [ $QUICK -eq 1 ]; then
        	run initiator create $host FC 51:00:50:56:9F:01:3B:A$k  --node 50:00:50:56:9F:01:3B:$i$j
            fi

            j=$(( $j + 1 ))
        done
    done

    secho "Setup hosts and clusters for $TENANT ended"
}


#
# create a tenant and project for running the sanity tests
#
tenant_setup()
{
    run security add_authn_provider $LOCAL_LDAP_AUTHN_MODE $LOCAL_LDAP_AUTHN_URLS $LOCAL_LDAP_AUTHN_MANAGER_DN $LOCAL_LDAP_AUTHN_MANAGER_PWD $LOCAL_LDAP_AUTHN_SEARCH_BASE $LOCAL_LDAP_AUTHN_SEARCH_FILTER $LOCAL_LDAP_AUTHN_AUTHN_GROUP_ATTR "$LOCAL_LDAP_AUTHN_NAME" $LOCAL_LDAP_AUTHN_DOMAINS "$LOCAL_LDAP_AUTHN_WHITELIST" $LOCAL_LDAP_AUTHN_SEARCH_SCOPE --group_object_classes "$LOCAL_LDAP_AUTHN_GROUP_OBJECT_CLASSES" --group_member_attributes "$LOCAL_LDAP_AUTHN_GROUP_MEMBER_ATTRIBUTES"
    run security get_authn_provider "$LOCAL_LDAP_AUTHN_NAME"
    if [ "$EXTRA_PARAM" = "search" ] ; then
        run security search_authn_provider $(echo $LOCAL_LDAP_AUTHN_NAME | head -c 2)
        run security tag_authn_provider "$LOCAL_LDAP_AUTHN_NAME" $TAG
        run security search_authn_provider $SEARCH_PREFIX --tag true
    fi

    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        run tenant add_group $LOCAL_LDAP_AUTHN_DOMAINS "$LOCAL_LDAP_VIPR_USER_GROUP"
        run security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD
        run security login $SYSADMIN $SYSADMIN_PASSWORD
        run tenant add_attribute $LOCAL_LDAP_AUTHN_DOMAINS $LOCAL_LDAP_TENANT_ATTRIBUTE_KEY $LOCAL_LDAP_TENANT_ATTRIBUTE_ROOT_TENANT_VALUE
        run security add_tenant_role subject_id $LOCAL_LDAP_SUPERUSER_USERNAME TENANT_ADMIN
        run security add_zone_role subject_id $LOCAL_LDAP_SUPERUSER_USERNAME SYSTEM_ADMIN
        run security add_zone_role subject_id $LOCAL_LDAP_SUPERUSER_USERNAME SYSTEM_MONITOR
        run security add_zone_role subject_id $LOCAL_LDAP_SUPERUSER_USERNAME SECURITY_ADMIN 
        run security add_zone_role subject_id $LOCAL_LDAP_SUPERUSER_USERNAME SYSTEM_AUDITOR
        run security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
        run security verify_user_roles "SYSTEM_ADMIN,SYSTEM_MONITOR,SECURITY_ADMIN,SYSTEM_AUDITOR,TENANT_ADMIN"
    fi
}

#
# create a tenant and project for running the sanity tests
#
project_setup()
{
    tenant show $TENANT &> /dev/null && return $?
    run tenant create $TENANT $LOCAL_LDAP_AUTHN_DOMAINS 'OU' ${seed}
    secho "Tenant $TENANT created."
    if [ "$EXTRA_PARAM" = "search" ] ; then
        tenant search $(echo $TENANT | head -c 2)
        tenant tag "$TENANT" $TAG
        tenant search $SEARCH_PREFIX --scope $TENANT --tag true
    fi

    project show $PROJECT &> /dev/null && return $?
    run project create $PROJECT --tenant $TENANT 
    project show $SRDF_PROJECT &> /dev/null && return $?
    run project create $SRDF_PROJECT --tenant $TENANT
    if [ "$EXTRA_PARAM" = "search" ] ; then
        project search $(echo $PROJECT | head -c 2)
        project tag "$PROJECT" $TAG
        project search $SEARCH_PREFIX --scope $TENANT --tag true
    fi
}

#
# setup 3 types of cos
# -- file cos for isilon tests
# -- file cos for vnx tests
# -- block cos for vnx FC & iSCSI test
#
isilon_cos_setup()
{
    echo "setting up isilon COS"
    run cos create file $COS_ISIFILE 				\
	--description 'Virtual-Pool-for-Isilon' true 	\
                            --protocols NFS CIFS --max_snapshots 10 \
                            --provisionType 'Thin' \
			    --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_ISIFILE file $ROOT_TENANT
    if [ "$EXTRA_PARAM" = "search" ] ; then
        cos search $(echo $COS_ISIFILE | head -c 2) --resource_type file_vpool
        cos tag "$COS_ISIFILE" "file" $TAG
        cos search $SEARCH_PREFIX --tag true --resource_type file_vpool
    fi
}

ecs_cos_setup()
{
    echo "setting up ECS cos-vpool"
    run cos create object $COS_ECS --description 'cos_vpool-for-ECS' true --protocols S3 --provisionType 'Thick' --neighborhoods $NH
}

vnxfile_cos_setup()
{
    echo "setting up VNX COS"
    run cos create file $COS_VNXFILE 				\
	--description 'Virtual-Pool-for-VNX-file' true \
                         --protocols NFS CIFS --max_snapshots 10    \
                         --provisionType 'Thin' \
		         --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_VNXFILE file $ROOT_TENANT
}

netapp_cos_setup()
{
    run cos create file $COS_NETAPP 				\
	--description 'Virtual-Pool-for-NETAPP' false 	\
                         --protocols NFS CIFS --max_snapshots 10 --provisionType 'Thin' \
			 --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_NETAPP file $ROOT_TENANT
}

netappc_cos_setup()
{
    run cos create file $COS_NETAPPC                                \
	--description 'Virtual-Pool-for-NETAPPC' false   \
                         --protocols NFS CIFS --max_snapshots 10 --provisionType 'Thin' \
                         --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_NETAPPC file $ROOT_TENANT
}

vnxe_cos_setup()
{
    secho "setting up VNXe Virtual Pool"
    run cos create file $COS_VNXE 				\
	--description 'Virtual-Pool-for-VNXe-file' true \
                         --protocols NFS CIFS --max_snapshots 10    \
                         --provisionType 'Thin' \
		         --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_VNXE file $ROOT_TENANT
    
    run cos create block $COS_VNXEBLOCK_CG                            \
	--description 'Virtual-Pool-for-VNXe-block-cg' true         \
                         --protocols iSCSI                   \
                         --numpaths 2 \
                         --max_snapshots 10 \
                         --system_type vnxe \
                         --provisionType 'Thin' \
                         --neighborhoods $NH \
                         --multiVolumeConsistency 
                         
    run cos create block $COS_VNXEBLOCK_FC 				\
	--description 'Virtual-Pool-for-VNX-block-FC' true 	\
                         --protocols FC 			\
                         --numpaths 2 \
                         --max_snapshots 10 \
	                	 --system_type vnxe \
                         --provisionType 'Thin' \
                         --neighborhoods $NH

    run cos create block $COS_VNXEBLOCK_ISCSI 			\
	--description 'Virtual-Pool-for-VNXe-block-iSCSI' true \
                         --protocols iSCSI			\
                         --numpaths 2 \
                         --max_snapshots 10 \
	                  --system_type vnxe \
                         --provisionType 'Thin' \
                         --expandable true \
                         --neighborhoods $NH
                                 
}

datadomainfile_cos_setup()
{
    run cos create file $COS_DDFILE                \
	--description 'Virtual-Pool-for-DATADOMAIN-file' true   \
             --protocols NFS CIFS --max_snapshots 10 \
             --provisionType 'Thin' \
             --long_term_retention 'true' \
             --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_DDFILE file $ROOT_TENANT
}

vnxblock_cos_setup()
{
    secho "VNX Block Virtual Pool setup"
    if [ $QUICK -eq 0 ]; then
      run cos create block $COS_VNXBLOCK 				\
	  --description 'Virtual-Pool-for-VNX-block' true 	\
                         --protocols FC iSCSI			\
                         --numpaths 2 \
                         --max_snapshots 10 \
	                 --system_type vnxblock \
                         --provisionType 'Thin' \
                         --neighborhoods $NH
    else 
      run cos create block $COS_VNXBLOCK                            \
	  --description 'Virtual-Pool-for-VNX-block' true         \
                         --protocols FC                    \
                         --numpaths 2 \
                         --max_snapshots 10 \
                         --system_type vnxblock \
                         --provisionType 'Thin' \
                         --neighborhoods $NH
    fi
    
    if [ "$EXTRA_PARAM" = "search" ] ; then
        cos search $(echo $COS_VNXBLOCK | head -c 2) --resource_type block_vpool
        cos tag "$COS_VNXBLOCK" "block" $TAG
        cos search $SEARCH_PREFIX --tag true --resource_type block_vpool
    fi

    run cos create block $COS_VNXBLOCK_FC 				\
	--description 'Virtual-Pool-for-VNX-block-FC' true 	\
                         --protocols FC 			\
                         --numpaths 2 \
                         --max_snapshots 10 \
	                 --system_type vnxblock \
                         --provisionType 'Thin' \
                         --neighborhoods $NH

    run cos create block $COS_VNXBLOCK_ISCSI 			\
	--description 'Virtual-Pool-for-VNX-block-iSCSI' true \
                         --protocols iSCSI			\
                         --numpaths 2 \
                         --max_snapshots 10 \
	                 --system_type vnxblock \
                         --provisionType 'Thin' \
			 --neighborhoods $NH


    run cos create block $COS_VNXBLOCK_THIN 				\
	--description 'VNX-thin-storage' true      \
                             --protocols FC iSCSI	    \
                             --numpaths 2 \
                             --max_snapshots 10 \
	                 --system_type vnxblock \
                             --provisionType 'Thin' \
                             --expandable true \
                         --neighborhoods $NH

    run cos create block $COS_VNXBLOCK_THICK 				\
	--description 'VNX-thick-storage' true      \
                             --protocols FC iSCSI	    \
                             --numpaths 2 \
                             --max_snapshots 10 \
	                 --system_type vnxblock \
                             --provisionType 'Thick' \
                             --expandable true \
                         --neighborhoods $NH
                                 
}

xtremio_cos_setup()
{
    run cos create block $XTREMIO_3X_COS_FC                          \
	--description 'Virtual-Pool-for-XtremIO-block-FC' false      \
                         --protocols FC                         \
                         --numpaths 1 \
                         --max_snapshots 10 \
                         --system_type xtremio \
                         --provisionType 'Thin' \
                         --neighborhoods $NH
    run cos allow $XTREMIO_3X_COS_FC block $TENANT

    run cos create block $XTREMIO_4X_COS_FC                          \
	--description 'Virtual-Pool-for-XtremIO-block-FC' false      \
                         --protocols FC                         \
                         --numpaths 1 \
                         --max_snapshots 10 \
                         --system_type xtremio \
                         --provisionType 'Thin' \
                         --neighborhoods $NH   \
                         --multiVolumeConsistency
    run cos allow $XTREMIO_4X_COS_FC block $TENANT  
}

xtremio_port_setup()
{
    secho "XtremIO port setup"
    run storageport update $XTREMIO_3X_NATIVEGUID FC --addvarrays $NH
    run storageport update $XTREMIO_4X_NATIVEGUID FC --addvarrays $NH
}

xtremio_storage_setup() {
    secho "Starting XtremIO 3.x storageprovider create"
    storageprovider create xtremio3x $XTREMIO_3X_IP 443 $XTREMIO_3X_USER $XTREMIO_3X_PASSWD xtremio 

    secho "Starting XtremIO 4.x storageprovider create"
    storageprovider create xtremio4x $XTREMIO_4X_IP 443 $XTREMIO_4X_USER $XTREMIO_4X_PASSWD xtremio

    secho 'XtremIO discover storage systems'
    storagedevice discover_all
    storagedevice list
}

xtremio_network_setup()
{
    secho "XtremIO network setup"
    networksystem show $BROCADE_NETWORK &> /dev/null && return $?
    networksystem create $BROCADE_NETWORK brocade --smisip $BROCADE_IP --smisport 5988 --smisuser $BROCADE_USER --smispw $BROCADE_PW --smisssl false
    run transportzone assign ${SRDF_VMAXA_VSAN} $NH
}

xtremio_varray_setup()
{
    secho 'XtremIO varray setup for $TENANT'
    run neighborhood allow $NH $TENANT
}

xtremio_ingest_host_setup() {
    PWWN1=10:00:00:00:C9:42:6D:50
    WWNN1=20:00:00:00:C9:42:6D:50
    PWWN2=10:00:00:00:C9:42:6D:51
    WWNN2=20:00:00:00:C9:42:6D:51

    run hosts create ${XTREMIO_INGEST_HOST} $TENANT Windows ${XTREMIO_INGEST_HOST} --port 8111 --username user --password 'password' --osversion 1.0 

    run initiator create ${XTREMIO_INGEST_HOST} FC ${PWWN1} --node ${WWNN1}
    run initiator create ${XTREMIO_INGEST_HOST} FC ${PWWN2} --node ${WWNN2}
    if [ "$XIO_QUICK_PARAM" = "quick" ]; then
        run transportzone add VSAN_11 ${PWWN1}
        run transportzone add VSAN_11 ${PWWN2}
    else
        run transportzone add ${SRDF_VMAXA_VSAN} ${PWWN1}
        run transportzone add ${SRDF_VMAXA_VSAN} ${PWWN2}
    fi

}

xtremio_ingest_test()
{
    secho "XtremIO ingest tests"
    INGEST_3X_NATIVEGUID=$XIO_3X_SIM_NATIVEGUID
    INGEST_4X_NATIVEGUID=$XIO_4X_SIM_NATIVEGUID

    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        INGEST_3X_NATIVEGUID=$XTREMIO_3X_NATIVEGUID
        INGEST_4X_NATIVEGUID=$XTREMIO_4X_NATIVEGUID
    fi

    # Labels
    export_name=$XTREMIOEXPORT_GROUP
    XIO_RANDOM=${RANDOM}
    XIO_INGEST_VOL1_CG=${XIO_INGEST_VOL1_CGBASE}${XIO_RANDOM}
    XIO_INGEST_VOL1=${XIO_INGEST_VOL1BASE}1${XIO_RANDOM}
    XIO_INGEST_VOL2=${XIO_INGEST_VOL1BASE}2${XIO_RANDOM}
    XIO_INGEST_SNAP1=xioingestsnap1${XIO_RANDOM}
    XIO_INGEST_SNAP2=xioingestsnap2${XIO_RANDOM}

    # Create the consistency group + volume
    run blockconsistencygroup create ${PROJECT} ${XIO_INGEST_VOL1_CG}
    run volume create ${XIO_INGEST_VOL1} ${PROJECT} ${NH} ${XTREMIO_4X_COS_FC} 1GB --consistencyGroup ${XIO_INGEST_VOL1_CG}

    run volume create ${XIO_INGEST_VOL2} ${PROJECT} ${NH} ${XTREMIO_3X_COS_FC} 1GB


    # Create XIO snapshots
    secho "Create snapshots"
    run_noundo blocksnapshot create ${PROJECT}/${XIO_INGEST_VOL1} ${XIO_INGEST_SNAP1}
    XIO_INGEST_SNAP1_ARRAY_LABEL=`/opt/storageos/bin/dbutils list BlockSnapshot | grep deviceLabel | awk '{print $3}'`
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1} --vipronly

    run_noundo blocksnapshot create ${PROJECT}/${XIO_INGEST_VOL2} ${XIO_INGEST_SNAP2}
    XIO_INGEST_SNAP2_ARRAY_LABEL=`/opt/storageos/bin/dbutils list BlockSnapshot | grep deviceLabel | awk '{print $3}'`
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2} --vipronly

    # Delete the volume and CG
    run volume delete ${PROJECT}/${XIO_INGEST_VOL1} --vipronly
    run blockconsistencygroup delete ${XIO_INGEST_VOL1_CG} --vipronly

    run volume delete ${PROJECT}/${XIO_INGEST_VOL2} --vipronly

    secho "XIO ingest discovery"

    # Discover unmanaged volumes on the array
    run storagedevice discover_namespace $INGEST_3X_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $INGEST_4X_NATIVEGUID 'UNMANAGED_VOLUMES'

    ##Make sure we fail when we ingest the volumes because snaps are not ingested yet
    fail unmanagedvolume ingest_unexport ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL1}"
    fail unmanagedvolume ingest_unexport ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL2}"

    run unmanagedvolume ingest_unexport ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP1_ARRAY_LABEL}"
    run unmanagedvolume ingest_unexport ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP2_ARRAY_LABEL}"

    # Verify the volumes, CG and snaps got created; fail script if this fails.
    run volume show ${PROJECT}/${XIO_INGEST_VOL1}
    run blockconsistencygroup show "${XIO_INGEST_VOL1_CG}"
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL}

    run volume show ${PROJECT}/${XIO_INGEST_VOL2}
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}

    ## Run block tests for the volumes and snaps
    xtremio_export_test ${export_name}1 ${XIO_INGEST_VOL1} ${XIO_INGEST_VOL2} ${XIO_INGEST_SNAP1_ARRAY_LABEL} ${XIO_INGEST_SNAP2_ARRAY_LABEL}
    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        blockconsistencygroup_restore_snapshot $XIO_INGEST_VOL1_CG $XIO_INGEST_SNAP1_ARRAY_LABEL
    fi

    # Delete the volumes, snaps and CG again
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL} --vipronly
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL} --vipronly

    run_noundo volume delete ${PROJECT}/${XIO_INGEST_VOL1} --vipronly
    run_noundo blockconsistencygroup delete ${XIO_INGEST_VOL1_CG} --vipronly

    run_noundo volume delete ${PROJECT}/${XIO_INGEST_VOL2} --vipronly

    # Discover unmanaged volumes on the array again
    run storagedevice discover_namespace $INGEST_3X_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $INGEST_4X_NATIVEGUID 'UNMANAGED_VOLUMES'
    
    ##Re-ingest
    ##Make sure we fail when we ingest the snaps because volumes are not ingested yet

    fail unmanagedvolume ingest_unexport ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP1_ARRAY_LABEL}"
    fail unmanagedvolume ingest_unexport ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP2_ARRAY_LABEL}"

    run unmanagedvolume ingest_unexport ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL1}"
    run unmanagedvolume ingest_unexport ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL2}"

    # Verify the volumes, CG and snaps got created; fail script if this fails.
    run volume show ${PROJECT}/${XIO_INGEST_VOL1}
    run blockconsistencygroup show "${XIO_INGEST_VOL1_CG}"
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL}

    run volume show ${PROJECT}/${XIO_INGEST_VOL2}
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}

    ## Run block tests for the volumes and snaps
    xtremio_export_test ${export_name}1 ${XIO_INGEST_VOL1} ${XIO_INGEST_VOL2} ${XIO_INGEST_SNAP1_ARRAY_LABEL} ${XIO_INGEST_SNAP2_ARRAY_LABEL}
    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        blockconsistencygroup_restore_snapshot $XIO_INGEST_VOL1_CG $XIO_INGEST_SNAP1_ARRAY_LABEL
    fi

    # Real removal of volume/CG
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL}
    run_noundo volume delete ${PROJECT}/"${XIO_INGEST_VOL1}" --wait
    run_noundo blockconsistencygroup delete ${XIO_INGEST_VOL1_CG}

    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}
    run_noundo volume delete ${PROJECT}/"${XIO_INGEST_VOL2}" --wait

}

xtremio_ingest_export_test()
{
    secho "XtremIO ingest export tests"

    INGEST_3X_NATIVEGUID=$XIO_3X_SIM_NATIVEGUID
    INGEST_4X_NATIVEGUID=$XIO_4X_SIM_NATIVEGUID

    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        INGEST_3X_NATIVEGUID=$XTREMIO_3X_NATIVEGUID
        INGEST_4X_NATIVEGUID=$XTREMIO_4X_NATIVEGUID
    fi

    # Labels
    XIO_INGEST_VOL1_CG=${XIO_INGEST_VOL1_CGBASE}${XIO_RANDOM}exp
    XIO_INGEST_VOL1=${XIO_INGEST_VOL1BASE}1${XIO_RANDOM}exp
    XIO_INGEST_VOL2=${XIO_INGEST_VOL1BASE}2${XIO_RANDOM}exp
    XIO_INGEST_SNAP1=xioingestsnap1${XIO_RANDOM}exp
    XIO_INGEST_SNAP2=xioingestsnap2${XIO_RANDOM}exp

    xtremio_ingest_host_setup

    # Create the consistency group + volume
    run blockconsistencygroup create ${PROJECT} ${XIO_INGEST_VOL1_CG}
    run volume create ${XIO_INGEST_VOL1} ${PROJECT} ${NH} ${XTREMIO_4X_COS_FC} 1GB --consistencyGroup ${XIO_INGEST_VOL1_CG}

    run volume create ${XIO_INGEST_VOL2} ${PROJECT} ${NH} ${XTREMIO_3X_COS_FC} 1GB

    # Export the volume to a host

    run export_group create ${PROJECT} EG-xio-ingest ${NH} --type Host --volspec "${PROJECT}/${XIO_INGEST_VOL1},${PROJECT}/${XIO_INGEST_VOL2}" --hosts "${XTREMIO_INGEST_HOST}"

    # Create XIO snapshots
    secho "Create snapshots"
    run blocksnapshot create ${PROJECT}/${XIO_INGEST_VOL1} ${XIO_INGEST_SNAP1}
    # Capture the deviceLabel of the snapshot which will be the label after unmanaged volume discovery
    XIO_INGEST_SNAP1_ARRAY_LABEL=`/opt/storageos/bin/dbutils list BlockSnapshot | grep deviceLabel | awk '{print $3}'`
    run export_group update ${PROJECT}/EG-xio-ingest --addVolspec "${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1}"
    run blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1} --vipronly

    run blocksnapshot create ${PROJECT}/${XIO_INGEST_VOL2} ${XIO_INGEST_SNAP2}
    XIO_INGEST_SNAP2_ARRAY_LABEL=`/opt/storageos/bin/dbutils list BlockSnapshot | grep deviceLabel | awk '{print $3}'`
    run export_group update ${PROJECT}/EG-xio-ingest --addVolspec "${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2}"
    run blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2} --vipronly

    # Delete the volume and CG
    run volume delete ${PROJECT}/${XIO_INGEST_VOL1} --vipronly
    run export_group delete ${PROJECT}/EG-xio-ingest
    run blockconsistencygroup delete ${XIO_INGEST_VOL1_CG} --vipronly

    run volume delete ${PROJECT}/${XIO_INGEST_VOL2} --vipronly

    secho "XIO ingest discovery"

    # Discover unmanaged volumes on the array
    run storagedevice discover_namespace $INGEST_3X_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $INGEST_4X_NATIVEGUID 'UNMANAGED_VOLUMES'

    ##Make sure we fail when we ingest the volumes because snaps are not ingested yet
    fail unmanagedvolume ingest_export ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL1}" --host ${XTREMIO_INGEST_HOST}
    fail unmanagedvolume ingest_export ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL2}" --host ${XTREMIO_INGEST_HOST}


    run unmanagedvolume ingest_export ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP1_ARRAY_LABEL}" --host ${XTREMIO_INGEST_HOST}

    run unmanagedvolume ingest_export ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP2_ARRAY_LABEL}" --host ${XTREMIO_INGEST_HOST}

    # Verify the volumes, EG, CG and snaps got created; fail script if this fails.
    run volume show ${PROJECT}/${XIO_INGEST_VOL1}
    run blockconsistencygroup show "${XIO_INGEST_VOL1_CG}"
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL}

    run volume show ${PROJECT}/${XIO_INGEST_VOL2}
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}
    run export_group show  ${PROJECT}/${XTREMIO_INGEST_HOST}

    ## Run block tests for the volumes and snaps
    xtremio_export_test ${export_name}1 ${XIO_INGEST_VOL1} ${XIO_INGEST_VOL2} ${XIO_INGEST_SNAP1_ARRAY_LABEL} ${XIO_INGEST_SNAP2_ARRAY_LABEL}
    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        blockconsistencygroup_restore_snapshot $XIO_INGEST_VOL1_CG $XIO_INGEST_SNAP1_ARRAY_LABEL
    fi

    # Delete the volumes, snaps and CG again
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL} --vipronly
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL} --vipronly

    run_noundo volume delete ${PROJECT}/${XIO_INGEST_VOL1} --vipronly
    run_noundo blockconsistencygroup delete ${XIO_INGEST_VOL1_CG} --vipronly
    run_noundo export_group delete ${PROJECT}/${XTREMIO_INGEST_HOST}
    run_noundo volume delete ${PROJECT}/${XIO_INGEST_VOL2} --vipronly

    # Discover unmanaged volumes on the array again
    run storagedevice discover_namespace $INGEST_3X_NATIVEGUID 'UNMANAGED_VOLUMES'
    run storagedevice discover_namespace $INGEST_4X_NATIVEGUID 'UNMANAGED_VOLUMES'
    
    ##Re-ingest
    ##Make sure we fail when we ingest the snaps because volumes are not ingested yet

    fail unmanagedvolume ingest_export ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP1_ARRAY_LABEL}" --host ${XTREMIO_INGEST_HOST}

    fail unmanagedvolume ingest_export ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_SNAP2_ARRAY_LABEL}" --host ${XTREMIO_INGEST_HOST}

    run unmanagedvolume ingest_export ${NH} ${XTREMIO_4X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL1}" --host ${XTREMIO_INGEST_HOST}

    run unmanagedvolume ingest_export ${NH} ${XTREMIO_3X_COS_FC} $PROJECT --volspec "${XIO_INGEST_VOL2}" --host ${XTREMIO_INGEST_HOST}


    # Verify the volumes, CG and snaps got created; fail script if this fails.
    run volume show ${PROJECT}/${XIO_INGEST_VOL1}
    run blockconsistencygroup show "${XIO_INGEST_VOL1_CG}"
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL}

    run volume show ${PROJECT}/${XIO_INGEST_VOL2}
    run blocksnapshot show ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}
    run export_group show  ${PROJECT}/${XTREMIO_INGEST_HOST}

    ## Run block tests for the volumes and snaps
    xtremio_export_test ${export_name}1 ${XIO_INGEST_VOL1} ${XIO_INGEST_VOL2} ${XIO_INGEST_SNAP1_ARRAY_LABEL} ${XIO_INGEST_SNAP2_ARRAY_LABEL}
    
    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        blockconsistencygroup_restore_snapshot $XIO_INGEST_VOL1_CG $XIO_INGEST_SNAP1_ARRAY_LABEL
    fi

    # Real removal of volume/CG
    run_noundo export_group update ${PROJECT}/${XTREMIO_INGEST_HOST} --remVols "${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL},${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}"
    run_noundo export_group update ${PROJECT}/${XTREMIO_INGEST_HOST} --remVols "${PROJECT}/${XIO_INGEST_VOL1},${PROJECT}/${XIO_INGEST_VOL2}"
    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL1}/${XIO_INGEST_SNAP1_ARRAY_LABEL}
    run_noundo volume delete ${PROJECT}/"${XIO_INGEST_VOL1}" --wait

    run_noundo blocksnapshot delete ${PROJECT}/${XIO_INGEST_VOL2}/${XIO_INGEST_SNAP2_ARRAY_LABEL}
    run_noundo volume delete ${PROJECT}/"${XIO_INGEST_VOL2}" --wait
    run_noundo export_group delete ${PROJECT}/${XTREMIO_INGEST_HOST}

}


xtremio_simulator_setup()
{
    project_setup
    xtremio_varray_setup   
    
    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop controller_discovery_refresh_interval 5
 
    storageprovider create xtremio3x $XIO_SIMULATOR_IP 4430 root root xtremio 
    storageprovider create xtremio4x $XIO_SIMULATOR_IP 4431 root root xtremio
    run networksystem create $FABRIC_SIMULATOR  mds --devip $SIMULATOR_CISCO_MDS_IP --devport 22 --username $RP_FABRIC_SIM_USER --password $RP_FABRIC_SIM_PW
    
    storagedevice discover_all
    storagedevice list    
    run transportzone assign VSAN_11 ${NH}

    # Make sure all of the pools are updated and matched
    run storagedevice discover_all

    run storagepool update $XIO_3X_SIM_NATIVEGUID --nhadd $NH --type block
    run storagepool update $XIO_4X_SIM_NATIVEGUID --nhadd $NH --type block

    
    run storageport update $XIO_3X_SIM_NATIVEGUID FC --addvarrays $NH
    run storageport update $XIO_4X_SIM_NATIVEGUID FC --addvarrays $NH

    #Add the host ports to the network which contians the storage ports
    for i in A1 A2 A3 A4 A5 A6 A7 A8 C1 C2 C3 C4 C5 C6 C7 C8
    do
        wwn=`pwwn $i`
    
        secho "Adding $wwn to zone VSAN_11..."
	run transportzone add VSAN_11 $wwn
    done


    for i in B1 B2 B3 B4 B5 B6 B7 B8 D1 D2 D3 D4 D5 D6 D7 D8
    do
        wwn=`pwwn $i`
        secho "Adding $wwn to zone VSAN_11..."
        run transportzone add VSAN_11 $wwn
    done
    xtremio_cos_setup
    run cos update block $XTREMIO_3X_COS_FC --storage $XIO_3X_SIM_NATIVEGUID
    run cos update block $XTREMIO_4X_COS_FC --storage $XIO_4X_SIM_NATIVEGUID
}

xtremio_common_setup()
{
    project_setup
    xtremio_varray_setup
    xtremio_network_setup
    xtremio_storage_setup
    xtremio_port_setup
    
    
    #Add the host ports to the network which contians the storage ports
    for i in A1 A2 A3 A4 A5 A6 A7 A8 C1 C2 C3 C4 C5 C6 C7 C8
    do
        wwn=`pwwn $i`
    
        secho "Adding $wwn to zone $SRDF_VMAXA_VSAN..."
	run transportzone add $SRDF_VMAXA_VSAN $wwn
    done


    for i in B1 B2 B3 B4 B5 B6 B7 B8 D1 D2 D3 D4 D5 D6 D7 D8
    do
        wwn=`pwwn $i`
        secho "Adding $wwn to zone $SRDF_VMAXA_VSAN..."
        run transportzone add $SRDF_VMAXA_VSAN $wwn
    done
    xtremio_cos_setup
    run cos update block $XTREMIO_3X_COS_FC --storage $XTREMIO_3X_NATIVEGUID
    run cos update block $XTREMIO_4X_COS_FC --storage $XTREMIO_4X_NATIVEGUID
}

xtremio_setup()
{
    if [ "$XIO_QUICK_PARAM" = "quick" ]; then
        secho "XtremIO Simulator setup"
        xtremio_simulator_setup
    else
        secho "XtremIO setup"
        xtremio_common_setup
    fi
    
}

xtremio_tests()
{
   echo 'xtremio tests invoked'
   if [ "$XIO_BLOCK_TESTS" = "1" ]; then
       xtremio_block_tests
       xtremio_blockconsistency_tests
   fi
   # Ingest specific tests
   if [ "$XIO_INGESTTESTS" = "1" ]; then
        secho "XIO ingest volumes"
        xtremio_ingest_test
        secho "XIO ingest tests complete"

        secho "XIO ingest exported volumes"
        xtremio_ingest_export_test
        secho "XIO ingest tests complete"

   fi
}

xtremio_blockconsistency_tests()
{
    # Create Consistency Group
    blockconsistencygroup_create_test
    blockconsistencygroup_show_test
    xtremio_blockconsistencygroup_add_volume_test $CONSISTENCY_GROUP
    xtremio_blockconsistencygroup_snapshot_tests xio-$CONSISTENCY_GROUP $XTREMIO_4X_COS_FC xio-$CONSISTENCY_GROUP_SNAPSHOT
    blockconsistencygroup_bulk_test
    ### Delete volume from consistency group
    echo "Deleting volume from consistency group"
    run volume delete $PROJECT/volume-${CONSISTENCY_GROUP}
}

xtremio_blockconsistencygroup_add_volume_test()
{
    cg_name=$1

    ### Create Volume
    echo "Adding volume to consistency group"
    run volume create  volume-${cg_name} $PROJECT $NH $XTREMIO_4X_COS_FC 1280000000 --consistencyGroup $cg_name

    ### Check that volume is inside the group
    echo "Checking volume is part of consistency group"
    run blockconsistencygroup check_volume  $PROJECT volume-${cg_name} $cg_name --expected

    ### Check that consistencygroup cannot be deleted at this point
    echo "Checking consistency group cannot be deleted with active volumes"
    run blockconsistencygroup delete_with_volumes $cg_name
}

xtremio_blockconsistencygroup_snapshot_tests()
{    
    cg_name=$1
    cg_cos=$2
    cg_snapshot=$3
    
    echo "Running Consistency Group Snapshot tests"
    echo "Consistency Group: $cg_name"
    echo "CoS: $cg_cos"
    echo "CG Snapshot: $cg_snapshot" 
    
    blockconsistencygroup_setup_snapshot $cg_name $cg_cos
    blockconsistencygroup_create_snapshot $cg_name $cg_snapshot
    if [ "$XIO_QUICK_PARAM" != "quick" ]; then
        blockconsistencygroup_restore_snapshot $cg_name $cg_snapshot
    fi
    blockconsistencygroup_show_snapshot $cg_name $cg_snapshot
    blockconsistencygroup_list_snapshot $cg_name $cg_snapshot
    blockconsistencygroup_deactivate_snapshot $cg_name $cg_snapshot

    echo "Deleting volume in snapshot consistency group"
    run volume delete $PROJECT/volume-${cg_name}   
}

xtremio_block_tests()
{
    export_name=$XTREMIOEXPORT_GROUP
    export_host=$XTREMIOEXPORT_GROUP_HOST
    v1=${XTREMIOVOL}1
    cos1=$XTREMIO_3X_COS_FC
    v2=${XTREMIOVOL}2
    cos2=$XTREMIO_4X_COS_FC
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    snap2_label=snap2-${HOSTNAME}-${RANDOM}

    run volume create ${v1} $PROJECT $NH $cos1 $BLK_SIZE --thinVolume true
    run volume create ${v2} $PROJECT $NH $cos2 $BLK_SIZE --thinVolume true
    run blocksnapshot create $PROJECT/${v1} ${snap1_label}
    run blocksnapshot create $PROJECT/${v2} ${snap2_label}


    xtremio_export_test ${export_name}1 ${v1} ${v2} ${snap1_label} ${snap2_label}

    run blocksnapshot restore $snap2
    run volume bulkget

    run blocksnapshot delete $PROJECT/${v1}/${snap1_label}
    run blocksnapshot delete $PROJECT/${v2}/${snap2_label}
    run volume delete $PROJECT/${v1} --wait
    run volume delete $PROJECT/${v2} --wait
}


xtremio_export_test()
{
    vol1=$PROJECT/$2
    vol2=$PROJECT/$3
    snap1=${vol1}/$4
    snap2=${vol2}/$5
    proj=$PROJECT
    tenant=$TENANT
    c=1
    h=1
    expname=$1
    hostname=$hostbase$tenant$c$h
    echo $vol1 $vol2 $proj $tenant $hostname $expname

    exp=$proj/$expname
    nwwn=`nwwn $i$j`
    k=`wwnIdx $c $h`
    pwwn1=`pwwn A$k`
    pwwn2=`pwwn B$k`
    pwwn3=`pwwn C$k`
    pwwn4=`pwwn D$k`

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi blocksnapshots $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group create $proj $expname $NH --volspec "$vol1,$snap1,$snap2" --inits "$hostname/$pwwn1"
    run export_group show $exp
    run export_group update $exp --addVolspec "$vol2" --remVols $vol1
    run export_group show $exp
    run export_group update $exp --remInits "$hostname/$pwwn1"
    run export_group show $exp
    run export_group update $exp --remVols $vol2,$snap1,$snap2
    run export_group show $exp

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi exportgroups $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group delete $exp
}


#
# SRDF
#
srdf_setup()
{
    echo 'SRDF setup'
    srdf_setup_once
    srdf_common_setup
}

srdf_setup_once()
{ 
    login_nd_configure_smtp_nd_add_licenses
    tenant_setup
    run blockconsistencygroup create $SRDF_PROJECT $CONSISTENCY_GROUP_SRDF
    run blockconsistencygroup create $SRDF_PROJECT ${CONSISTENCY_GROUP_SRDF}v3
    networksystem show $BROCADE_NETWORK &> /dev/null && return $?
    networksystem create $BROCADE_NETWORK brocade --smisip $BROCADE_IP --smisport 5988 --smisuser $BROCADE_USER --smispw $BROCADE_PW --smisssl false
    
	# smisprovider show $SRDF_VMAXA_SMIS_DEV &> /dev/null && return $?
    # smisprovider create $SRDF_VMAXA_SMIS_DEV $SRDF_VMAXA_SMIS_IP $SRDF_VMAXA_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $SRDF_VMAXA_SMIS_SSL
    # smisprovider show $SRDF_VMAXB_SMIS_DEV &> /dev/null && return $?
    # smisprovider create $SRDF_VMAXB_SMIS_DEV $SRDF_VMAXB_SMIS_IP $SRDF_VMAXB_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $SRDF_VMAXB_SMIS_SSL
    smisprovider show $SRDF_V3_VMAXA_SMIS_DEV &> /dev/null && return $?
    smisprovider create $SRDF_V3_VMAXA_SMIS_DEV $SRDF_V3_VMAXA_SMIS_IP $SRDF_V3_VMAXA_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $SRDF_V3_VMAXA_SMIS_SSL
    smisprovider show $SRDF_V3_VMAXB_SMIS_DEV &> /dev/null && return $?
    smisprovider create $SRDF_V3_VMAXB_SMIS_DEV $SRDF_V3_VMAXB_SMIS_IP $SRDF_V3_VMAXB_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $SRDF_V3_VMAXB_SMIS_SSL

    # Adding additional array in order to test pool matching.
    # smisprovider show $VMAX_SMIS_DEV &> /dev/null && return $?
    # smisprovider create $VMAX_SMIS_DEV $VMAX_SMIS_IP $VMAX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX_SMIS_SSL
    smisprovider show $VNX_SMIS_DEV &> /dev/null && return $?
    smisprovider create $VNX_SMIS_DEV $VNX_SMIS_IP $VNX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VNX_SMIS_SSL

    storagedevice discover_all --ignore_error
    discoveredsystem show $SRDF_VMAXA_NATIVEGUID &> /dev/null && return $?
    discoveredsystem show $SRDF_VMAXB_NATIVEGUID &> /dev/null && return $?
    discoveredsystem show $SRDF_V3_VMAXA_NATIVEGUID &> /dev/null && return $?
    discoveredsystem show $SRDF_V3_VMAXB_NATIVEGUID &> /dev/null && return $?
}

srdf_common_setup() {

    #zone_setup - for CDP use NH as the protection neighborhood as well
    echo 'Adding endpoints to transport zone'
    run transportzone assign ${SRDF_VMAXA_VSAN} $NH
    # TODO: Try to get rid of this; these should be in the network already.
    for storageport in ${SRDF_VMAXA_STORAGEPORTS}
    do 
      run transportzone add $NH/${SRDF_VMAXA_VSAN} ${storageport}
    done
    echo 'Done adding endpoints to network'
    run transportzone assign ${SRDF_VMAXB_VSAN} $NH
    for storageport in ${SRDF_VMAXB_STORAGEPORTS}
    do 
      run transportzone add ${NH}/${SRDF_VMAXB_VSAN} ${storageport}
    done
    echo 'Done adding endpoints to network'

    project_setup

    echo "Setup ACLs on neighborhood for $TENANT"
    run neighborhood allow $NH $TENANT
    run neighborhood allow $NH $TENANT

    srdf_v3_varray_setup
    # srdf_cos_setup
    srdf_v3_cos_setup
}

srdf_cos_setup()
{
    # Create the target first so it exists when we create the source vpool
    cos show $COS_VMAXBLOCK_SRDF_TARGET block &> /dev/null && return $?
    run cos create block $COS_VMAXBLOCK_SRDF_TARGET		          \
	--description 'Target-Virtual-Pool-for-SRDF-Protection' true \
			 --protocols FC 		          \
			 --numpaths 1				  \
			 --max_snapshots 10 			  \
			 --provisionType 'Thin'	          \
			 --neighborhoods $NH                      \
                         --multiVolumeConsistency                  \
                         --system_type vmax			

    run cos update block $COS_VMAXBLOCK_SRDF_TARGET --storage ${SRDF_VMAXB_NATIVEGUID}
    run cos update block $COS_VMAXBLOCK_SRDF_TARGET --storage ${SRDF_VMAXA_NATIVEGUID}
    run cos allow $COS_VMAXBLOCK_SRDF_TARGET block $TENANT

    cos show ${COS_VMAXBLOCK_SRDF_SOURCE}_sync block &> /dev/null && return $?
    run cos create block ${COS_VMAXBLOCK_SRDF_SOURCE}_sync                 \
	--description 'Source-Virtual-Pool-for-Synchronous-SRDF-Protection' true \
			 --protocols FC 		        \
			 --numpaths 1				\
			 --max_snapshots 10			\
	                 --provisionType 'Thin'	        \
                         --system_type vmax                     \
			 --neighborhoods $NH                    \
			 --srdf "${NH}:${COS_VMAXBLOCK_SRDF_TARGET}:SYNCHRONOUS"

    run cos update block ${COS_VMAXBLOCK_SRDF_SOURCE}_sync --storage ${SRDF_VMAXA_NATIVEGUID}
    run cos update block ${COS_VMAXBLOCK_SRDF_SOURCE}_sync --storage ${SRDF_VMAXB_NATIVEGUID}
    run cos allow ${COS_VMAXBLOCK_SRDF_SOURCE}_sync block $TENANT

    cos show ${COS_VMAXBLOCK_SRDF_SOURCE}_async block &> /dev/null && return $?
    run cos create block ${COS_VMAXBLOCK_SRDF_SOURCE}_async                 \
	--description 'Source-Virtual-Pool-for-Asynchronous-SRDF-Protection' true \
			 --protocols FC 		        \
			 --numpaths 1				\
			 --max_snapshots 10			\
	                 --provisionType 'Thin'	        \
                         --system_type vmax                     \
			 --neighborhoods $NH                    \
                         --multiVolumeConsistency               \
			 --srdf "${NH}:${COS_VMAXBLOCK_SRDF_TARGET}:ASYNCHRONOUS"

    run cos update block ${COS_VMAXBLOCK_SRDF_SOURCE}_async --storage ${SRDF_VMAXA_NATIVEGUID}
    run cos update block ${COS_VMAXBLOCK_SRDF_SOURCE}_async --storage ${SRDF_VMAXB_NATIVEGUID}
    run cos allow ${COS_VMAXBLOCK_SRDF_SOURCE}_async block $TENANT
}

srdf_v3_varray_setup()
{
    secho 'SRDF V3 varray setup'

    # Create Virtual Array
    run neighborhood create $V3_SRDF_VARRAY
    run neighborhood allow $V3_SRDF_VARRAY $TENANT
    
    if [ "$SRDF_V3_VMAXA_NATIVEGUID" != "NONE" ]; then
        storageport update $SRDF_V3_VMAXA_NATIVEGUID FC --addvarrays $V3_SRDF_VARRAY
    fi
    
    if [ "$SRDF_V3_VMAXB_NATIVEGUID" != "NONE" ]; then
        storageport update $SRDF_V3_VMAXB_NATIVEGUID FC --addvarrays $V3_SRDF_VARRAY
    fi
}

srdf_v3_cos_setup()
{
    secho "SRDF V3 Virtual Pool setup" 
    # Create the target first so it exists when we create the source vpool
    cos show $COS_VMAXBLOCK_V3_SRDF_TARGET block &> /dev/null && return $?
    run cos create block $COS_VMAXBLOCK_V3_SRDF_TARGET		          \
	--description 'Target-Virtual-Pool-for-V3-SRDF-Protection' true \
			 --protocols FC 		          \
			 --numpaths 1				  \
			 --max_snapshots 10 			  \
			 --provisionType 'Thin'	          \
			 --neighborhoods $V3_SRDF_VARRAY                      \
                         --multiVolumeConsistency                  \
                         --system_type vmax			
    
    run cos update block $COS_VMAXBLOCK_V3_SRDF_TARGET --storage ${SRDF_V3_VMAXB_NATIVEGUID}
    run cos update block $COS_VMAXBLOCK_V3_SRDF_TARGET --storage ${SRDF_V3_VMAXA_NATIVEGUID}
    run cos allow $COS_VMAXBLOCK_V3_SRDF_TARGET block $TENANT
    
    cos show ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active block &> /dev/null && return $?
    run cos create block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active                 \
	--description 'Source-Virtual-Pool-for-Active-SRDF-Protection' true \
			 --protocols FC 		        \
			 --numpaths 1				\
			 --max_snapshots 10			\
	                 --provisionType 'Thin'	        \
                         --system_type vmax                     \
                         --multiVolumeConsistency		\
			 --neighborhoods $V3_SRDF_VARRAY                    \
			 --srdf "${V3_SRDF_VARRAY}:${COS_VMAXBLOCK_V3_SRDF_TARGET}:ACTIVE"

    run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active --storage ${SRDF_V3_VMAXA_NATIVEGUID}
    run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active --storage ${SRDF_V3_VMAXB_NATIVEGUID}
    run cos allow ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active block $TENANT 
	
	
	## ASYNCHRONOUS
	
	cos show ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async block &> /dev/null && return $?
    run cos create block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async                 \
	--description 'Source-Virtual-Pool-for-Async-SRDF-Protection' true \
			 --protocols FC 		        \
			 --numpaths 1				\
			 --max_snapshots 10			\
	                 --provisionType 'Thin'	        \
                         --system_type vmax                     \
                         --multiVolumeConsistency		\
			 --neighborhoods $V3_SRDF_VARRAY                    \
			 --srdf "${V3_SRDF_VARRAY}:${COS_VMAXBLOCK_V3_SRDF_TARGET}:ASYNCHRONOUS"

    run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async --storage ${SRDF_V3_VMAXA_NATIVEGUID}
    run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async --storage ${SRDF_V3_VMAXB_NATIVEGUID}
    run cos allow ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async block $TENANT 
	
	## SYNCHRONOUS
	
	cos show ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync block &> /dev/null && return $?
    run cos create block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync                 \
	--description 'Source-Virtual-Pool-for-Sync-SRDF-Protection' true \
			 --protocols FC 		        \
			 --numpaths 1				\
			 --max_snapshots 10			\
	                 --provisionType 'Thin'	        \
                         --system_type vmax                     \
                         --multiVolumeConsistency		\
			 --neighborhoods $V3_SRDF_VARRAY                    \
			 --srdf "${V3_SRDF_VARRAY}:${COS_VMAXBLOCK_V3_SRDF_TARGET}:SYNCHRONOUS"

    run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync --storage ${SRDF_V3_VMAXA_NATIVEGUID}
    run cos update block ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync --storage ${SRDF_V3_VMAXB_NATIVEGUID}
    run cos allow ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync block $TENANT 
	
	
}

srdf_cos_matcher_test()
{
    # Match various pool criterias, ensure SRDF vpools produce the right pools
    echo "BEGIN: Virtual Pool matching test: right now these tests are visually verified!"
    run cos match block true --protocols FC 		        \
			 --numpaths 1				\
			 --max_snapshots 10			\
	                 --provisionType 'Thin'	        \
			 --neighborhoods $NH                    \
                         --multiVolumeConsistency               \
			 --srdf "${NH}:${COS_VMAXBLOCK_V3_SRDF_TARGET}:ASYNCHRONOUS"
    #expectN=`grep "name" /tmp/file.txt | wc -l`
    #if [ $expectN -eq 3 ]
    #then
    #   echo "Expected 3 storage pools, got $expectN instead"
    #   fail;
    #fi   
    echo "END: Virtual Pool matching test: right now these tests are visually verified!"
}

srdf_cos_change()
{
   # Create non-protected volume, then move it to a cos that has protection
   srdfvolumecc="${SRDF_VOLUME}-coschange"
   run volume create ${srdfvolumecc} $PROJECT $NH $COS_VMAXBLOCK_V3_SRDF_TARGET 1GB
   echo "Created volume, now sleeping..."
   sleep 60
   echo "Changing cos..."
   run volume change_cos $PROJECT/${srdfvolumecc} ${COS_VMAXBLOCK_V3_SRDF_TARGET}_sync
   echo "Changed cos"
   sleep 60
   echo "Deleting volume"
   run volume delete $PROJECT/${srdfvolumecc} --wait
}

srdf_v3_cos_change()
{
   # Create non-protected volume, then move it to a cos that has protection
   srdfvolumecc="${SRDF_VOLUME}-coschange"
   run volume create ${srdfvolumecc} $PROJECT $V3_SRDF_VARRAY $COS_VMAXBLOCK_V3_SRDF_TARGET 1GB
   echo "Created volume, now sleeping..."
   sleep 60
   echo "Changing cos..."
   run volume change_cos $PROJECT/${srdfvolumecc} ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active
   echo "Changed cos"
   sleep 60
   echo "Deleting volume"
   run volume delete $PROJECT/${srdfvolumecc} --wait
}

srdf_cos_change_meta()
{
   # Create non-protected meta volume, then move it to a cos that has protection
   srdfvolumeccmeta="${SRDF_VOLUME}-coschangemeta"
   run volume create ${srdfvolumeccmeta} $SRDF_PROJECT $NH $COS_VMAXBLOCK_SRDF_TARGET 300GB
   echo "Created volume, now sleeping..."
   sleep 60
   echo "Changing cos..."
   run volume change_cos $PROJECT/${srdfvolumeccmeta} ${COS_VMAXBLOCK_SRDF_SOURCE}_sync
   echo "Changed cos"
   sleep 60
   echo "Deleting volume"
   run volume delete $SRDF_PROJECT/${srdfvolumeccmeta} --wait
}



srdf_basic_sync_test()
{
   srdfvolume="${SRDF_VOLUME}_sync"
   run volume create ${srdfvolume}1 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync 1GB
   sleep 300
   #run volume create ${srdfvolume}2 $PROJECT $NH ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync 1GB --consistencygroup $CONSISTENCY_GROUP_SRDF
   #run volume create ${srdfvolume}3 $PROJECT $NH ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync 1GB --count 2
   #run volume delete $PROJECT/${srdfvolume}2 --wait
   run volume delete $SRDF_PROJECT/${srdfvolume}1 --wait
   
   run volume create ${srdfvolume}1 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_sync 1GB --consistencyGroup ${CONSISTENCY_GROUP_SRDF}
   run volume delete $SRDF_PROJECT/${srdfvolume}1 --wait
   #run volume delete $PROJECT/${srdfvolume}3-1 --wait
   #run volume delete $PROJECT/${srdfvolume}3-2 --wait
}

srdf_basic_async_test()
{
   srdfvolume="${SRDF_VOLUME}_async"
   run volume create ${srdfvolume}1 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async 1GB --consistencyGroup ${CONSISTENCY_GROUP_SRDF}
   run volume create ${srdfvolume}2 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async 1GB --consistencyGroup ${CONSISTENCY_GROUP_SRDF}
   echo "Created 2 volumes... now going to sleep..."
   sleep 60
   echo "Deleting volumes..."
   #run volume create ${srdfvolume}3 $PROJECT $NH ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_async 1GB --count 2
   run volume delete $SRDF_PROJECT/${srdfvolume}2 --wait
   run volume delete $SRDF_PROJECT/${srdfvolume}1 --wait
   #run volume delete $PROJECT/${srdfvolume}3-1 --wait
   #run volume delete $PROJECT/${srdfvolume}3-2 --wait
}

srdf_basic_active_test()
{
   srdfvolume="${SRDF_VOLUME}_active"
   run volume create ${srdfvolume}1 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active 1GB
   run volume create ${srdfvolume}2 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active 1GB
   sleep 60
   run volume delete $SRDF_PROJECT/${srdfvolume}1 --wait
   run volume delete $SRDF_PROJECT/${srdfvolume}2 --wait

   # rediscover storage systems for RDF group mode getting updated
   echo "Rediscovering storage systems"
   storagedevice discover_all --ignore_error
   run volume create ${srdfvolume}3 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active 1GB --consistencyGroup ${CONSISTENCY_GROUP_SRDF}v3
   run volume create ${srdfvolume}4 $SRDF_PROJECT $V3_SRDF_VARRAY ${COS_VMAXBLOCK_V3_SRDF_SOURCE}_active 1GB --consistencyGroup ${CONSISTENCY_GROUP_SRDF}v3
   run volume delete $SRDF_PROJECT/${srdfvolume}3 --wait
   run volume delete $SRDF_PROJECT/${srdfvolume}4 --wait
}

srdf_tests()
{
   echo 'Run SRDF tests'
   echo 'About to run cos matcher test'
   srdf_cos_matcher_test
   
   echo 'About to run basic SRDF active test'
   sleep 60
   srdf_basic_active_test
   echo 'About to run V3 SRDF cos change test'
   sleep 60
   srdf_v3_cos_change
   
   echo 'About to run basic SRDF async test'
   sleep 60
   srdf_basic_async_test
   echo 'About to run basic SRDF sync test'
   sleep 60
   srdf_basic_sync_test
   
   echo 'About to run SRDF cos change test'
   sleep 60
   srdf_cos_change
   # echo 'About to run SRDF cos change test for meta volume'
   # sleep 60
   # srdf_cos_change_meta
   echo 'Done Running SRDF tests'
}

vmax_cos_setup()
{
    if [ $QUICK -eq 0 ]; then
       run cos create block $COS_VMAXBLOCK				\
	   --description 'Virtual-Pool-for-VMAX-block-FC+iSCSI' true 	\
                         --protocols FC iSCSI 			\
                         --numpaths 1 \
                         --max_snapshots 10 \
                         --provisionType 'Thin' \
	                 --system_type vmax \
                         --expandable true \
                         --neighborhoods $NH
    else
       run cos create block $COS_VMAXBLOCK                          \
	   --description 'Virtual-Pool-for-VMAX-block-FC+iSCSI' true       \
                         --protocols FC                   \
                         --numpaths 2 \
                         --max_snapshots 10 \
                         --provisionType 'Thin' \
                         --system_type vmax \
                         --expandable true \
                         --neighborhoods $NH
    fi

    run cos create block $COS_VMAXBLOCK_FC				\
	--description 'Virtual-Pool-for-VMAX-block-FC' true 	\
                         --protocols FC 			\
                         --numpaths 2 \
                         --max_snapshots 10 \
	                 --system_type vmax \
                         --provisionType 'Thin' \
			 --neighborhoods $NH

    run cos create block $COS_VMAXBLOCK_ISCSI			\
	--description 'Virtual-Pool-for-VMAX-block-iSCSI' true \
                         --protocols iSCSI 			\
                         --numpaths 1 \
                         --max_snapshots 10 \
	                 --system_type vmax \
                         --provisionType 'Thin' \
			 --neighborhoods $NH

    run cos create block $COS_VMAXBLOCK_THIN 				\
	--description 'VMAX-thin-storage' true      \
                             --protocols FC iSCSI	    \
                             --numpaths 1 \
                             --max_snapshots 10 \
	                     --system_type vmax \
                             --provisionType 'Thin' \
                             --expandable true \
                         --neighborhoods $NH

    run cos create block $COS_VMAXBLOCK_THICK 				\
	--description 'VMAX-thick-storage' true      \
                             --protocols FC iSCSI	    \
                             --numpaths 1  \
                             --max_snapshots 10 \
	                     --system_type vmax \
                             --provisionType 'Thick' \
			 --neighborhoods $NH
}

mirrorblock_cos_setup()
{
    run cos create block $COS_MIRROR                       \
	--description 'VMAX-Mirror-block-FC' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --max_mirrors 10           \
                            --provisionType 'Thin'     \
	                    --system_type vmax \
                            --expandable false \
			    --neighborhoods $NH

    run cos create block $COS_MIRROR_WITH_OPTIONAL         \
	--description 'VMAX-Mirror-block-FC-with-optional-CoS' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --max_mirrors 1            \
                            --provisionType 'Thin'     \
                            --mirror_cos $COS_VMAXBLOCK_FC   \
	                    --system_type vmax \
                            --expandable false \
			    --neighborhoods $NH

    run cos create block $COS_MIRROR_WITH_2_MIRRORS        \
	--description 'VMAX-Mirror-block-FC-with-2-mirrors-maximum' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --max_mirrors 2            \
                            --provisionType 'Thin'     \
	                    --system_type vmax \
                            --expandable false \
			    --neighborhoods $NH

    run cos create block $COS_MIRROR_BEFORE_CHANGE         \
	--description 'VMAX-block-FC-with-no-mirrors-explicitly' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --max_mirrors 0            \
                            --provisionType 'Thin'     \
	                    --system_type vmax \
                            --expandable false \
			    --neighborhoods $NH

    run cos create block $COS_MIRROR_AFTER_CHANGE          \
	--description 'VMAX-block-FC-with-1-mirror-explicitly' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --max_mirrors 1            \
                            --provisionType 'Thin'     \
	                    --system_type vmax \
                            --expandable false \
			    --neighborhoods $NH

    run cos create block $COS_MIRROR_VNX                   \
	--description 'VNX-block-mirror' true 	\
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --max_mirrors 3            \
                            --provisionType 'Thin'     \
	                    --system_type vnxblock \
                            --expandable false \
			    --neighborhoods $NH

    run cos create block $COS_VMAX_CG_MIRROR \
	--description 'Virtual-Pool-for-VMAX-block-FC' true \
                            --protocols FC \
                            --numpaths 2 \
                            --max_snapshots 10 \
                            --max_mirrors 10 \
                            --system_type vmax \
                            --expandable false \
                            --provisionType 'Thin' \
                            --neighborhoods $NH \
                            --multiVolumeConsistency
}

consistencygroup_block_cos_setup()
{
    # Create CoS for VNX
    run cos create block $VNX_COS_GROUP                       \
	--description 'Consistency-Group-Block-VNX-CoS' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
			    --neighborhoods $NH        \
                            --multiVolumeConsistency   

    run cos update block $VNX_COS_GROUP --storage $VNXB_NATIVEGUID
    run cos allow $VNX_COS_GROUP block $TENANT

    # Create CoS for VMAX
    run cos create block $VMAX_COS_GROUP                       \
	--description 'Consistency-Group-Block-VMAX-CoS' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
			    --neighborhoods $NH                \
                            --multiVolumeConsistency

    run cos update block $VMAX_COS_GROUP --storage $VMAX_NATIVEGUID
    run cos allow $VMAX_COS_GROUP block $TENANT

    # Create CoS without the MultiVolumeConsistency attribute
    COS_GROUP_INVALID=cos-group-no-multivolumeconsistency"$date"
    run cos create block $COS_GROUP_INVALID                       \
	--description 'Consistency-Group-Block-VirtualPool-with-no-multiVolumeConsistency' true   \
                            --protocols FC iSCSI       \
                            --numpaths 1               \
                            --max_snapshots 10         \
                            --provisionType 'Thin'     \
			    --neighborhoods $NH
}

vplex_cos_setup()
{
    run cos create block cosvplexlocal true                   \
                     --description 'Local-CoS-for-VPlex'      \
                     --protocols FC                           \
                     --numpaths 2                             \
                     --provisionType 'Thin'                   \
                     --highavailability vplex_local           \
                     --neighborhoods $NH $NH2                 \
                     --max_snapshots 1                        \
                     --max_mirrors 1                          \
                     --expandable false 

    run cos allow cosvplexlocal block $TENANT
    if [ "${VPLEX_QUICK_PARAM}" = "quick" ]; then
	run cos update block cosvplexlocal --storage $VPLEX_SIM_VMAX1_NATIVEGUID
	run cos update block cosvplexlocal --storage $VPLEX_SIM_VMAX2_NATIVEGUID
	run cos update block cosvplexlocal --storage $VPLEX_SIM_VMAX3_NATIVEGUID
    else
	run cos update block cosvplexlocal --storage $VPLEX_VNX1_NATIVEGUID
	if [ "$AUTH" != 'ipv6' ] ; then
	    run cos update block cosvplexlocal --storage $VPLEX_VNX2_NATIVEGUID
	fi
	run cos update block cosvplexlocal --storage $VPLEX_VMAX_NATIVEGUID
    fi

    run cos create block cosvplexdist true                          \
                     --description 'Distributed-CoS-for-VPlex'      \
                     --protocols FC                                 \
                     --numpaths 2                                   \
                     --provisionType 'Thin'                         \
                     --highavailability vplex_distributed           \
                     --neighborhoods $NH $NH2                       \
                     --haNeighborhood $NH2                          \
                     --max_snapshots 1                              \
                     --max_mirrors 1                                \
                     --mirror_cos cosvplexlocal                     \
                     --expandable false

    run cos allow cosvplexdist block $TENANT
    if [ "${VPLEX_QUICK_PARAM}" = "quick" ]; then
	run cos update block cosvplexdist --storage $VPLEX_SIM_VMAX1_NATIVEGUID
	run cos update block cosvplexdist --storage $VPLEX_SIM_VMAX2_NATIVEGUID
	run cos update block cosvplexdist --storage $VPLEX_SIM_VMAX3_NATIVEGUID
    else
	run cos update block cosvplexdist --storage $VPLEX_VNX1_NATIVEGUID
	run cos update block cosvplexdist --storage $VPLEX_VMAX_NATIVEGUID
    fi
}

vplex_setup()
{
    vplex_setup_once
}

vplex_setup_once()
{
    if [ "$AUTH" == 'ipv6' ] ; then
	echo "Setting up VPLEX environment for IPv6"
	# Discover the storage systems 
	smisprovider create $VPLEX_VMAX_SMIS_DEV_NAME $VPLEX_VMAX_SMIS_IP $VPLEX_VMAX_SMIS_PORT $VPLEX_SMIS_USER "$VPLEX_SMIS_PASSWD" $VPLEX_VMAX_SMIS_SSL
	storageprovider create $VPLEX_DEV_NAME $VPLEX_IP 443 $VPLEX_USER "$VPLEX_PASSWD" vplex
	storagedevice discover_all
	storagedevice list
	storageport list $VPLEX_VNX1_NATIVEGUID
	storageport list $VPLEX_VMAX_NATIVEGUID
	storageport list $VPLEX_GUID

	# Setup the varrays. $NH contains VPLEX cluster-1 and $NH2 contains VPLEX cluster-2.
	storageport update $VPLEX_GUID FC --group director-1-1-A --addvarrays $NH
	storageport update $VPLEX_GUID FC --group director-1-1-B --addvarrays $NH
	storageport update $VPLEX_GUID FC --group director-2-1-A --addvarrays $NH2
	storageport update $VPLEX_GUID FC --group director-2-1-B --addvarrays $NH2
	# The arrays are assigned to individual varrays as well.
	storageport update $VPLEX_VNX1_NATIVEGUID FC --addvarrays $NH
	storageport update $VPLEX_VMAX_NATIVEGUID FC --addvarrays $NH2        
    elif [ "${VPLEX_QUICK_PARAM}" = "quick" ]; then
	echo "Setting up VPLEX environment connected to simulators on: ${VPLEX_SIM_IP}"

	# Discover the Brocade SAN switch.
	echo "Configuring MDS/Cisco Simulator using SSH on: $VPLEX_SIM_MDS_IP"
	run networksystem create $FABRIC_SIMULATOR  mds --devip $VPLEX_SIM_MDS_IP --devport 22 --username $VPLEX_SIM_MDS_USER --password $VPLEX_SIM_MDS_PW
	transportzone listall

	# Discover the storage systems 
	echo "Discovering back-end storage arrays using ECOM/SMIS simulator on: $VPLEX_SIM_SMIS_IP..."
	smisprovider show $VPLEX_SIM_SMIS_DEV_NAME &> /dev/null && return $?
	run smisprovider create $VPLEX_SIM_SMIS_DEV_NAME $VPLEX_SIM_SMIS_IP $VPLEX_VMAX_SMIS_SIM_PORT $VPLEX_SIM_SMIS_USER "$VPLEX_SIM_SMIS_PASSWD" false

	echo "Discovering VPLEX using simulator on: ${VPLEX_SIM_IP}..."
	storageprovider show $VPLEX_SIM_DEV_NAME &> /dev/null && return $?
	run storageprovider create $VPLEX_SIM_DEV_NAME $VPLEX_SIM_IP 443 $VPLEX_SIM_USER "$VPLEX_SIM_PASSWD" vplex
	run storagedevice discover_all

	# Setup the varrays. $NH contains VPLEX cluster-1 and $NH2 contains VPLEX cluster-2.
	run storageport update $VPLEX_SIM_VPLEX_GUID FC --group director-1-1-A --addvarrays $NH
	run storageport update $VPLEX_SIM_VPLEX_GUID FC --group director-1-1-B --addvarrays $NH
	run storageport update $VPLEX_SIM_VPLEX_GUID FC --group director-2-1-A --addvarrays $NH2
	run storageport update $VPLEX_SIM_VPLEX_GUID FC --group director-2-1-B --addvarrays $NH2
	# The arrays are assigned to individual varrays as well.
	run storageport update $VPLEX_SIM_VMAX1_NATIVEGUID FC --addvarrays $NH
	run storageport update $VPLEX_SIM_VMAX3_NATIVEGUID FC --addvarrays $NH
	run storageport update $VPLEX_SIM_VMAX2_NATIVEGUID FC --addvarrays $NH2
	vplex_cos_setup
	VPLEX_GUID=$VPLEX_SIM_VPLEX_GUID
	CLUSTER1NET_NAME=$CLUSTER1NET_SIM_NAME
        echo "Done setting up VPLEX environment..."
    else
	echo "Setting up VPLEX environment for IPv4"

	#Discover the Brocade SAN switch.
	brocade_setup_once

	# Discover the storage systems 
	echo "Discovering Storage Assets"
	smisprovider show $VPLEX_VMAX_SMIS_DEV_NAME &> /dev/null && return $?
	run smisprovider create $VPLEX_VMAX_SMIS_DEV_NAME $VPLEX_VMAX_SMIS_IP $VPLEX_VMAX_SMIS_PORT $VPLEX_SMIS_USER "$VPLEX_SMIS_PASSWD" $VPLEX_VMAX_SMIS_SSL
	run smisprovider create $VPLEX_VNX1_SMIS_DEV_NAME $VPLEX_VNX1_SMIS_IP $VPLEX_VNX1_SMIS_PORT $VPLEX_SMIS_USER "$VPLEX_SMIS_PASSWD" $VPLEX_VNX1_SMIS_SSL
	run smisprovider create $VPLEX_VNX2_SMIS_DEV_NAME $VPLEX_VNX2_SMIS_IP $VPLEX_VNX2_SMIS_PORT $VPLEX_SMIS_USER "$VPLEX_SMIS_PASSWD" $VPLEX_VNX2_SMIS_SSL
	storageprovider show $VPLEX_DEV_NAME &> /dev/null && return $?
	run storageprovider create $VPLEX_DEV_NAME $VPLEX_IP 443 $VPLEX_USER "$VPLEX_PASSWD" vplex
	run storagedevice discover_all
	storagedevice list
	storageport list $VPLEX_VNX1_NATIVEGUID --v
	storageport list $VPLEX_VMAX_NATIVEGUID --v
	storageport list $VPLEX_VNX2_NATIVEGUID --v
	storageport list $VPLEX_GUID --v
	sleep 90
	storageport list $VPLEX_GUID --v

	# Setup the varrays. $NH contains VPLEX cluster-1 and $NH2 contains VPLEX cluster-2.
	run storageport update $VPLEX_GUID FC --group director-1-1-A --addvarrays $NH
	run storageport update $VPLEX_GUID FC --group director-1-1-B --addvarrays $NH
	run storageport update $VPLEX_GUID FC --group director-2-1-A --addvarrays $NH2
	run storageport update $VPLEX_GUID FC --group director-2-1-B --addvarrays $NH2
	storageport list $VPLEX_GUID --v
	# The arrays are assigned to individual varrays as well.
	run storageport update $VPLEX_VNX1_NATIVEGUID FC --addvarrays $NH
	run storageport update $VPLEX_VNX2_NATIVEGUID FC --addvarrays $NH
	run storageport update $VPLEX_VMAX_NATIVEGUID FC --addvarrays $NH2
	vplex_cos_setup
	storageport list $VPLEX_GUID --v
    fi
}

#
# vplex tests
#
vplex_tests()
{
    storageport list $VPLEX_GUID --v
    if [ "$AUTH" != 'ipv6' ] ; then
	echo "**** Executing VPLEX tests"

	hname=$(hostname)
	if [ $hname = "standalone" ]; then
	    hname=$SHORTENED_HOST
	fi
	echo "hostname is $hname"
	
	localVolume1=$hname-${RANDOM}-VPlexLocal1
	localVolume2=$hname-${RANDOM}-VPlexLocal2
	localMirror1=$hname-${RANDOM}-VPlexLocalMirror1
	localMirror2=$hname-${RANDOM}-VPlexLocalMirror2
	sourceSideSuffix=-0
	localSnapshot=$hname-${RANDOM}-VPlexLocalSnap
	distVolume1=$hname-${RANDOM}-VPlexDist1
	distSrcLocalMirror1=$hname-${RANDOM}-srcLocalMirror1
	distSnapshot=$hname-${RANDOM}-VPlexDist1Snap
	distVolume2=$hname-${RANDOM}-VPlexDist2
	distSrcLocalMirror2=$hname-${RANDOM}-srcLocalMirror2
	host=$PROJECT.lss.emc.com
	hostLbl=$PROJECT
	PWWN1=10:00:00:E0:7E:00:00:0F
	WWNN1=20:00:00:E0:7E:00:00:0F
	PWWN2=10:00:00:90:FA:18:0E:99
	WWNN2=20:00:00:90:FA:18:0E:99

	echo "**** Creating VPLEX local volumes"
	run volume create $localVolume1 $PROJECT $NH cosvplexlocal $BLK_SIZE
	run volume create $localVolume2 $PROJECT $NH cosvplexlocal $BLK_SIZE --count=2
	
	echo "**** Creating VPLEX local mirrors"
	run blockmirror attach  $PROJECT/$localVolume1 $localMirror1 1
	run blockmirror attach  $PROJECT/$localVolume2-1 $localMirror2 1

	echo "**** Deleting VPLEX local mirror, waiting 20 seconds first..."
	sleep 20
	run blockmirror deactivate  $PROJECT/$localVolume2-1 $localMirror2$sourceSideSuffix

	echo "**** Creating VPLEX distributed volumes"
	run volume create $distVolume1 $PROJECT $NH cosvplexdist $BLK_SIZE
	run volume create $distVolume2 $PROJECT $NH cosvplexdist $BLK_SIZE

	echo "**** Creating local mirrors for the Distributed volumes on the source side"
	run blockmirror attach  $PROJECT/$distVolume1 $distSrcLocalMirror1 1
	run blockmirror attach  $PROJECT/$distVolume2 $distSrcLocalMirror2 1
	
	echo "**** Deleting local mirror for the VPLEX Distributed volume, waiting 20 seconds first..."
	sleep 20
	run blockmirror deactivate  $PROJECT/$distVolume2 $distSrcLocalMirror2$sourceSideSuffix
	
	echo "**** Volumes created so far, before export testing starts:"
	run volume list $PROJECT
	
	echo "**** Creating host"
	run hosts create $hostLbl $TENANT Windows $host --port 8111 --username user --password 'password' --osversion 1.0
	
	echo "**** Creating initiators"
	run initiator create $hostLbl FC $PWWN1 --node $WWNN1
	run initiator create $hostLbl FC $PWWN2 --node $WWNN2
	
	echo "**** Adding WWNs to Network"
	run transportzone add ${NH}/${CLUSTER1NET_NAME} $PWWN1
	run transportzone add ${NH}/${CLUSTER1NET_NAME} $PWWN2
	
	echo "**** Exporting VPLEX volumes to host in varray " $NH
	run export_group create $PROJECT $hname-1$host $NH --volspec "$PROJECT/$distVolume1" --inits "$hostLbl/$PWWN1","$hostLbl/$PWWN2"
	run export_group update $PROJECT/$hname-1$host --addVolspec $PROJECT/$distVolume2
	run export_group update $PROJECT/$hname-1$host --addVolspec $PROJECT/$localVolume1
	run export_group update $PROJECT/$hname-1$host --addVolspec $PROJECT/$localVolume2-1
	
	# echo "**** Exporting VPLEX volumes to host in varray " $NH2
        # run export_group create $PROJECT $hname-2$host $NH2 --volspec "$PROJECT/$distVolume1+1" --inits "$hostLbl/$PWWN2"
	echo "Exports for $distVolume1"
	volume exports $PROJECT/$distVolume1 --v
	echo "Exports for $distVolume2"
	volume exports $PROJECT/$distVolume2 --v
	echo "Exports for $localVolume1"
	volume exports $PROJECT/$localVolume1 --v
	echo "Exports for $localVolume2-1"
	volume exports $PROJECT/$localVolume1 --v
	
	echo "**** Creating VPLEX volume snapshots"
	run blocksnapshot create $PROJECT/$localVolume1 $localSnapshot
	blocksnapshot list $PROJECT/$localVolume1
	blocksnapshot show $PROJECT/$localVolume1/$localSnapshot
	run blocksnapshot create $PROJECT/$distVolume1 $distSnapshot
	blocksnapshot list $PROJECT/$distVolume1
	blocksnapshot show $PROJECT/$distVolume1/$distSnapshot

	echo "**** Restoring VPLEX volume snapshots"
        # Must deactivate mirrors before restoring snapshots now.
	run blockmirror deactivate  $PROJECT/$localVolume1 $localMirror1$sourceSideSuffix
	run blocksnapshot restore $PROJECT/$localVolume1/$localSnapshot
	run blockmirror deactivate  $PROJECT/$distVolume1 $distSrcLocalMirror1$sourceSideSuffix
	run blocksnapshot restore $PROJECT/$distVolume1/$distSnapshot

	echo "**** Deleting VPLEX volume snapshots"
	run blocksnapshot delete $PROJECT/$localVolume1/$localSnapshot
	run blocksnapshot delete $PROJECT/$distVolume1/$distSnapshot

	echo "**** Deleting VPLEX exports"
	run export_group delete $PROJECT/$hname-1$host
	# run export_group delete $PROJECT/$hname-2$host

	echo "**** Deleting VPLEX volumes"
	run volume delete $PROJECT/$localVolume1 --wait
	run volume delete $PROJECT/$localVolume2-1 --wait
        run volume delete $PROJECT/$localVolume2-2 --wait

	run volume delete $PROJECT/$distVolume1 --wait
	run volume delete $PROJECT/$distVolume2 --wait

	echo "**** Deleting Host"
	hosts delete $hostLbl

	echo "**** Completed VPLEX Tests"
    fi
}

# Run vplexexport test externally
vplexexport_setup() {
    vplex_setup_once
}
vplexexport_tests() {
    echo "************* Running export-tests/vplexexport.sh  *****************"
    run export-tests/vplexexport.sh test1
}

sanzoning_setup()
{
    sanzoning_setup_once
}

sanzoning_setup_once()
{
    vmaxblock_discovery
    vnxblock_discovery
}

# Sanzoning Tests
sanzoning_tests()
{
    echo "************* Running export-tests/sanzoning.sh  *****************"
#    run export-tests/sanzoning.sh addvolumezonecheck
#   run export-tests/sanzoning.sh cleanup
    run export-tests/sanzoning.sh sanzonereuse
}

vmaxblock_discovery()
{
    # do this only once
    smisprovider show $VMAX_SMIS_DEV &> /dev/null && return $?

    if [ $QUICK -eq 0 ]; then
       smisprovider create $VMAX_SMIS_DEV $VMAX_SMIS_IP $VMAX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX_SMIS_SSL
    fi

    storagedevice discover_all --ignore_error
}

vnxblock_discovery()
{
    # Discover VNX block array
    smisprovider show $VNX_SMIS_DEV &> /dev/null && return $?

    if [ $QUICK -eq 0 ]; then
       smisprovider create $VNX_SMIS_DEV $VNX_SMIS_IP $VNX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VNX_SMIS_SSL
    else
       smisprovider create $VNX_SMIS_DEV $SIMULATOR_SMIS_IP 5988 $SMIS_USER "$SMIS_PASSWD" false
    fi

    storagedevice discover_all --ignore_error
}

######################### Start of RecoverPoint ############################
#
# RecoverPoint can be run in a simulated and physical environment.
#

#########################
# RecoverPoint Vpool create
#########################

# All varrays for regular targets
rp_targets()
{
    run cos create block rp_targets$1 $POOLS_AUTO_MATCH \
        --description 'RP-Targets' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --multiVolumeConsistency \
        --neighborhoods $RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3 $RECOVERPOINT_VARRAY4 \
        --max_snapshots 10
}

rpxio_targets()
{
    run cos create block rpxio_targets$1 $POOLS_AUTO_MATCH \
        --description 'RP-XIO-Targets' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --multiVolumeConsistency \
        --neighborhoods $RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3 $RECOVERPOINT_VARRAY4 \
        --max_snapshots 10
}

# All varrays for RP+VPLEX targets
rpvplex_targets()
{
    run cos create block rpvplex_targets$1 $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Targets' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3 $RECOVERPOINT_VARRAY4 \
        --max_snapshots 10 \
        --highavailability vplex_local
}

# All varrays for HA
rpvplex_ha()
{
    run cos create block rpvplex_ha$1 $POOLS_AUTO_MATCH \
        --description 'HA' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --multiVolumeConsistency \
        --neighborhoods $RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3 $RECOVERPOINT_VARRAY4 \
        --max_snapshots 10 \
        --highavailability vplex_local
}

# Regular vpool used as base for change vpool tests
rp_regular()
{
    run cos create block rp_regular$1 $POOLS_AUTO_MATCH \
        --description 'Regular-vpool-used-as-base-for-change-vpool-tests' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --max_snapshots 10
}

# Regular VPLEX local vpool used as base for change vpool tests
rpvplex_local_regular()
{
    run cos create block rpvplex_local_regular$1 $POOLS_AUTO_MATCH \
        --description 'Regular-VPLEX-local-vpool-used-as-base-for-change-vpool-tests' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --max_snapshots 10 \
        --highavailability vplex_local
}

# Regular VPLEX dist vpool used as base for change vpool tests
rpvplex_dist_regular()
{
    run cos create block rpvplex_dist_regular$1  $POOLS_AUTO_MATCH \
        --description 'Regular-VPLEX-dist-vpool-used-as-base-for-change-vpool-tests' \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --max_snapshots 10 \
        --haCos rpvplex_ha
}

# RP CDP
rp_cdp()
{
    run cos create block rp_cdp$1 $POOLS_AUTO_MATCH \
        --description 'RP-Source-CDP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES

    run cos create block rp_cdp$1-sync $POOLS_AUTO_MATCH \
        --description 'RP-Source-CDP-with-Sync'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES
}

# RP XIO CDP
rpxio_cdp()
{
    run cos create block rpxio_cdp$1 $POOLS_AUTO_MATCH \
        --description 'RP-XIO-Source-CDP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpxio_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES

    run cos create block rpxio_cdp$1-sync $POOLS_AUTO_MATCH \
        --description 'RP-XIO-Source-CDP-with-Sync'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rpxio_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES
}

# RP CRR
rp_crr()
{
    run cos create block rp_crr$1 $POOLS_AUTO_MATCH \
        --description 'RP-Source-CRR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --rp_copy_mode ASYNCHRONOUS \
        --max_snapshots 10 \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES

    run cos create block rp_crr$1-sync $POOLS_AUTO_MATCH \
        --description 'RP-Source-CRR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --rp_copy_mode SYNCHRONOUS \
        --max_snapshots 10 \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES
}

# RP CLR
rp_clr()
{
    run cos create block rp_clr$1 $POOLS_AUTO_MATCH \
        --description 'RP-Source-CLR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES

    run cos create block rp_clr$1-sync $POOLS_AUTO_MATCH \
        --description 'RP-Source-CLR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES
}

# RP+VPLEX CDP
rpvplex_cdp()
{
    run cos create block rpvplex_cdp$1 $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Source-CDP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --journalVpool rp_targets \
        --journalVarray $RECOVERPOINT_VARRAY1 \
        --highavailability vplex_local

    run cos create block rpvplex_cdp$1-sync $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Source-CDP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --journalVpool rp_targets \
        --journalVarray $RECOVERPOINT_VARRAY1 \
        --highavailability vplex_local
}

# RP+VPLEX CRR
rpvplex_crr()
{
    run cos create block rpvplex_crr$1 $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Source-CRR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --rp_copy_mode ASYNCHRONOUS \
        --max_snapshots 10 \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha

    run cos create block rpvplex_crr$1-sync $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Source-CRR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha
}

# RP+VPLEX CLR
rpvplex_clr()
{
    run cos create block rpvplex_clr$1 $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Source-CLR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --journalVpool rp_targets \
        --journalVarray $RECOVERPOINT_VARRAY1 \
        --haCos rpvplex_ha

    run cos create block rpvplex_clr$1-sync $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-Source-CLR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --journalVpool rp_targets \
        --journalVarray $RECOVERPOINT_VARRAY1 \
        --haCos rpvplex_ha
}

# MetroPoint CDP
rpmp_cdp()
{
    run cos create block rpmp_cdp$1 $POOLS_AUTO_MATCH \
        --description 'MetroPoint-Source-CDP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY2':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false

    run cos create block rpmp_cdp$1-sync $POOLS_AUTO_MATCH \
        --description 'MetroPoint-Source-CDP'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY2':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false
}

# MetroPoint CRR
rpmp_crr()
{
    run cos create block rpmp_crr$1 $POOLS_AUTO_MATCH \
        --description 'MetroPoint-Source-CRR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false

    run cos create block rpmp_crr$1-sync $POOLS_AUTO_MATCH \
        --description 'MetroPoint-Source-CRR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false
}

# MetroPoint CLR
rpmp_clr()
{
    run cos create block rpmp_clr$1 $POOLS_AUTO_MATCH \
        --description 'MetroPoint-CLR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY2':rp_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode ASYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false

    run cos create block rpmp_clr$1-sync $POOLS_AUTO_MATCH \
        --description 'MetroPoint-CLR'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --protectionCoS $RECOVERPOINT_VARRAY1':rp_targets'$1':min',$RECOVERPOINT_VARRAY2':rp_targets'$1':min',$RECOVERPOINT_VARRAY3':rp_targets'$1':min' \
        --max_snapshots 10 \
        --rp_copy_mode SYNCHRONOUS \
        --rp_rpo_value 5 \
        --rp_rpo_type MINUTES \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha \
        --metropoint true \
        --activeProtectionAtHASite false
}

# RP - NO PROTECTION
rp_noprotection()
{
    run cos create block rp_noprotection$1 $POOLS_AUTO_MATCH \
        --description 'RP---NO-PROTECTION'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --max_snapshots 10 \
        --multiVolumeConsistency
}

# RP+VPLEX LOCAL - NO PROTECTION
rpvplexlocal_noprotection()
{
    run cos create block rpvplexlocal_noprotection$1 $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-LOCAL---NO-PROTECTION'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3 $RECOVERPOINT_VARRAY4 \
        --max_snapshots 10 \
        --multiVolumeConsistency \
        --highavailability vplex_local
}

# RP+VPLEX DIST - NO PROTECTION
rpvplexdist_noprotection()
{
    run cos create block rpvplexdist_noprotection$1 $POOLS_AUTO_MATCH \
        --description 'RP+VPLEX-DIST---NO-PROTECTION'  \
        --protocols FC \
        --numpaths 2 \
        --provisionType 'Thin' \
        --neighborhoods $RECOVERPOINT_VARRAY1 \
        --multiVolumeConsistency \
        --max_snapshots 10 \
        --highavailability vplex_distributed \
        --haNeighborhood $RECOVERPOINT_VARRAY2 \
        --haCos rpvplex_ha
}

rp_vpool_setup()
{
    if [ "${RP_TESTS}" = "1" -o "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        rp_targets
        rp_regular
        rp_noprotection
        if [ "${RP_CDP}" = "1" ]; then
            rp_cdp
        fi
        if [ "${RP_CRR}" = "1" ]; then
            rp_crr
        fi
        if [ "${RP_CLR}" = "1" ]; then
            rp_clr
        fi
    fi

    if [ "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        rp_targets
        rp_regular
        rpvplex_targets
        rpvplex_ha
        rpvplexlocal_noprotection
        rpvplexdist_noprotection
        rp_noprotection
        if [ "${RP_CDP}" = "1" ]; then
            rpvplex_cdp
            rpmp_cdp
        fi
        if [ "${RP_CRR}" = "1" ]; then
            rpvplex_crr
            rpmp_crr
        fi
        if [ "${RP_CLR}" = "1" ]; then
            rpvplex_clr
            rpmp_clr
        fi

    fi

    if [ "${RPXIO_TESTS}" = "1" ]; then
        rpxio_targets
        rp_noprotection
        if [ "${RP_CDP}" = "1" ]; then
            rpxio_cdp
        fi
    fi
}

rp_varray_setup()
{
    secho 'RecoverPoint varray setup'

    # Create Virtual Array
    run neighborhood create $RECOVERPOINT_VARRAY1
    run neighborhood allow $RECOVERPOINT_VARRAY1 $TENANT
    run neighborhood allow $RECOVERPOINT_VARRAY1 `tenant root`

    run neighborhood create $RECOVERPOINT_VARRAY2
    run neighborhood allow $RECOVERPOINT_VARRAY2 $TENANT
    run neighborhood allow $RECOVERPOINT_VARRAY2 `tenant root`

    run neighborhood create $RECOVERPOINT_VARRAY3
    run neighborhood allow $RECOVERPOINT_VARRAY3 $TENANT
    run neighborhood allow $RECOVERPOINT_VARRAY3 `tenant root`

    run neighborhood create $RECOVERPOINT_VARRAY4
    run neighborhood allow $RECOVERPOINT_VARRAY4 $TENANT
    run neighborhood allow $RECOVERPOINT_VARRAY4 `tenant root`
}

rp_network_setup()
{
    secho 'RecoverPoint network setup'

    networksystem create $RECOVERPOINT_BROCADE_NETWORK brocade --smisip $RECOVERPOINT_BROCADE_NETWORK_PROVIDER --smisport 5988 --smisuser $RECOVERPOINT_BROCADE_NETWORK_PROVIDER_USERNAME --smispw "$RECOVERPOINT_BROCADE_NETWORK_PROVIDER_PASSWORD" --smisssl false
    sleep 20
    transportzone listall
}

rp_storage_setup()
{
    secho 'RecoverPoint storage setup'

    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop controller_max_thin_pool_subscription_percentage 600

    if [ "${RP_TESTS}" = "1" -o "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        for var in A B C D
        do
           provider='RECOVERPOINT_SMIS_PROVIDER_'$var
           provider=${!provider}

           provider_ip='RECOVERPOINT_SMIS_PROVIDER_'$var'_IP'
           provider_ip=${!provider_ip}

           array_guid='RECOVERPOINT_STORAGE_ARRAY_'$var'_GUID'
           array_guid=${!array_guid}

           smis_port='RECOVERPOINT_STORAGE_ARRAY_'$var'_SMIS_PORT'          
           smis_port=${!smis_port}

           smis_ssl='RECOVERPOINT_STORAGE_ARRAY_'$var'_SMIS_SSL'
           smis_ssl=${!smis_ssl}
 
           if [ "$array_guid" != "NONE" ]; then
               smisprovider create $provider $provider_ip $smis_port $SMIS_USER $SMIS_PASSWORD $smis_ssl
           fi
        done
    fi

    if [ "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        rp_vplex_setup
    fi

    if [ "${RPXIO_TESTS}" = "1" ]; then
        rp_xio_setup
    fi

    secho 'RecoverPoint discover storage systems'
    run storagedevice discover_all
    storagedevice list
}

rp_xio_setup()
{
    secho 'RecoverPoint XIO setup'
    run storageprovider create xtremeio1 $RECOVERPOINT_XTREMIO_IP 443 $RECOVERPOINT_XTREMIO_USER $RECOVERPOINT_XTREMIO_PASSWD xtremio
}

rp_vplex_setup()
{
    secho 'RecoverPoint VPLEX setup'

    # Create VPLEX
    if [ "$RECOVERPOINT_VPLEX_A" != "NONE" ]; then
        storageprovider create $RECOVERPOINT_VPLEX_A $RECOVERPOINT_VPLEX_A_IP $RECOVERPOINT_VPLEX_A_PORT $RECOVERPOINT_VPLEX_A_USER $RECOVERPOINT_VPLEX_A_PASSWORD vplex
    fi

    if [ "$RECOVERPOINT_VPLEX_B" != "NONE" ]; then
        if [ "${RP_CRR}" = "1" -o "${RP_CLR}" = "1" ]; then
            storageprovider create $RECOVERPOINT_VPLEX_B $RECOVERPOINT_VPLEX_B_IP $RECOVERPOINT_VPLEX_B_PORT $RECOVERPOINT_VPLEX_B_USER $RECOVERPOINT_VPLEX_B_PASSWORD vplex
        fi
    fi
    sleep 15
}

rp_port_setup()
{
    secho 'RecoverPoint port setup'

    if [ "${RP_TESTS}" = "1" -o "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        if [ "$RECOVERPOINT_STORAGE_ARRAY_A_GUID" != "NONE" ]; then
            run storageport update $RECOVERPOINT_STORAGE_ARRAY_A_GUID FC --addvarrays $RECOVERPOINT_VARRAY1
        fi

        if [ "$RECOVERPOINT_STORAGE_ARRAY_B_GUID" != "NONE" ]; then
            run storageport update $RECOVERPOINT_STORAGE_ARRAY_B_GUID FC --addvarrays $RECOVERPOINT_VARRAY2
        fi
        
        if [ "$RECOVERPOINT_STORAGE_ARRAY_C_GUID" != "NONE" ]; then
            run storageport update $RECOVERPOINT_STORAGE_ARRAY_C_GUID FC --addvarrays $RECOVERPOINT_VARRAY3
        fi
        
        if [ "$RECOVERPOINT_STORAGE_ARRAY_D_GUID" != "NONE" ]; then
            run storageport update $RECOVERPOINT_STORAGE_ARRAY_D_GUID FC --addvarrays $RECOVERPOINT_VARRAY4
        fi
    fi

    # VPLEX
    # Override port to varray assigments for VPLEX clusters to prevent port mixing in clusters
    # Source Site
    # Cluster 1
    if [ "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        if [ "$RECOVERPOINT_VPLEX_A_GUID" != "NONE" ] && [ "$RECOVERPOINT_STORAGE_ARRAY_A_GUID" != "NONE" ]; then
            run storageport update $RECOVERPOINT_VPLEX_A_GUID FC --group director-1-1-A --addvarrays $RECOVERPOINT_VARRAY1
            run storageport update $RECOVERPOINT_VPLEX_A_GUID FC --group director-1-1-B --addvarrays $RECOVERPOINT_VARRAY1
        fi
            # Cluster 2
        if [ "$RECOVERPOINT_VPLEX_A" != "NONE" ] && [ "$RECOVERPOINT_STORAGE_ARRAY_B_GUID" != "NONE" ]; then
            run storageport update $RECOVERPOINT_VPLEX_A_GUID FC --group director-2-1-A --addvarrays $RECOVERPOINT_VARRAY2
            run storageport update $RECOVERPOINT_VPLEX_A_GUID FC --group director-2-1-B --addvarrays $RECOVERPOINT_VARRAY2
        fi

        if [ "${RP_CRR}" = "1" -o "${RP_CLR}" = "1" ]; then
            # Target Site
            # Cluster 1
            if [ "$RECOVERPOINT_VPLEX_B" != "NONE" ] && [ "$RECOVERPOINT_STORAGE_ARRAY_C_GUID" != "NONE" ]; then
                run storageport update $RECOVERPOINT_VPLEX_B_GUID FC --group director-1-1-A --addvarrays $RECOVERPOINT_VARRAY3
                run storageport update $RECOVERPOINT_VPLEX_B_GUID FC --group director-1-1-B --addvarrays $RECOVERPOINT_VARRAY3
            fi

            # Cluster 2
            if [ "$RECOVERPOINT_VPLEX_B" != "NONE" ] && [ "$RECOVERPOINT_STORAGE_ARRAY_D_GUID" != "NONE" ]; then
                run storageport update $RECOVERPOINT_VPLEX_B_GUID FC --group director-2-1-A --addvarrays $RECOVERPOINT_VARRAY4
                run storageport update $RECOVERPOINT_VPLEX_B_GUID FC --group director-2-1-B --addvarrays $RECOVERPOINT_VARRAY4
            fi
        fi
    fi

    if [ "${RPXIO_TESTS}" = "1" ]; then
        run storageport update $RECOVERPOINT_XTREMIO_GUID FC --addvarrays $RECOVERPOINT_VARRAY1
    fi

}

rp_protection_system_setup()
{
    secho 'RecoverPoint protection system setup'

    protectionsystem show $RECOVERPOINT &> /dev/null && return $?

    run protectionsystem create $RECOVERPOINT \
    $RP_SYSTEM_TYPE \
    $RECOVERPOINT_IP \
    $RECOVERPOINT_PORT \
    $RECOVERPOINT_USER \
    $RECOVERPOINT_PASSWORD \
    1
}

rp_cg_setup()
{
    secho 'RecoverPoint CG setup'

    if [ "${RP_TESTS}" = "1" ]; then
        run blockconsistencygroup create $PROJECT $RP_CONSISTENCY_GROUP
    fi

    if [ "${RPVPLEX_TESTS}" = "1" ]; then
        run blockconsistencygroup create $PROJECT $RP_VPLEX_CONSISTENCY_GROUP
    fi

    if [ "${RPMP_TESTS}" = "1" ]; then
        run blockconsistencygroup create $PROJECT $RP_METROPOINT_CONSISTENCY_GROUP
    fi

    if [ "${RPXIO_TESTS}" = "1" ]; then
        run blockconsistencygroup create $PROJECT $RP_XIO_CONSISTENCY_GROUP
    fi
}

rp_isolate_rpa_clusters()
{
    if [ "$RP_USE_RPA_ISOLATION" = "1" ]; then
        secho 'RecoverPoint isolate RPA clusters'
        if [ "${RPXIO_TESTS}" = "0" ]; then
            run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster $RECOVERPOINT_RPA_CLUSTER1 --addvarrays $RECOVERPOINT_VARRAY1
            run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster $RECOVERPOINT_RPA_CLUSTER3 --addvarrays $RECOVERPOINT_VARRAY2
            run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster $RECOVERPOINT_RPA_CLUSTER4 --addvarrays $RECOVERPOINT_VARRAY3
        else
            run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster $RECOVERPOINT_XIORPA_CLUSTER1 --addvarrays $RECOVERPOINT_VARRAY1
        fi
    fi
}

recoverpoint_exports_tests()
{
    rpsnap=rpsnap-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    blocksnap=blocksnap-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    rpexport=$PROJECT/$RP_EXPORT_GROUP

    #Create RP/Block snapshots
    secho 'Create snapshots'
    run blocksnapshot create ${PROJECT}/${rpvolume} ${rpsnap} --type rp
    run blocksnapshot create ${PROJECT}/${rpvolume} ${blocksnap}
    run blocksnapshot activate ${PROJECT}/${rpvolume}/${blocksnap}
    run blocksnapshot delete ${PROJECT}/${rpvolume}/${blocksnap}
    run blocksnapshot create ${PROJECT}/${rpvolume}-target-${rp_src_varray} ${blocksnap}-${rp_src_varray}
    run blocksnapshot delete ${PROJECT}/${rpvolume}-target-${rp_src_varray}/${blocksnap}-${rp_src_varray}-${rp_src_varray}

    PWWN1=`pwwn 6F`;
    NWWN1=`pwwn 7F`;
    run hosts create ${RP_EXPORT_GROUP_HOST} $TENANT Windows ${RP_EXPORT_GROUP_HOST} --port 8111 --username $RP_HOST_USER --password '$RP_HOST_PW' --osversion 1.0
    run initiator create ${RP_EXPORT_GROUP_HOST} FC $PWWN1 --node $NWWN1

    if [ "$PARAM" = "vmaxblock" ]; then
        run transportzone add ${rp_src_varray}/${RP_VMAXB_VSAN} $PWWN1
    elif [ "$PARAM" = "vnxblock" ]; then
        run transportzone add ${rp_src_varray}/${RP_VNXB_VSAN} $PWWN1
    else
        # simulators being used
        run transportzone add ${rp_src_varray}/${SIMULATOR_VSAN_11} $PWWN1
    fi

    sleep 20

    secho 'recoverpoint exports'
    run export_group create $PROJECT ${RP_EXPORT_GROUP} ${rp_src_varray} --volspec ${PROJECT}/${rpvolume}/${rpsnap}-${rp_src_varray} --inits "${RP_EXPORT_GROUP_HOST}/$PWWN1"
    run export_group show ${rpexport}
    run export_group update ${rpexport} --remVol ${PROJECT}/${rpvolume}/${rpsnap}-${rp_src_varray}
    run export_group update ${rpexport} --addVolspec ${PROJECT}/${rpvolume}/${rpsnap}-${rp_src_varray}
    run export_group show ${rpexport}
    # CTRL-4638: --remInits will not call RP device controller to disable bookmark.
    #run export_group update ${rpexport} --remInits "${RP_EXPORT_GROUP_HOST}/$PWWN1"
    #run export_group update ${rpexport} --addInits "${RP_EXPORT_GROUP_HOST}/$PWWN1"
    run export_group show ${rpexport}
    run export_group delete ${rpexport}

    # currently having a volume and an RP snap in the same export group and deleting that export group doesn't work (CTRL?)
    #echo 'Running export of RP snapshot when volume already exists in mask'
    #run export_group create $PROJECT ${RP_EXPORT_GROUP}2 $NH --volspec ${PROJECT}/${rpvolume} --inits "${RP_EXPORT_GROUP_HOST}/$PWWN1"
    #run export_group update ${rpexport}2 --addVolspec ${PROJECT}/${rpvolume}/${rpsnap}-${NH}
    #run export_group update ${rpexport}2 --remVol ${PROJECT}/${rpvolume}/${rpsnap}-${NH}
    #run export_group update ${rpexport}2 --addVolspec ${PROJECT}/${rpvolume}/${rpsnap}-${NH}
    #run export_group delete ${rpexport}2

    secho 'done with RP exports, cleaning up'
    run blocksnapshot delete ${PROJECT}/${rpvolume}/${rpsnap}-${NH}
    run hosts delete ${RP_EXPORT_GROUP_HOST}
    secho 'done with recoverpoint exports cleanup'
}

recoverpoint_export_bookmark_tests()
{
    if [ "$RP_EXPORT_BOOKMARK_TESTS" = "1" ]; then
        secho 'Running RecoverPoint Export Bookmark Tests'

        # The source source volume - an arbitrary source volume from the CG
        volume=${1}
        # The virtual array corresonding to the source volume (src_volume)
        rp_src_varray=${2}
        # The target virtual arrays
        rp_tgt_varrays=${3}
        # The virtual pool
        vpool=${4}
        # RP type
        rptype=${5}

        rpsnap=rpsnap-${RP_MODIFIED_HOSTNAME}-${RANDOM}
        blocksnap=blocksnap-${RP_MODIFIED_HOSTNAME}-${RANDOM}
        rpexport=$PROJECT/$RP_EXPORT_GROUP

        # Have to account for "-1" being added to volume name if
        # volume count is greater than 1.
        volume_name_modifier=''
        if [ "$RP_VOLUME_COUNT" = "1" ]; then
            volume_name_modifier=''
        else
            volume_name_modifier='-1'
        fi

        #Create RP/Block snapshots (only create for the first volume if there is more than 1)
        secho 'Creating RP bookmarks for each source volume'
	
        all_bookmarks=()
  
        for i in `seq $RP_VOLUME_COUNT`
        do
            #Create/show the snapshot/bookmark that was created for each target virtual array
        
            #If we only have a single volume we do not want to append anything to volume name.  Otherwise we want to
            #append a volume count identifier.
            volume_identifier=''
            if [ "$RP_VOLUME_COUNT" -gt "1" ]; then
                volume_identifier='-'${i}
            fi    
        
            run blocksnapshot create ${PROJECT}/${volume}${volume_identifier} ${rpsnap} --type rp
	    for tgt_varray in ${rp_tgt_varrays}
	    do
                #NOTE: RP bookmarks get created with for each target copy (virtual array)
                all_bookmarks=("${all_bookmarks[@]}" "${PROJECT}/${volume}${volume_identifier}/${rpsnap}-${tgt_varray}")
                blocksnapshot show ${PROJECT}/${volume}${volume_identifier}/${rpsnap}-${tgt_varray}
	    done
        done

        recoverpoint_export_bookmark_host_setup ${vpool} ${rptype}

        sleep 20
	
        secho 'Running export RP bookmark failure tests'

        if [ "$RP_VOLUME_COUNT" -gt "1" ]; then	
            secho 'Adding multiple bookmarks for the same target during export group create - expected failure'
            #join the list of all the bookmarks created with commas
            volspec="$(printf "%s,"  ${all_bookmarks[@]} | cut -d "," -f 1-${#all_bookmarks[@]})"
            fail export_group create $PROJECT ${RP_EXPORT_GROUP} ${rp_src_varray} --type Host --volspec ${volspec} --hosts "${RP_HOSTNAME}"
        fi

        secho 'Creating 2 RP bookmarks for the same volume.'
        run blocksnapshot create ${PROJECT}/${volume}${volume_name_modifier} ${rpsnap}-failtest1 --type rp
        blocksnapshot show ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest1-${rp_tgt_varrays[0]}
        run blocksnapshot create ${PROJECT}/${volume}${volume_name_modifier} ${rpsnap}-failtest2 --type rp
        blocksnapshot show ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest2-${rp_tgt_varrays[0]}
    
        run export_group create $PROJECT ${RP_EXPORT_GROUP} ${rp_tgt_varrays[0]} --type Host --volspec ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest1-${rp_tgt_varrays[0]} --hosts "${RP_HOSTNAME}"
        export_group show ${rpexport}
        secho 'Exporting RP bookmark for the same target copy as a bookmark already exported - expected failure'
        fail export_group update ${rpexport} --addVolspec ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest2-${rp_tgt_varrays[0]}
        export_group show ${rpexport}
        run export_group delete ${rpexport}
    
        secho 'Finished running export RP bookmark failure tests'

        secho 'Running export RP bookmark tests'
        for tgt_varray in ${rp_tgt_varrays}
        do
            secho 'Running RP bookmark export tests for target copy '${tgt_varray}
            run export_group create $PROJECT ${RP_EXPORT_GROUP}-${tgt_varray} ${tgt_varray} --type Host --volspec ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-${tgt_varray} --hosts "${RP_HOSTNAME}"
            export_group show ${rpexport}-${tgt_varray}
            run export_group update ${rpexport}-${tgt_varray} --remVol ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-${tgt_varray}
            run export_group update ${rpexport}-${tgt_varray} --addVolspec ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-${tgt_varray}
            export_group show ${rpexport}-${tgt_varray}
            run export_group delete ${rpexport}-${tgt_varray}
        done

        secho 'Finished with export RP bookmark tests, cleaning up'

        blocksnapshot delete ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest1-${rp_tgt_varrays[0]}
        blocksnapshot delete ${PROJECT}/${volume}${volume_name_modifier}/${rpsnap}-failtest2-${rp_tgt_varrays[0]}

        for i in ${RP_VOLUME_COUNT}
        do
            for tgt_varray in ${rp_tgt_varrays}
            do
                volume_identifier=''
                if [ "$RP_VOLUME_COUNT" -gt "1" ]; then
                    volume_identifier='-'${i}
                fi
                blocksnapshot delete ${PROJECT}/${volume}${volume_identifier}/${rpsnap}-${tgt_varray}
            done
        done

        run hosts delete ${RP_HOSTNAME}
        secho 'Done with RecoverPoint bookmark export cleanup'
    else
        secho 'Skipping RecoverPoint Export Bookmark Tests'
    fi
}

recoverpoint_export_bookmark_host_setup()
{
    vpool=${1}
    rptype=${2}

    if [ "${vpool}" = "rp${rptype}_cdp" ]; then
        PWWN1=10:00:00:E0:E0:E0:A1:E0
        WWNN1=20:00:00:E0:E0:E0:A1:E0
        PWWN2=10:00:00:E0:E0:E0:A2:E0
        WWNN2=20:00:00:E0:E0:E0:A2:E0
        PWWN3=10:00:00:E0:E0:E0:A3:D0
        WWNN3=20:00:00:E0:E0:E0:A3:D0
        RP_HOSTNAME=${RP_EXPORT_GROUP_HOST}-${RANDOM}-cdp
    elif [ "${vpool}" = "rp${rptype}_crr" ]; then
        PWWN1=10:00:00:E0:E0:E0:B1:F0
        WWNN1=20:00:00:E0:E0:E0:B1:F0
        PWWN2=10:00:00:E0:E0:E0:B2:F0
        WWNN2=20:00:00:E0:E0:E0:B2:F0
        PWWN3=10:00:00:E0:E0:E0:B3:D0
        WWNN3=20:00:00:E0:E0:E0:B3:D0
        RP_HOSTNAME=${RP_EXPORT_GROUP_HOST}-${RANDOM}-crr
    elif [ "${vpool}" = "rp${rptype}_clr" ]; then
        PWWN1=10:00:00:E0:E0:E0:C1:D0
        WWNN1=20:00:00:E0:E0:E0:C1:D0
        PWWN2=10:00:00:E0:E0:E0:C2:D0
        WWNN2=20:00:00:E0:E0:E0:C2:D0
        PWWN3=10:00:00:E0:E0:E0:C3:D0
        WWNN3=20:00:00:E0:E0:E0:C3:D0
        RP_HOSTNAME=${RP_EXPORT_GROUP_HOST}-${RANDOM}-clr
    fi

    run hosts create ${RP_HOSTNAME} $TENANT Windows ${RP_HOSTNAME} --port 8111 --username user --password 'password' --osversion 1.0
    run initiator create ${RP_HOSTNAME} FC ${PWWN1} --node ${WWNN1}
    run initiator create ${RP_HOSTNAME} FC ${PWWN2} --node ${WWNN2}
    run initiator create ${RP_HOSTNAME} FC ${PWWN3} --node ${WWNN3}

    if [ "$RP_QUICK_PARAM" == "quick" ]; then
        run transportzone add ${RECOVERPOINT_VARRAY1}/VSAN_11 ${PWWN1}
        run transportzone add ${RECOVERPOINT_VARRAY2}/VSAN_12 ${PWWN2}
        run transportzone add ${RECOVERPOINT_VARRAY3}/VSAN_13 ${PWWN3}
    else
        run transportzone add ${RECOVERPOINT_VARRAY1}/FABRIC_losam082-fabric ${PWWN1}
    fi
}

recoverpoint_auto_snapshot_cleanup_test()
{
    if [ "$RP_AUTO_SNAP_CLEANUP_TESTS" = "1" ]; then    
        secho 'Running RecoverPoint Auto Snapshot Cleanup Tests'
        rpsnap=rpsnap_for_cleanup-${RP_MODIFIED_HOSTNAME}-${RANDOM}
        blocksnap=blocksnap-${RP_MODIFIED_HOSTNAME}-${RANDOM}

        src_volume=${1}
        tgt_volume=${2}
        tgt_varray=${3}
        tgt2_volume=${4}

        # Create RP/Block snapshot
        secho 'Create snapshot'
        run blocksnapshot create ${PROJECT}/${src_volume} ${rpsnap} --type rp
        blocksnapshot show ${PROJECT}/${src_volume}/${rpsnap}-${tgt_varray} | grep inactive | grep false
        run protectionsystem discover $RECOVERPOINT

        secho 'Unaffected RP bookmark is being checked after RP discovery to ensure it is still active'
        blocksnapshot show ${PROJECT}/${src_volume}/${rpsnap}-${tgt_varray} | grep inactive | grep false

        secho 'Stopping protection for the copy and then re-enabling'
        run volume change_link ${PROJECT}/${src_volume} stop ${PROJECT}/${tgt_volume} rp
    
        sleep 15
        run volume change_link ${PROJECT}/${src_volume} start ${PROJECT}/${tgt_volume} rp

        if [ "${tgt2_volume}" != "" ]; then
            secho 'Stopping protection for the copy and then re-enabling'
            run volume change_link ${PROJECT}/${src_volume} stop ${PROJECT}/${tgt2_volume} rp
            sleep 15
            run volume change_link ${PROJECT}/${src_volume} start ${PROJECT}/${tgt2_volume} rp
        fi

        secho 'Re-run discovery, RP bookmark should be cleaned up automatically'
        sleep 10
        run protectionsystem discover $RECOVERPOINT

        secho 'Removed RP bookmark is being checked after RP discovery to ensure it is no longer active'
        # Use "fail" instead of run because we expect this bookmark to fail.
        fail blocksnapshot show ${PROJECT}/${src_volume}/${rpsnap}-${tgt_varray}
    else
        secho 'Skipping RecoverPoint Auto Snapshot Cleanup Tests'
    fi
}

recoverpoint_ingest_host_setup()
{
    vpool=${1}

    if [ "$RP_QUICK_PARAM" == "quick" ]; then
        CLUSTER1NET_NAME=VSAN_11
    else
        CLUSTER1NET_NAME=FABRIC_losam082-fabric
    fi

    # Make different initiators for different hosts
    if [ "${rptypetag}" = "" ]; then
        DD="AA";
    elif [ "${rptypetag}" = "vplex" ]; then
        DD="BB";
    elif [ "${rptypetag}" = "mp" ]; then
        DD="CC";
    elif [ "${rptypetag}" = "xio" ]; then
        DD="DD";
    fi
    
    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        PWWN1=10:00:00:E0:E0:E0:${DD}:E0
        WWNN1=20:00:00:E0:E0:E0:${DD}:E0
        PWWN2=10:00:00:E0:E0:E0:${DD}:E1
        WWNN2=20:00:00:E0:E0:E0:${DD}:E1
        RP_HOSTNAME=${RECOVERPOINT_HOST_VARRAY1}${rptypetag}cdp
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        PWWN1=10:00:00:E0:E0:E0:${DD}:F0
        WWNN1=20:00:00:E0:E0:E0:${DD}:F0
        PWWN2=10:00:00:E0:E0:E0:${DD}:F1
        WWNN2=20:00:00:E0:E0:E0:${DD}:F1
        RP_HOSTNAME=${RECOVERPOINT_HOST_VARRAY1}${rptypetag}crr
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        PWWN1=10:00:00:E0:E0:E0:${DD}:D0
        WWNN1=20:00:00:E0:E0:E0:${DD}:D0
        PWWN2=10:00:00:E0:E0:E0:${DD}:D1
        WWNN2=20:00:00:E0:E0:E0:${DD}:D1
        RP_HOSTNAME=${RECOVERPOINT_HOST_VARRAY1}${rptypetag}clr
    fi

    run hosts create ${RP_HOSTNAME}1 $TENANT Windows ${RP_HOSTNAME}1 --port 8111 --username user --password 'password' --osversion 1.0 

    run initiator create ${RP_HOSTNAME}1 FC ${PWWN1} --node ${WWNN1}
    run initiator create ${RP_HOSTNAME}1 FC ${PWWN2} --node ${WWNN2}

    run transportzone add ${RECOVERPOINT_VARRAY1}/${CLUSTER1NET_NAME} ${PWWN1}
    run transportzone add ${RECOVERPOINT_VARRAY1}/${CLUSTER1NET_NAME} ${PWWN2}

    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        PWWN1=10:00:00:E0:E0:E0:${DD}:C0
        WWNN1=20:00:00:E0:E0:E0:${DD}:C0
        PWWN2=10:00:00:E0:E0:E0:${DD}:C1
        WWNN2=20:00:00:E0:E0:E0:${DD}:C1
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        PWWN1=10:00:00:E0:E0:E0:${DD}:B0
        WWNN1=20:00:00:E0:E0:E0:${DD}:B0
        PWWN2=10:00:00:E0:E0:E0:${DD}:B1
        WWNN2=20:00:00:E0:E0:E0:${DD}:B1
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        PWWN1=10:00:00:E0:E0:E0:${DD}:A0
        WWNN1=20:00:00:E0:E0:E0:${DD}:A0
        PWWN2=10:00:00:E0:E0:E0:${DD}:A1
        WWNN2=20:00:00:E0:E0:E0:${DD}:A1
    fi

    run hosts create ${RP_HOSTNAME}2 $TENANT Windows ${RP_HOSTNAME}2 --port 8111 --username user --password 'password' --osversion 1.0 

    run initiator create ${RP_HOSTNAME}2 FC ${PWWN1} --node ${WWNN1}
    run initiator create ${RP_HOSTNAME}2 FC ${PWWN2} --node ${WWNN2}

    run transportzone add ${RECOVERPOINT_VARRAY1}/${CLUSTER1NET_NAME} ${PWWN1}
    run transportzone add ${RECOVERPOINT_VARRAY1}/${CLUSTER1NET_NAME} ${PWWN2}
}

recoverpoint_ingest_discovery()
{
    INGEST_NATIVEGUID=$VMAX1_SIMULATOR_NATIVEGUID
    RP_NATIVEGUID=$RECOVERPOINT_RP_SIM_NATIVEGUID

    if [ "$RP_QUICK_PARAM" != "quick" ]; then
        if [ "${RPXIO_TESTS}" = "1" ]; then
            INGEST_NATIVEGUID=$RECOVERPOINT_XTREMIO_GUID
        else
            INGEST_NATIVEGUID=$RECOVERPOINT_STORAGE_ARRAY_A_GUID
        fi
        RP_NATIVEGUID=$RECOVERPOINT_RP_NATIVEGUID
    elif [ "${RPXIO_TESTS}" = "1" ]; then
        INGEST_NATIVEGUID=$XIO_4X_SIM_NATIVEGUID
    fi

    if [ ${RP_CRR} = "1" -o ${RP_CLR} = "1" -o "${RPMP_TESTS}" = "1" ]; then
        INGEST_NATIVEGUID2=$RECOVERPOINT_STORAGE_ARRAY_B_GUID
        INGEST_NATIVEGUID3=$RECOVERPOINT_STORAGE_ARRAY_C_GUID
        if [ "$RP_QUICK_PARAM" == "quick" ]; then
            INGEST_NATIVEGUID2=$VMAX2_SIMULATOR_NATIVEGUID
            INGEST_NATIVEGUID3=$VMAX3_SIMULATOR_NATIVEGUID
        fi
    fi

    secho 'RP ingest discovery'

    # Discover unmanaged volumes on the array
    run storagedevice discover_namespace $INGEST_NATIVEGUID 'UNMANAGED_VOLUMES'

    if [ ${RP_CRR} = "1" -o ${RP_CLR} = "1" -o "${RPMP_TESTS}" = "1" ]; then
        run storagedevice discover_namespace $INGEST_NATIVEGUID2 'UNMANAGED_VOLUMES'
    fi

    if [ ${RP_CRR} = "1" -o ${RP_CLR} = "1" ]; then
        run storagedevice discover_namespace $INGEST_NATIVEGUID3 'UNMANAGED_VOLUMES'
    fi

    if [ ${RPVPLEX_TESTS} = "1" -o "${RPMP_TESTS}" = "1" ]; then
        run storagedevice discover_namespace $RECOVERPOINT_VPLEX_A_GUID 'UNMANAGED_VOLUMES'
        if [ ${RP_CRR} = "1" -o ${RP_CLR} = "1" ]; then
            run storagedevice discover_namespace $RECOVERPOINT_VPLEX_B_GUID 'UNMANAGED_VOLUMES'
        fi
    fi

    # Especially with the simulator, the previous RP discovery may still be running
    state=`protectionsystem list | grep ${RP_SIMULATOR} | awk '{print $(NF-1)}'`
    while [ "$state" = "IN_PROGRESS" ]; do
        echo "Waiting for RP protection discovery to complete..."
        sleep 10
        state=`protectionsystem list | grep ${RP_SIMULATOR} | awk '{print $(NF-1)}'`
    done

    # Sleep 5 to get past discovery required timeframe
    sleep 10

    run protectionsystem discover_namespace $RP_NATIVEGUID 'UNMANAGED_CGS'
}

recoverpoint_snapshot_test()
{
    srcvolume=$1
    tgtvarray=$2
    rpsnapname=$3
    blocksnapname=$4

    # Bypassing snapshot tests for two reasons
    secho 'Bypassing snapshot tests until:'
    secho '- Simulator is not removing snapshots, so unmanaged discovery is pulling in snapshots we deleted'
    secho '- Deleting the snapshot in ViPR is not removing the BlockSnapshotSession reference, so inventory-only delete is failing'
    return

    # Create RP/Block snapshots
    secho 'Create snapshots'

    # Create array-based snapshot off of the RP target
    run_noundo blocksnapshot create ${PROJECT}/${srcvolume}-target-${tgtvarray} ${blocksnapname}-${tgtvarray}
    run_noundo blocksnapshot delete ${PROJECT}/${srcvolume}-target-${tgtvarray}/${blocksnapname}-${tgtvarray}

    # Create array-based snapshot off of the RP source
    run_noundo blocksnapshot create ${PROJECT}/${srcvolume} ${blocksnapname}

    # RP Simulator can't handle restore operation at this time.
    if [ "$RP_QUICK_PARAM" != "quick" ]; then
        run_noundo blocksnapshot restore ${PROJECT}/${srcvolume}/${blocksnapname}
    fi

    run_noundo blocksnapshot delete ${PROJECT}/${srcvolume}/${blocksnapname}

    # When we create these snapshots, the name will be augemented with the target varray, therefore we can't unroll it.
    run_noundo blocksnapshot create ${PROJECT}/${srcvolume} ${rpsnapname} --type rp
    run_noundo blocksnapshot delete ${PROJECT}/${srcvolume}/${rpsnapname}-${tgtvarray}
}


recoverpoint_ingest_test()
{
    secho 'RP ingest tests'

    vpool=${1}

    # Debug only.  If you're diagnosing basic issues in ingestion, this flag cuts down the extra tests and keeps the testing 
    # to the basics of ingestion itself.
    shorttest=0

    # Labels
    RECOVERPOINT_INGEST_VOL1_CG=${RECOVERPOINT_INGEST_VOL1_CGBASE}${RP_SANITY_RANDOM}_${vpool}
    RECOVERPOINT_INGEST_VOLBASE=${RECOVERPOINT_INGEST_VOL1BASE}${RP_SANITY_RANDOM}
    RECOVERPOINT_INGEST_VOL1_SRC=${RECOVERPOINT_INGEST_VOLBASE}_${vpool}

    RECOVERPOINT_INGEST_VOL1_SRC_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY1}-journal-1
    RECOVERPOINT_INGEST_SNAPSHOT_VARRAY=${RECOVERPOINT_VARRAY1}

    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        RECOVERPOINT_INGEST_VOL1_TGT=${RECOVERPOINT_INGEST_VOL1_SRC}-target-${RECOVERPOINT_VARRAY1}
        RECOVERPOINT_INGEST_VOL1_TGT_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY1}-journal-2
        RECOVERPOINT_TGT_VARRAY=${RECOVERPOINT_VARRAY1}

        if [ "${vpool}" = "rpmp_cdp" ]; then
            # Also need to ingest the MP standby production journal, varray2 target and journal volumes as well.
            RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-1
            RECOVERPOINT_INGEST_VOL1_STANDBY_TGT=${RECOVERPOINT_INGEST_VOL1_SRC}-target-${RECOVERPOINT_VARRAY2}
            RECOVERPOINT_INGEST_VOL1_STANDBY_TGT_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-2
        fi
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        RECOVERPOINT_INGEST_VOL1_TGT=${RECOVERPOINT_INGEST_VOL1_SRC}-target-${RECOVERPOINT_VARRAY3}
        RECOVERPOINT_INGEST_VOL1_TGT_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY3}-journal-1
        RECOVERPOINT_INGEST_SNAPSHOT_VARRAY=${RECOVERPOINT_VARRAY3}
        RECOVERPOINT_TGT_VARRAY=${RECOVERPOINT_VARRAY3}

        if [ "${vpool}" = "rpmp_crr" ]; then
            # Also need to ingest the MP standby production journal, varray2 target and journal volumes as well.
            RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-1
        fi
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        RECOVERPOINT_INGEST_VOL1_V1TGT=${RECOVERPOINT_INGEST_VOL1_SRC}-target-${RECOVERPOINT_VARRAY1}
        RECOVERPOINT_INGEST_VOL1_V1TGT_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY1}-journal-2
        RECOVERPOINT_INGEST_VOL1_V3TGT=${RECOVERPOINT_INGEST_VOL1_SRC}-target-${RECOVERPOINT_VARRAY3}
        RECOVERPOINT_INGEST_VOL1_V3TGT_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY3}-journal-1

        if [ "${vpool}" = "rpmp_clr" ]; then
            # Also need to ingest the MP standby production journal, varray2 target and journal volumes as well.
            RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-1
            RECOVERPOINT_INGEST_VOL1_STANDBY_TGT=${RECOVERPOINT_INGEST_VOL1_SRC}-target-${RECOVERPOINT_VARRAY2}
            RECOVERPOINT_INGEST_VOL1_STANDBY_TGT_JRNL=${RECOVERPOINT_INGEST_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-2
        fi

        # For post-ingest operations
        RECOVERPOINT_INGEST_VOL1_TGT=${RECOVERPOINT_INGEST_VOL1_V1TGT}
        RECOVERPOINT_INGEST_SNAPSHOT_VARRAY=${RECOVERPOINT_VARRAY3}
        RECOVERPOINT_TGT_VARRAY=${RECOVERPOINT_VARRAY1}
    fi

    rp_target_vpool=rp_targets
    if [ "${RPXIO_TESTS}" = "1" ]; then
        rp_target_vpool=rpxio_targets
    fi

    # Create the consistency group + volume
    run blockconsistencygroup create ${PROJECT} ${RECOVERPOINT_INGEST_VOL1_CG} --noarrayconsistency
    run volume create ${RECOVERPOINT_INGEST_VOL1_SRC} ${PROJECT} ${RECOVERPOINT_VARRAY1} ${vpool} 1GB --consistencyGroup ${RECOVERPOINT_INGEST_VOL1_CG}

    if [ "${shorttest}" -eq "0" ]; then
        # Run snapshot tests
        recoverpoint_snapshot_test ${RECOVERPOINT_INGEST_VOL1_SRC} ${RECOVERPOINT_TGT_VARRAY} rpsnap1-${RP_MODIFIED_HOSTNAME}-${RANDOM} blocksnap1-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    fi

    # Delete the volume and CG
    run volume delete ${PROJECT}/${RECOVERPOINT_INGEST_VOL1_SRC} --vipronly
    run blockconsistencygroup delete ${RECOVERPOINT_INGEST_VOL1_CG} --vipronly

    # Run discovery
    recoverpoint_ingest_discovery

    rpsnap=rpsnap_for_validation-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        if [ "${shorttest}" -eq "0" ]; then
            # Make sure it fails against a different sync policy than what's on the RPA
            fail unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_cdp-sync $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
            
            # Ingest the volume, but then immediately inventory-only delete it, rediscover, and ingest it again.
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_cdp $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
            run_noundo volume delete ${PROJECT}/"${RECOVERPOINT_INGEST_VOL1_SRC}" --vipronly --force
            # Recreate unmanaged volumes/protection sets
 
            # Run discovery
            recoverpoint_ingest_discovery
        fi

        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT_JRNL}"

        if [ "${shorttest}" -eq "0" ]; then
            # Make sure you fail to create an RP bookmark because you're not fully ingested yet
            fail blocksnapshot create ${PROJECT}/${RECOVERPOINT_INGEST_VOL1_SRC} ${rpsnap}1 --type rp
            # Make sure you fail to create an array snapshot because you're not fully ingested yet
            fail blocksnapshot create ${PROJECT}/${RECOVERPOINT_INGEST_VOL1_TGT} ${rpsnap}2
        fi

        if [ "${vpool}" = "rpmp_cdp" ]; then
            # MP active production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL}"
            # MP target CDP on varray2
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT}"
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        fi
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_cdp $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT_JRNL}"
        if [ "${vpool}" = "rpvplex_crr" -o "${vpool}" = "rpmp_crr" ]; then
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        fi
        if [ "${vpool}" = "rpmp_crr" ]; then
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL}"
        fi
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_crr $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V1TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V1TGT_JRNL}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V3TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V3TGT_JRNL}"

        if [ "${vpool}" = "rpmp_clr" ]; then
            # MP active production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL}"
            # MP target CDP on varray2
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT}"
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        fi

        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_clr $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
    fi

    # Verify the CG got created; fail script if this fails.
    run blockconsistencygroup show ViPR-"${RECOVERPOINT_INGEST_VOL1_CG}"

    # Run snapshot tests
    if [ "${shorttest}" -eq "0" ]; then
        recoverpoint_snapshot_test ${RECOVERPOINT_INGEST_VOL1_SRC} ${RECOVERPOINT_TGT_VARRAY} rpsnap2-${RP_MODIFIED_HOSTNAME}-${RANDOM} blocksnap2-${RP_MODIFIED_HOSTNAME}-${RANDOM}

        # now perform failover tests
        recoverpoint_failover_test "${RECOVERPOINT_INGEST_VOL1_SRC}" "${RECOVERPOINT_INGEST_VOL1_TGT}"
    fi

    # Add a volume to the protection
    run volume create "${RECOVERPOINT_INGEST_VOL1_SRC}"-2 ${PROJECT} ${RECOVERPOINT_VARRAY1} ${vpool} 8GB --consistencyGroup ViPR-"${RECOVERPOINT_INGEST_VOL1_CG}"

    if [ "${shorttest}" -eq "0" ]; then
        # Run snapshot tests
        recoverpoint_snapshot_test ${RECOVERPOINT_INGEST_VOL1_SRC} ${RECOVERPOINT_TGT_VARRAY} rpsnap3-${RP_MODIFIED_HOSTNAME}-${RANDOM} blocksnap3-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    fi

    # Remove a volume from the protection
    run_noundo volume delete ${PROJECT}/"${RECOVERPOINT_INGEST_VOL1_SRC}"-2 --wait

    # Inventory-only remove of RP volume 
    run_noundo volume delete ${PROJECT}/"${RECOVERPOINT_INGEST_VOL1_SRC}" --vipronly
    # COP-21592: inventory-only delete of the last volume deletes the CG
    # blockconsistencygroup show ViPR-${RECOVERPOINT_INGEST_VOL1_CG}
    run_noundo blockconsistencygroup delete ViPR-${RECOVERPOINT_INGEST_VOL1_CG} --vipronly

    # Run discovery
    recoverpoint_ingest_discovery

    # Re-ingest
    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        if [ "${shorttest}" -eq "0" ]; then
            # Ingest the volume, but then immediately inventory-only delete it, rediscover, and ingest it again.
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT}"
            run_noundo volume delete ${PROJECT}/"${RECOVERPOINT_INGEST_VOL1_TGT}" --vipronly --force
            # Recreate unmanaged volumes/protection sets

            # Run discovery
            recoverpoint_ingest_discovery
        fi

        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT_JRNL}"
        if [ "${vpool}" = "rpmp_cdp" ]; then
            # MP active production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL}"
            # MP target CDP on varray2
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT}"
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        fi
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_cdp $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
        # Verify the CG got created; fail script if this fails.
        run blockconsistencygroup show ViPR-"${RECOVERPOINT_INGEST_VOL1_CG}"
        if [ "${shorttest}" -eq "0" ]; then
            recoverpoint_auto_snapshot_cleanup_test "${RECOVERPOINT_INGEST_VOL1_SRC}" "${RECOVERPOINT_INGEST_VOL1_TGT}" ${RECOVERPOINT_INGEST_SNAPSHOT_VARRAY} ${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT}
        fi
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_crr $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_TGT_JRNL}"

        if [ "${vpool}" = "rpmp_crr" ]; then
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL}"
        fi

        if [ "${vpool}" = "rpvplex_crr" -o "${vpool}" = "rpmp_crr" ]; then
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        fi

        # Verify the CG got created; fail script if this fails.
        run blockconsistencygroup show ViPR-"${RECOVERPOINT_INGEST_VOL1_CG}"
        if [ "${shorttest}" -eq "0" ]; then
            recoverpoint_auto_snapshot_cleanup_test "${RECOVERPOINT_INGEST_VOL1_SRC}" "${RECOVERPOINT_INGEST_VOL1_TGT}" ${RECOVERPOINT_INGEST_SNAPSHOT_VARRAY}
        fi
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rp${rptypetag}_clr $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V1TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V1TGT_JRNL}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V3TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_V3TGT_JRNL}"

        if [ "${vpool}" = "rpmp_clr" ]; then
            # MP active production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_SRC_JRNL}"
            # MP target CDP on varray2
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT}"
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_STANDBY_TGT_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_VOL1_SRC_JRNL}"
        fi

        # Verify the CG got created; fail script if this fails.
        run blockconsistencygroup show ViPR-"${RECOVERPOINT_INGEST_VOL1_CG}"
        if [ "${shorttest}" -eq "0" ]; then
            recoverpoint_auto_snapshot_cleanup_test "${RECOVERPOINT_INGEST_VOL1_SRC}" "${RECOVERPOINT_INGEST_VOL1_TGT}" ${RECOVERPOINT_INGEST_SNAPSHOT_VARRAY} "${RECOVERPOINT_INGEST_VOL1_V3TGT}"
        fi
    fi

    if [ "${shorttest}" -eq "0" ]; then
        # Run snapshot tests
        recoverpoint_snapshot_test ${RECOVERPOINT_INGEST_VOL1_SRC} ${RECOVERPOINT_TGT_VARRAY} rpsnap4-${RP_MODIFIED_HOSTNAME}-${RANDOM} blocksnap4-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    fi

    # Real removal of volume/CG
    run_noundo volume delete ${PROJECT}/"${RECOVERPOINT_INGEST_VOL1_SRC}" --wait
    run_noundo blockconsistencygroup delete ViPR-${RECOVERPOINT_INGEST_VOL1_CG}
}

recoverpoint_ingest_export_test()
{
    secho 'RP ingest export tests'

    # Some more thought needs to go into this, how to integrate this into the regular test suite
    # and efficiently take into the account the test variables for RP.

    vpool=${1}

    # Debug only.  If you're diagnosing basic issues in ingestion, this flag cuts down the extra tests and keeps the testing 
    # to the basics of ingestion itself.
    shorttest=0

    # Create a host to export volumes to
    recoverpoint_ingest_host_setup ${vpool}

    # Set up labels
    RECOVERPOINT_INGEST_EXPORT_VOL1_CG=${RECOVERPOINT_INGEST_VOL1_CGBASE}${RP_SANITY_RANDOM}exp_${vpool}
    RECOVERPOINT_INGEST_EXPORT_VOLBASE=${RECOVERPOINT_INGEST_VOL1BASE}${RP_SANITY_RANDOM}exp
    RECOVERPOINT_INGEST_EXPORT_VOL1_SRC=${RECOVERPOINT_INGEST_EXPORT_VOLBASE}_${vpool}

    # Create the consistency group and volume
    run blockconsistencygroup create ${PROJECT} ${RECOVERPOINT_INGEST_EXPORT_VOL1_CG}  --noarrayconsistency
    run volume create ${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC} ${PROJECT} ${RECOVERPOINT_VARRAY1} ${vpool} 1GB --consistencyGroup ${RECOVERPOINT_INGEST_EXPORT_VOL1_CG}

    # Export the volume to a host
    run export_group create ${PROJECT} EG${vpool} ${RECOVERPOINT_VARRAY1} --type Host --volspec "${PROJECT}/${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --hosts "${RP_HOSTNAME}1,${RP_HOSTNAME}2"

    # Inventory-only delete the volume, export, and CG
    run volume delete ${PROJECT}/${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC} --vipronly
    run export_group delete ${PROJECT}/EG${vpool}
    run blockconsistencygroup delete ${RECOVERPOINT_INGEST_EXPORT_VOL1_CG} --vipronly

    RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY1}-journal-1
    RECOVERPOINT_INGEST_SNAPSHOT_VARRAY=${RECOVERPOINT_VARRAY1}

    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        RECOVERPOINT_INGEST_EXPORT_VOL1_TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-target-${RECOVERPOINT_VARRAY1}
        RECOVERPOINT_INGEST_EXPORT_VOL1_TGT_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY1}-journal-2

        if [ "${vpool}" = "rpmp_cdp" ]; then
                # Also need to ingest the MP standby production journal, varray2 target and journal volumes as well.
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_SRC_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-1
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-target-${RECOVERPOINT_VARRAY2}
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-2
        fi
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        RECOVERPOINT_INGEST_EXPORT_VOL1_TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-target-${RECOVERPOINT_VARRAY3}
        RECOVERPOINT_INGEST_EXPORT_VOL1_TGT_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY3}-journal-1
        RECOVERPOINT_INGEST_SNAPSHOT_VARRAY=${RECOVERPOINT_VARRAY3}
        if [ "${vpool}" = "rpmp_crr" ]; then
            # Also need to ingest the MP standby production journal, varray2 target and journal volumes as well.
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_SRC_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-1
        fi
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        RECOVERPOINT_INGEST_EXPORT_VOL1_V1TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-target-${RECOVERPOINT_VARRAY1}
        RECOVERPOINT_INGEST_EXPORT_VOL1_V1TGT_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY1}-journal-2
        RECOVERPOINT_INGEST_EXPORT_VOL1_V3TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-target-${RECOVERPOINT_VARRAY3}
        RECOVERPOINT_INGEST_EXPORT_VOL1_V3TGT_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY3}-journal-1

        if [ "${vpool}" = "rpmp_clr" ]; then
            # Also need to ingest the MP standby production journal, varray2 target and journal volumes as well.
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_SRC_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-1
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-target-${RECOVERPOINT_VARRAY2}
            RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT_JRNL=${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}-${RECOVERPOINT_VARRAY2}-journal-2
        fi

        # For post-ingest operations
        RECOVERPOINT_INGEST_EXPORT_VOL1_TGT=${RECOVERPOINT_INGEST_EXPORT_VOL1_V1TGT}
        RECOVERPOINT_INGEST_SNAPSHOT_VARRAY=${RECOVERPOINT_VARRAY3}
    fi

    rp_target_vpool=rp_targets
    if [ "${RPXIO_TESTS}" = "1" ]; then
        rp_target_vpool=rpxio_targets
    fi

    # Run discovery
    recoverpoint_ingest_discovery

    rpsnap=rpsnap_for_validation-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    if [ "${vpool}" = "rp${rptypetag}_cdp" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_TGT_JRNL}"
        if [ "${vpool}" = "rpmp_cdp" ]; then
            # MP active production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL}"
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_SRC_JRNL}"
            # MP target CDP on varray2
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT}"
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL}"
        fi
        run unmanagedvolume ingest_export ${RECOVERPOINT_VARRAY1} rp${rptypetag}_cdp $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --host ${RP_HOSTNAME}1
        run unmanagedvolume ingest_export ${RECOVERPOINT_VARRAY1} rp${rptypetag}_cdp $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --host ${RP_HOSTNAME}2
        RECOVERPOINT_TGT_VARRAY=${RECOVERPOINT_VARRAY1}
    elif [ "${vpool}" = "rp${rptypetag}_crr" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_TGT_JRNL}"
        if [ "${vpool}" = "rpmp_crr" ]; then
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_SRC_JRNL}"
        fi
        if [ "${vpool}" = "rpvplex_crr" -o "${vpool}" = "rpmp_crr" ]; then
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL}"
        fi
        run unmanagedvolume ingest_export ${RECOVERPOINT_VARRAY1} rp${rptypetag}_crr $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --host ${RP_HOSTNAME}1
        run unmanagedvolume ingest_export ${RECOVERPOINT_VARRAY1} rp${rptypetag}_crr $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --host ${RP_HOSTNAME}2
        RECOVERPOINT_TGT_VARRAY=${RECOVERPOINT_VARRAY3}
    elif [ "${vpool}" = "rp${rptypetag}_clr" ]; then
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_V1TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_V1TGT_JRNL}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_V3TGT}"
        run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY3} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_V3TGT_JRNL}"

        if [ "${vpool}" = "rpmp_clr" ]; then
            # MP active production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL}"
            # MP standby production journal
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rpvplexlocal_noprotection $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_SRC_JRNL}"
            # MP target CDP on varray2
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT}"
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY2} rp_targets $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_STANDBY_TGT_JRNL}"
        else
            run unmanagedvolume ingest_unexport ${RECOVERPOINT_VARRAY1} ${rp_target_vpool} $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC_JRNL}"
        fi

        run unmanagedvolume ingest_export ${RECOVERPOINT_VARRAY1} rp${rptypetag}_clr $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --host ${RP_HOSTNAME}1
        run unmanagedvolume ingest_export ${RECOVERPOINT_VARRAY1} rp${rptypetag}_clr $PROJECT --volspec "${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --host ${RP_HOSTNAME}2
        RECOVERPOINT_TGT_VARRAY=${RECOVERPOINT_VARRAY1}
    fi

    # Verify the CG got created; fail script if this fails.
    run blockconsistencygroup show ViPR-"${RECOVERPOINT_INGEST_EXPORT_VOL1_CG}"

    SRCUID=`volume list ${PROJECT} | grep ${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC} | awk '{print $7}'`

    if [ "${shorttest}" -eq "0" ]; then
        # Run snapshot tests
        recoverpoint_snapshot_test ${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC} ${RECOVERPOINT_TGT_VARRAY} rpsnap5-${RP_MODIFIED_HOSTNAME}-${RANDOM} blocksnap5-${RP_MODIFIED_HOSTNAME}-${RANDOM}
    fi

    # TBD: Add volume group snapshot creation, snap, delete support here.
    #run volumegroup create source-vgroup copy1 COPY
    #run volumegroup add-volumes source-vgroup ${SRCUID}
    #run blocksnapshot create ${PROJECT}/${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC} ${blocksnap}
    #run blocksnapshot activate ${PROJECT}/${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}/${blocksnap}
    #run blocksnapshot delete ${PROJECT}/${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}/${blocksnap}

    # Real removal of volume/CG
    run_noundo export_group delete ${PROJECT}/${RP_HOSTNAME}1
    run_noundo export_group delete ${PROJECT}/${RP_HOSTNAME}2
    run_noundo volume delete ${PROJECT}/"${RECOVERPOINT_INGEST_EXPORT_VOL1_SRC}" --wait
    run_noundo blockconsistencygroup delete ViPR-${RECOVERPOINT_INGEST_EXPORT_VOL1_CG}
}

recoverpoint_ingest()
{
    rptypetag=${1}

    # Ingest specific tests
    secho "RP ingest ${rptypetag} volumes"

    if [ "$RP_CDP" = "1" ] ; then
        recoverpoint_ingest_test rp${rptypetag}_cdp
        recoverpoint_ingest_export_test rp${rptypetag}_cdp
    fi

    if [ "$RP_CRR" = "1" ] ; then
        recoverpoint_ingest_test rp${rptypetag}_crr
        recoverpoint_ingest_export_test rp${rptypetag}_crr
    fi
    
    if [ "$RP_CLR" = "1" ] ; then
        if [ "$RPMP_CLR" = "1" ] ; then
            secho "RP MetroPoint CLR Test is currently not working due to a placement error.  Bypassing."
            return;
        fi
        recoverpoint_ingest_test rp${rptypetag}_clr
        recoverpoint_ingest_export_test rp${rptypetag}_clr        
    fi

    secho "RP ingest ${rptypetag} tests complete"
}

recoverpoint_failover_test()
{
    if [ "$RP_FAILOVER_TESTS" = "1" ]; then
        secho 'Running RecoverPoint Failover Tests'

        src_volume=${1}
        tgt_volume=${2}

        secho 'Verify the current state of the source and target vols'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
        run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC

        secho 'Failover'
        run volume change_link ${PROJECT}/${src_volume} failover ${PROJECT}/${tgt_volume} rp

        secho 'Verify post failover'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
        run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER

        secho 'Cancel the failover'
        run volume change_link ${PROJECT}/${src_volume} failover-cancel ${PROJECT}/${tgt_volume} rp

        secho 'Verify post cancel, everything should be back to the start'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
        run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC

        secho 'Failover again'
        run volume change_link ${PROJECT}/${src_volume} failover ${PROJECT}/${tgt_volume} rp

        secho 'Verify post failover'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
        run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER

        secho 'Swap'
        run volume change_link ${PROJECT}/${src_volume} swap ${PROJECT}/${tgt_volume} rp

        secho 'Verify post swap'
        run volume verify ${PROJECT}/${src_volume} personality TARGET
        run volume verify ${PROJECT}/${src_volume} access_state NOT_READY
        run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
        run volume verify ${PROJECT}/${tgt_volume} personality SOURCE
        run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
        run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC

        # Expand not working yet, might make separate test for just expand
        #run volume expand ${PROJECT}/${tgt_volume} ${RP_SIZE_EXPAND}

        secho 'Failover once again, post swap'
        run volume change_link ${PROJECT}/${tgt_volume} failover ${PROJECT}/${src_volume} rp

        secho 'Verify post failover, post swap'
        run volume verify ${PROJECT}/${src_volume} personality TARGET
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
        run volume verify ${PROJECT}/${tgt_volume} personality SOURCE
        run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
        run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER

        secho 'Swap back'
        run volume change_link ${PROJECT}/${tgt_volume} swap ${PROJECT}/${src_volume} rp

        secho 'Verify post swap back, everything should be back to normal'
        run volume verify ${PROJECT}/${src_volume} personality SOURCE
        run volume verify ${PROJECT}/${src_volume} access_state READWRITE
        run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
        run volume verify ${PROJECT}/${tgt_volume} personality TARGET
        run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
        run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
    else
        secho 'Skipping RecoverPoint Failover Tests'
   fi
}

recoverpoint_cg_failover_test()
{
    if [ "$RP_CG_FAILOVER_TESTS" = "1" ]; then
        secho 'Running RecoverPoint Failover CG Tests'

        # The name of the consistency group to swap
        cg_name=${1}
        # The source source volume - an arbitrary source volume from the CG
        src_volume=${2}
        # The virtual array corresonding to the source volume (src_volume)
        src_varray=${3}
        # The target volume name used to identify all target volumes
        tgt_volumes=${4}
        # The swap target virtual arrays
        tgt_varrays=${5}

        secho 'RP consistency group failover/swap tests'

        # Test the CG swap and swap-back for every target virtual array
        for tgt_varray in ${tgt_varrays}
        do
          secho 'Verify the current state of the source and target vols'
          run volume verify ${PROJECT}/${src_volume} personality SOURCE
          run volume verify ${PROJECT}/${src_volume} access_state READWRITE
          run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
      
          for tgt_volume in ${tgt_volumes}
          do
            run volume verify ${PROJECT}/${tgt_volume} personality TARGET
            run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
            run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
          done
      
          run blockconsistencygroup failover $cg_name --copyType rp --targetVarray $tgt_varray
      
          run volume verify ${PROJECT}/${src_volume} personality SOURCE
          run volume verify ${PROJECT}/${src_volume} access_state READWRITE
          run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
      
          # Post-failover target verification
          for tgt_volume in ${tgt_volumes}
          do
            # If the volume corresponds to the target varray used for the failover we
            # need to verify different values
            if [[ $tgt_volume == *$tgt_varray* ]]; then
                run volume verify ${PROJECT}/${tgt_volume} personality TARGET
                run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
                run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER
            else
                run volume verify ${PROJECT}/${tgt_volume} personality TARGET
                run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
                run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
            fi
          done
      
          run blockconsistencygroup failover_cancel $cg_name --copyType rp --targetVarray $tgt_varray

          run volume verify ${PROJECT}/${src_volume} personality SOURCE
          run volume verify ${PROJECT}/${src_volume} access_state READWRITE
          run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
      
          for tgt_volume in ${tgt_volumes}
          do
            run volume verify ${PROJECT}/${tgt_volume} personality TARGET
            run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
            run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
          done
      
          run blockconsistencygroup failover $cg_name --copyType rp --targetVarray $tgt_varray
      
          run volume verify ${PROJECT}/${src_volume} personality SOURCE
          run volume verify ${PROJECT}/${src_volume} access_state READWRITE
          run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
      
          # Post-failover target verification
          for tgt_volume in ${tgt_volumes}
          do
            # If the volume corresponds to the target varray used for the failover we
            # need to verify different values
            if [[ $tgt_volume == *$tgt_varray* ]]; then
                run volume verify ${PROJECT}/${tgt_volume} personality TARGET
                run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
                run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER
            else
                run volume verify ${PROJECT}/${tgt_volume} personality TARGET
                run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
                run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
            fi
          done
      
          run blockconsistencygroup swap $cg_name --copyType rp --targetVarray $tgt_varray

          # Post-swap source verification
          run volume verify ${PROJECT}/${src_volume} personality TARGET
          run volume verify ${PROJECT}/${src_volume} access_state NOT_READY
          run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
      
          # Post-swap target verification
          for tgt_volume in ${tgt_volumes}
          do
            run volume verify ${PROJECT}/${tgt_volume} personality SOURCE
            run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
            run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
          done
      
          run blockconsistencygroup failover $cg_name --copyType rp --targetVarray $tgt_varray

          run volume verify ${PROJECT}/${src_volume} personality TARGET
          run volume verify ${PROJECT}/${src_volume} access_state READWRITE
          run volume verify ${PROJECT}/${src_volume} link_status FAILED_OVER
      
          for tgt_volume in ${tgt_volumes}
          do
            # If the volume corresponds to the target varray used for the failover we
            # need to verify different values
            if [[ $tgt_volume == *$tgt_varray* ]]; then
                run volume verify ${PROJECT}/${tgt_volume} personality SOURCE
                run volume verify ${PROJECT}/${tgt_volume} access_state READWRITE
                run volume verify ${PROJECT}/${tgt_volume} link_status FAILED_OVER
            else
                run volume verify ${PROJECT}/${tgt_volume} personality SOURCE
                run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
                run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
            fi
          done
      
          run blockconsistencygroup swap $cg_name --copyType rp --targetVarray $src_varray

          # swap-back source verification
          run volume verify ${PROJECT}/${src_volume} personality SOURCE
          run volume verify ${PROJECT}/${src_volume} access_state READWRITE
          run volume verify ${PROJECT}/${src_volume} link_status IN_SYNC
      
          # swap-back target verification
          for tgt_volume in ${tgt_volumes}
          do
            run volume verify ${PROJECT}/${tgt_volume} personality TARGET
            run volume verify ${PROJECT}/${tgt_volume} access_state NOT_READY
            run volume verify ${PROJECT}/${tgt_volume} link_status IN_SYNC
          done
        done
    else
        secho 'Skipping RecoverPoint Failover CG Tests'
    fi
}

recoverpoint_vpool_change_test()
{
    # Run change vpool tests on the volume if the flag is set.
    # These tests will:
    #  1. Remove protection from the RP volume.
    #  2. Add protection to the now unprotected volume.
    #  3. Upgrade to MP (only in the case of MP CRR tests - for now)

    if [ "$RP_CHANGE_VPOOL" = "1" ]; then
        secho 'Running RecoverPoint Change Virtual Pool Tests'

        base_volume=${1}
        noprotection_vpool=${2}
        rp_vpool=${3}
        rp_cg=${4}
        upgrade_to_mp=${5}

        # Perform Upgrade to MP if the variable is present,
        # Only RP+VPLEX Distributed CRR is eligible for this operation (for now).
        if [ a${upgrade_to_mp} != a ] ; then
            secho 'Change Virtual Pool - Upgrade To MetroPoint'
            run volume change_cos ${PROJECT}/${base_volume} ${upgrade_to_mp}
        fi
        secho 'Change Virtual Pool - Remove RecoverPoint Protection'
        run volume change_cos ${PROJECT}/${base_volume} ${noprotection_vpool}

        secho 'Change Virtual Pool - Add RecoverPoint Protection'
        run volume change_cos ${PROJECT}/${base_volume} ${rp_vpool} --consistencyGroup ${rp_cg}

    else
        secho 'Skipping RecoverPoint Change Virtual Pool Tests'
    fi
}

recoverpoint_vpool_change_replication_mode_test()
{
    if [ "$RP_CHANGE_REPLICATION_MODE_TESTS" = "1" ]; then
        secho 'Running RecoverPoint Change Replication Mode Tests'

        base_volume=${1}
        target_vpool=${2}

        secho 'Change Virtual Pool - Update Replication Mode'
        run volume change_cos ${PROJECT}/${base_volume} ${target_vpool}
    else
        secho 'Skipping RecoverPoint Change Replication Mode Tests'
    fi
}

recoverpoint_add_journal_volume()
{
    if [ "$RP_ADD_JOURNAL_TESTS" = "1" ]; then    
        secho 'Running RecoverPoint Add Journal Volume Tests'

        copy_name=${1}
        journ_varray=${2} 
        journ_vpool=${3}     
        rp_cg=${4}

        secho 'RP add journal volume'
        run volume add_journal ${copy_name} $PROJECT ${journ_varray} ${journ_vpool} 10GB --consistencyGroup ${rp_cg} --count=1  
    else
        secho 'Skipping RecoverPoint Add Journal Volume Tests'
    fi
}

recoverpoint_simulator_varsetup() {
    if [ "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        # VPLEX
        VPLEX_DEV_NAME=$VPLEX_SIMULATOR
        VPLEX_IP=$SIMULATOR_IP
        RECOVERPOINT_VPLEX_A=${VPLEX_DEV_NAME}-local
        RECOVERPOINT_VPLEX_B=${VPLEX_DEV_NAME}-remote
        RECOVERPOINT_VPLEX_A_GUID=${VPLEX1_SIMULATOR_GUID}
        RECOVERPOINT_VPLEX_B_GUID=${VPLEX2_SIMULATOR_GUID}
        RECOVERPOINT_STORAGE_ARRAY_A_GUID=${VMAX1_SIMULATOR_NATIVEGUID}
        RECOVERPOINT_STORAGE_ARRAY_B_GUID=${VMAX2_SIMULATOR_NATIVEGUID}
        RECOVERPOINT_STORAGE_ARRAY_C_GUID=${VMAX3_SIMULATOR_NATIVEGUID}
        RECOVERPOINT_STORAGE_ARRAY_D_GUID=${VMAX4_SIMULATOR_NATIVEGUID}
    fi
    if [ "${RPXIO_TESTS}" = "1" ]; then
        RECOVERPOINT_XTREMIO_GUID=${XIO_4X_SIM_NATIVEGUID}
    fi
}

# Simulator setup
recoverpoint_simulator_setup()
{
    secho 'RecoverPoint Simulator running'
    project_setup
    rp_varray_setup

    run syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop controller_discovery_refresh_interval 5

    # Simulators being used
    run networksystem create $FABRIC_SIMULATOR  mds --devip $SIMULATOR_CISCO_MDS_IP --devport 22 --username $RP_FABRIC_SIM_USER --password $RP_FABRIC_SIM_PW
    run smisprovider create $PROVIDER_SIMULATOR $RP_PROVIDER_SIMULATOR_IP $RP_VMAX_SMIS_SIM_PORT $SMIS_USER "$SMIS_PASSWD" false
    
    if [ "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        storageprovider show ${RECOVERPOINT_VPLEX_A} &> /dev/null && return $?
        run storageprovider create ${RECOVERPOINT_VPLEX_A} $VPLEX_IP ${VPLEX1_SIMULATOR_PORT} $VPLEX_USER "$VPLEX_PASSWD" vplex
        if [ "${RP_CRR}" = "1" -o "${RP_CLR}" = "1" ]; then
            run storageprovider create ${RECOVERPOINT_VPLEX_B} $VPLEX_IP ${VPLEX2_SIMULATOR_PORT} $VPLEX_USER "$VPLEX_PASSWD" vplex
        fi
        rp_port_setup
    fi
    if [ "${RPXIO_TESTS}" = "1" ]; then
        run storageprovider create xtremeio1 $XIO_SIMULATOR_IP ${XIO_4X_SIMULATOR_PORT} root root xtremio
    fi

    run storagedevice discover_all
    run transportzone assign VSAN_11 ${RECOVERPOINT_VARRAY1}
    run transportzone assign VSAN_12 ${RECOVERPOINT_VARRAY2}
    run transportzone assign VSAN_13 ${RECOVERPOINT_VARRAY3}

    POOLS_AUTO_MATCH=false

    RECOVERPOINT_IP=$RP_SIMULATOR_IP
    rp_protection_system_setup

    # Make sure all of the pools are updated and matched
    run storagedevice discover_all

    run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster site_1rp1 --addvarrays $RECOVERPOINT_VARRAY1
    run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster site_2rp1 --addvarrays $RECOVERPOINT_VARRAY2
    run protectionsystem update --ip ${remote_ip} --name $RECOVERPOINT --cluster site_3rp1 --addvarrays $RECOVERPOINT_VARRAY3

    run storagepool update $VMAX1_SIMULATOR_NATIVEGUID --nhadd $RECOVERPOINT_VARRAY1 --type block
    run storagepool update $VMAX2_SIMULATOR_NATIVEGUID --nhadd $RECOVERPOINT_VARRAY2 --type block
    run storagepool update $VMAX3_SIMULATOR_NATIVEGUID --nhadd $RECOVERPOINT_VARRAY3 --type block
    if [ "${RPXIO_TESTS}" = "1" ]; then
        run storagepool update $XIO_4X_SIM_NATIVEGUID --nhadd $RECOVERPOINT_VARRAY1 --type block
    fi

    # Simulator adjustments to cos (shouldn't be needed but RP simulator only sees a subset of what's on the network)
    rp_vpool_setup

    if [ "${RP_TESTS}" = "1" -o "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        run cos update block rp_targets --storage $VMAX1_SIMULATOR_NATIVEGUID
        run cos update block rp_targets --storage $VMAX2_SIMULATOR_NATIVEGUID
        run cos update block rp_targets --storage $VMAX3_SIMULATOR_NATIVEGUID
        run cos update block rp_noprotection --storage $VMAX1_SIMULATOR_NATIVEGUID
        run cos update block rp_noprotection --storage $VMAX2_SIMULATOR_NATIVEGUID
        run cos update block rp_noprotection --storage $VMAX3_SIMULATOR_NATIVEGUID

        if [ "${RP_CDP}" = "1" ]; then
            run cos update block rp_cdp --storage $VMAX1_SIMULATOR_NATIVEGUID
            run cos update block rp_cdp-sync --storage $VMAX1_SIMULATOR_NATIVEGUID
        fi
        if [ "${RP_CRR}" = "1" ]; then
            run cos update block rp_crr --storage $VMAX1_SIMULATOR_NATIVEGUID
            run cos update block rp_crr --storage $VMAX3_SIMULATOR_NATIVEGUID
        fi
        if [ "${RP_CLR}" = "1" ]; then
            run cos update block rp_clr --storage $VMAX1_SIMULATOR_NATIVEGUID
            run cos update block rp_clr --storage $VMAX3_SIMULATOR_NATIVEGUID
        fi
    fi
    if [ "${RPVPLEX_TESTS}" = "1" -o "${RPMP_TESTS}" = "1" ]; then
        run cos update block rpvplex_targets --storage $VMAX1_SIMULATOR_NATIVEGUID
        run cos update block rpvplex_targets --storage $VMAX2_SIMULATOR_NATIVEGUID
        run cos update block rpvplex_targets --storage $VMAX3_SIMULATOR_NATIVEGUID
        run cos update block rpvplex_ha --storage $VMAX1_SIMULATOR_NATIVEGUID
        run cos update block rpvplex_ha --storage $VMAX2_SIMULATOR_NATIVEGUID
        run cos update block rpvplex_ha --storage $VMAX3_SIMULATOR_NATIVEGUID
        run cos update block rpvplexlocal_noprotection --storage $VMAX1_SIMULATOR_NATIVEGUID
        run cos update block rpvplexlocal_noprotection --storage $VMAX2_SIMULATOR_NATIVEGUID
        run cos update block rpvplexlocal_noprotection --storage $VMAX3_SIMULATOR_NATIVEGUID
        run cos update block rpvplexdist_noprotection --storage $VMAX1_SIMULATOR_NATIVEGUID
        run cos update block rpvplexdist_noprotection --storage $VMAX2_SIMULATOR_NATIVEGUID
        run cos update block rpvplexdist_noprotection --storage $VMAX3_SIMULATOR_NATIVEGUID

        if [ "${RP_CDP}" = "1" ]; then
            run cos update block rpvplex_cdp --storage $VMAX1_SIMULATOR_NATIVEGUID
            run cos update block rpvplex_cdp --storage $VMAX2_SIMULATOR_NATIVEGUID
            run cos update block rpvplex_cdp --storage $VMAX3_SIMULATOR_NATIVEGUID

            run cos update block rpmp_cdp --storage $VMAX1_SIMULATOR_NATIVEGUID
            run cos update block rpmp_cdp --storage $VMAX2_SIMULATOR_NATIVEGUID
            run cos update block rpmp_cdp --storage $VMAX3_SIMULATOR_NATIVEGUID
        fi
        if [ "${RP_CRR}" = "1" ]; then
            run cos update block rpvplex_crr --storage $VMAX1_SIMULATOR_NATIVEGUID
            run cos update block rpvplex_crr --storage $VMAX2_SIMULATOR_NATIVEGUID
            run cos update block rpvplex_crr --storage $VMAX3_SIMULATOR_NATIVEGUID

            run cos update block rpmp_crr --storage $VMAX1_SIMULATOR_NATIVEGUID
            run cos update block rpmp_crr --storage $VMAX2_SIMULATOR_NATIVEGUID
            run cos update block rpmp_crr --storage $VMAX3_SIMULATOR_NATIVEGUID
        fi
        if [ "${RP_CLR}" = "1" ]; then
            run cos update block rpvplex_clr --storage $VMAX1_SIMULATOR_NATIVEGUID
            run cos update block rpvplex_clr --storage $VMAX2_SIMULATOR_NATIVEGUID
            run cos update block rpvplex_clr --storage $VMAX3_SIMULATOR_NATIVEGUID

            run cos update block rpmp_clr --storage $VMAX1_SIMULATOR_NATIVEGUID
            run cos update block rpmp_clr --storage $VMAX2_SIMULATOR_NATIVEGUID
            run cos update block rpmp_clr --storage $VMAX3_SIMULATOR_NATIVEGUID
        fi
    fi
    if [ "${RPXIO_TESTS}" = "1" ]; then
        run cos update block rpxio_targets --storage $XIO_4X_SIM_NATIVEGUID
        run cos update block rp_noprotection --storage $XIO_4X_SIM_NATIVEGUID
        run cos update block rpxio_cdp --storage $XIO_4X_SIM_NATIVEGUID
    fi
}


recoverpoint_common_setup()
{
    project_setup
    rp_varray_setup
    rp_network_setup
    rp_storage_setup
    rp_port_setup
    rp_protection_system_setup
    rp_isolate_rpa_clusters
    rp_vpool_setup
}

recoverpoint_setup()
{
    POOLS_AUTO_MATCH=true
    if [ "$RP_QUICK_PARAM" = "quick" ]; then
        recoverpoint_simulator_varsetup
        if [ "$RP_RUN_SETUP" = "1" ]; then
            secho 'RecoverPoint Simulator setup'
            recoverpoint_simulator_setup
        else
            # This is intentionally a local-only execution for now.  Regular sanity for RP should always have RP_RUN_SETUP turned on.
            RP_SANITY_RANDOM=`/opt/storageos/bin/dbutils list UnManagedVolume | grep label | awk '{print $3}' | cut -c10-12 | head -1`
            secho "Determine existing volume label uses random value: ${RP_SANITY_RANDOM}"
        fi
    elif [ "$RP_RUN_SETUP" = "1" ]; then
        secho 'RecoverPoint setup'
        recoverpoint_common_setup
    fi
}

recoverpoint_cleanup_volumes()
{
    vol=${1}
    volcount=${2}

    # Cleanup volumes
    if [ "$volcount" = "1" ]; then
        run volume delete $PROJECT/${vol} --wait
    else
        for (( i=1; i<=$volcount; i++ ))
        do
            run volume delete $PROJECT/${vol}-$i --wait
        done
    fi
}

recoverpoint_cleanup_cgs()
{
    # Cleanup CGs
    if [ "${RP_INGESTTEST}" = "0" ]; then
        if [ "${RP_TESTS}" = "1" ]; then
            run blockconsistencygroup delete $RP_CONSISTENCY_GROUP
        fi

        if [ "${RPVPLEX_TESTS}" = "1" ]; then
            run blockconsistencygroup delete $RP_VPLEX_CONSISTENCY_GROUP
        fi

        if [ "${RPMP_TESTS}" = "1" ]; then
            run blockconsistencygroup delete $RP_METROPOINT_CONSISTENCY_GROUP
        fi

        if [ "${RPXIO_TESTS}" = "1" ]; then
            run blockconsistencygroup delete $RP_XIO_CONSISTENCY_GROUP
        fi
    fi
}

recoverpoint_tests()
{
    if [ "$RP_RUN_TESTS" = "1" ]; then
        secho 'Run RP tests'

        # Setup CGs
        rp_cg_setup

        # Default source varrays
        rp_src_varray=$RECOVERPOINT_VARRAY1
        rpvplex_src_varray=$RECOVERPOINT_VARRAY1
        rpmp_src_varray=$RECOVERPOINT_VARRAY1
	rpxio_src_varray=$RECOVERPOINT_VARRAY1

        # Vpools for change vpool - Remove Protection
        rp_noprotection_vpool='rp_noprotection'
        rpvplex_noprotection_vpool='rpvplexdist_noprotection'
        rpmp_noprotection_vpool='rpvplexdist_noprotection'
        rpxio_noprotection_vpool='rp_noprotection'

        rp_type=''

        if [ "$RP_CDP" = "1" ]; then
            secho 'RP CDP Tests'
            rp_type='cdp'

            # RP
            rp_tgt_varray=$RECOVERPOINT_VARRAY1
            rp_tgt_varrays=($RECOVERPOINT_VARRAY1)
            rp_tgt_vpool='rp_targets'

            # RP+VPLEX
            rpvplex_tgt_varray=$RECOVERPOINT_VARRAY1
            rpvplex_tgt_varrays=($RECOVERPOINT_VARRAY1)
            rpvplex_tgt_vpool='rpvplex_targets'
            # RP+VPLEX CDP uses VPLEX local, set as the no protection vpool
            rpvplex_noprotection_vpool='rpvplexlocal_noprotection'

            # MP
            rpmp_active_tgt_varray=$RECOVERPOINT_VARRAY1
            rpmp_standby_tgt_varray=$RECOVERPOINT_VARRAY2
            rpmp_tgt_varrays=($RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2)
            rpmp_tgt_vpool='rp_targets'
            
            # XIO
            rpxio_tgt_varray=$RECOVERPOINT_VARRAY1
            rpxio_tgt_varrays=($RECOVERPOINT_VARRAY1)
            rpxio_tgt_vpool='rpxio_targets'
        fi

        if [ "$RP_CRR" = "1" ]; then
            secho 'RP CRR Tests'
            rp_type='crr'

            # RP
            rp_tgt_varray=$RECOVERPOINT_VARRAY3
            rp_tgt_varrays=($RECOVERPOINT_VARRAY3)
            rp_tgt_vpool='rp_targets'

            # RP+VPLEX
            rpvplex_tgt_varray=$RECOVERPOINT_VARRAY3
            rpvplex_tgt_varrays=($RECOVERPOINT_VARRAY3)
            rpvplex_tgt_vpool='rpvplex_targets'

            # MP
            rpmp_active_tgt_varray=$RECOVERPOINT_VARRAY3
            rpmp_tgt_varrays=($RECOVERPOINT_VARRAY3)
            rpmp_standby_tgt_varray=''
            rpmp_tgt_vpool='rp_targets'

            # XIO
            rpxio_tgt_varray=$RECOVERPOINT_VARRAY1
            rpxio_tgt_varrays=($RECOVERPOINT_VARRAY1)
            rpxio_tgt_vpool='rpxio_targets'
        fi

        if [ "$RP_CLR" = "1" ]; then
            secho 'RP CLR Tests'
            rp_type='clr'

            # RP
            rp_tgt_varray=$RECOVERPOINT_VARRAY3
            rp_tgt_varrays=($RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY3)
            rp_tgt_vpool='rp_targets'

            # RP+VPLEX
            rpvplex_tgt_varray=$RECOVERPOINT_VARRAY1
            rpvplex_tgt_varrays=($RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY3)
            rpvplex_tgt_vpool='rpvplex_targets'

            # MP
            rpmp_active_tgt_varray=$RECOVERPOINT_VARRAY3
            rpmp_tgt_varrays=($RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY2 $RECOVERPOINT_VARRAY3)
            rpmp_standby_tgt_varray=''
            rpmp_tgt_vpool='rp_targets'

            # XIO
            rpxio_tgt_varray=$RECOVERPOINT_VARRAY3
            rpxio_tgt_varrays=($RECOVERPOINT_VARRAY1 $RECOVERPOINT_VARRAY3)
            rpxio_tgt_vpool='rpxio_targets'
        fi

        # RP
        rp_src_vpool='rp_'$rp_type
        rp_src_vpool_sync=${rp_src_vpool}'-sync'
        rpvolume=${RP_VOLUME}
        rpvolumetarget=${rpvolume}'-target-'${rp_tgt_varray}

        # RP+VPLEX
        rpvplex_src_vpool='rpvplex_'$rp_type
        rpvplex_src_vpool_sync=${rpvplex_src_vpool}'-sync'
        rpvplexvolume=${RP_VPLEX_VOLUME}
        rpvplexvolumetarget=${rpvplexvolume}'-target-'${rpvplex_tgt_varray}

        # MP
        rpmp_src_vpool='rpmp_'$rp_type
        rpmp_src_vpool_sync=${rpmp_src_vpool}'-sync'
        rpmpvolume=${RP_METROPOINT_VOLUME}
        rpmpvolumetarget=${mpvolume}'-target-'${rpmp_active_tgt_varray}

        # XIO
        rpxio_src_vpool='rpxio_'$rp_type
        rpxio_src_vpool_sync=${rpxio_src_vpool}'-sync'
        rpxiovolume=${RP_XIO_VOLUME}
        rpxiovolumetarget=${xiovolume}'-target-'${rpxio_active_tgt_varray}

        # Ingestion Tests
        if [ "$RP_INGESTTESTS" = "1" ]; then
            if [ "$RP_TESTS" = "1" ]; then
                recoverpoint_ingest ""
            fi

            if [ "$RPVPLEX_TESTS" = "1" ]; then
                recoverpoint_ingest "vplex"
            fi
            
            if [ "${RPMP_TESTS}" = "1" ]; then
                recoverpoint_ingest "mp"
            fi

            if [ "${RPXIO_TESTS}" = "1" ]; then
                recoverpoint_ingest "xio"
            fi
            
            # For now, if ingestion is turned on, complete the tests.  Once everything is stable, ingestion should just be part of normal RP testing.
            return
        fi

        # RP Volume create
        if [ "$RP_TESTS" = "1" ]; then
            secho 'RP volume create...'
            run volume create ${rpvolume} $PROJECT ${rp_src_varray} ${rp_src_vpool} $RP_VOLUME_SIZE --consistencyGroup $RP_CONSISTENCY_GROUP --count=$RP_VOLUME_COUNT
        fi

        if [ "$RPVPLEX_TESTS" = "1" ]; then
            secho 'RP+VPLEX volume create...'
            run volume create ${rpvplexvolume} $PROJECT ${rpvplex_src_varray} ${rpvplex_src_vpool} $RP_VOLUME_SIZE --consistencyGroup $RP_VPLEX_CONSISTENCY_GROUP --count=$RP_VOLUME_COUNT
        fi

        if [ "$RPMP_TESTS" = "1" ]; then
            secho 'MetroPoint volume create...'
            run volume create ${rpmpvolume} $PROJECT ${rpmp_src_varray} ${rpmp_src_vpool} $RP_VOLUME_SIZE --consistencyGroup $RP_METROPOINT_CONSISTENCY_GROUP --count=$RP_VOLUME_COUNT
        fi

        if [ "$RPXIO_TESTS" = "1" ]; then
            secho 'XIO volume create...'
            run volume create ${rpxiovolume} $PROJECT ${rpxio_src_varray} ${rpxio_src_vpool} $RP_VOLUME_SIZE --consistencyGroup $RP_XIO_CONSISTENCY_GROUP --count=$RP_VOLUME_COUNT
        fi

        # Have to account for "-1" being added to volume name if
        # volume count is greater than 1.
        volume_name_modifier=''
        if [ "$RP_VOLUME_COUNT" = "1" ]; then
            volume_name_modifier=''
        else
            volume_name_modifier='-1'
        fi

        # Compile a list of target volume names that have been created for RP
        rp_tgt_volumes=()
        for tgt in ${rp_tgt_varrays}
        do
          rp_tgt_volumes=("${rp_tgt_volumes[@]}" "${rpvolume}${volume_name_modifier}-target-${tgt}")
        done
        
        # Compile a list of target volume names that have been created for RP+VPlex
        rp_vplex_tgt_volumes=()
        for tgt in ${rpvplex_tgt_varrays}
        do
          rp_vplex_tgt_volumes=("${rp_vplex_tgt_volumes[@]}" "${rpvplexvolume}${volume_name_modifier}-target-${tgt}")
        done
        
        # Compile a list of target volume names that have been created for MetroPoint
        rpmp_tgt_volumes=()
        for tgt in ${rpmp_tgt_varrays}
        do
          rpmp_tgt_volumes=("${rpmp_tgt_volumes[@]}" "${rpmpvolume}${volume_name_modifier}-target-${tgt}")
        done

        # RP test cases
        # Run these test cases unless specified to only run create/delete tests
        if [ "$RP_RUN_VOLUME_CREATE_ONLY" = "0" ]; then
            if [ "$RP_TESTS" = "1" ]; then
                secho 'RP tests...'

                # Change Virtual Pool Test
                recoverpoint_vpool_change_test ${rpvolume}${volume_name_modifier} ${rp_noprotection_vpool} ${rp_src_vpool} ${RP_CONSISTENCY_GROUP}

                # Add Journal Test
                recoverpoint_add_journal_volume ${rp_tgt_varray} ${rp_tgt_varray} ${rp_tgt_vpool} ${RP_CONSISTENCY_GROUP}
                
                # Snapshot/Bookmark Test
                if [ "${RP_CLR}" = "1" ]; then
                    recoverpoint_auto_snapshot_cleanup_test ${rpvolume}${volume_name_modifier} ${rp_tgt_volumes[0]} ${rp_tgt_varrays[0]} ${rp_tgt_volumes[1]}
                else
                    recoverpoint_auto_snapshot_cleanup_test ${rpvolume}${volume_name_modifier} ${rp_tgt_volumes[0]} ${rp_tgt_varrays[0]}
                fi

                # Failover Test
                recoverpoint_failover_test ${rpvolume}${volume_name_modifier} ${rpvolumetarget}${volume_name_modifier}

                # CG Failover/Swap Test
                recoverpoint_cg_failover_test ${RP_CONSISTENCY_GROUP} ${rpvolume}${volume_name_modifier} ${rp_src_varray} ${rp_tgt_volumes} ${rp_tgt_varrays}

                # Change Replication Mode Test
                recoverpoint_vpool_change_replication_mode_test ${rpvolume}${volume_name_modifier} ${rp_src_vpool_sync}
                recoverpoint_vpool_change_replication_mode_test ${rpvolume}${volume_name_modifier} ${rp_src_vpool}

                # RP Export Bookmark Test
                recoverpoint_export_bookmark_tests ${rpvolume} $RECOVERPOINT_VARRAY1 ${rp_tgt_varrays} ${rp_src_vpool} ""
            fi

            # RP+VPLEX test cases
            if [ "$RPVPLEX_TESTS" = "1" ]; then
                secho 'RP+VPLEX tests...'

                # Change Virtual Pool Test
                if [ "$RP_CRR" = "1" ]; then
                    # Upgrade to MetroPoint change vpool test
                    recoverpoint_vpool_change_test ${rpvplexvolume}${volume_name_modifier} ${rpvplex_noprotection_vpool} ${rpvplex_src_vpool} ${RP_VPLEX_CONSISTENCY_GROUP} rpmp_crr
                else
                    recoverpoint_vpool_change_test ${rpvplexvolume}${volume_name_modifier} ${rpvplex_noprotection_vpool} ${rpvplex_src_vpool} ${RP_VPLEX_CONSISTENCY_GROUP}
                fi

                # Add Journal Test
                recoverpoint_add_journal_volume ${rpvplex_tgt_varray} ${rpvplex_tgt_varray} ${rpvplex_tgt_vpool} ${RP_VPLEX_CONSISTENCY_GROUP}

                # Snapshot/Bookmark Test
                recoverpoint_auto_snapshot_cleanup_test ${rpvplexvolume}${volume_name_modifier} ${rp_vplex_tgt_volumes[0]} ${rpvplex_tgt_varrays[0]}

                # Failover Test
                recoverpoint_failover_test ${rpvplexvolume}${volume_name_modifier} ${rpvplexvolumetarget}${volume_name_modifier}

                # CG Failover/Swap Test
                recoverpoint_cg_failover_test ${RP_VPLEX_CONSISTENCY_GROUP} ${rpvplexvolume}${volume_name_modifier} ${rpvplex_src_varray} ${rp_vplex_tgt_volumes} ${rp_tgt_varrays}

                # Change Replication Mode Test
                secho 'RP change vpool - replication mode, not supported on RP/VPLEX yet'
                # recoverpoint_vpool_change_replication_mode_test ${rpvplexvolume}${volume_name_modifier} ${rpvplex_src_vpool_sync}
                # recoverpoint_vpool_change_replication_mode_test ${rpvplexvolume}${volume_name_modifier} ${rpvplex_src_vpool}

                # RP Export Bookmark Test
                recoverpoint_export_bookmark_tests ${rpvplexvolume} $RECOVERPOINT_VARRAY1 ${rp_tgt_varrays} ${rpvplex_src_vpool} "vplex"
            fi

            # MP test cases
            if [ "$RPMP_TESTS" = "1" ]; then
                secho 'MetroPoint tests...'

                # Change Virtual Pool Test
                recoverpoint_vpool_change_test ${rpmpvolume}${volume_name_modifier} ${rpmp_noprotection_vpool} ${rpmp_src_vpool} ${RP_METROPOINT_CONSISTENCY_GROUP}

                # Add Journal Test
                recoverpoint_add_journal_volume ${rpmp_active_tgt_varray} ${rpmp_active_tgt_varray} ${rpmp_tgt_vpool} ${RP_METROPOINT_CONSISTENCY_GROUP}

                # Snapshot/Bookmark Test
                recoverpoint_auto_snapshot_cleanup_test ${rpmpvolume}${volume_name_modifier} ${rpmp_tgt_volumes[0]} ${rpmp_tgt_varrays[0]}

                # Failover Test
                recoverpoint_failover_test ${rpmpvolume}${volume_name_modifier} ${rpmpvolumetarget}${volume_name_modifier}

                # CG Failover/Swap Test
                recoverpoint_cg_failover_test ${RP_METROPOINT_CONSISTENCY_GROUP} ${rpmpvolume}${volume_name_modifier} ${rpmp_src_varray} ${rpmp_tgt_volumes} ${rpmp_tgt_varrays}

                # Change Replication Mode Test
                secho 'RP change vpool - replication mode, not supported on RP/MP yet'
                # recoverpoint_vpool_change_replication_mode_test ${rpmpvolume}${volume_name_modifier} ${rpmp_src_vpool_sync}
                # recoverpoint_vpool_change_replication_mode_test ${rpmpvolume}${volume_name_modifier} ${rpmp_src_vpool}

                # RP Export Bookmark Test
                recoverpoint_export_bookmark_tests ${rpmpvolume} $RECOVERPOINT_VARRAY1 ${rp_tgt_varrays} ${rpmp_src_vpool} "mp"
            fi

            # XIO test cases
            if [ "$RPXIO_TESTS" = "1" ]; then
                secho 'XIO tests...'

                # Change Virtual Pool Test
                recoverpoint_vpool_change_test ${rpxiovolume}${volume_name_modifier} ${rpxio_noprotection_vpool} ${rpxio_src_vpool} ${RP_XIO_CONSISTENCY_GROUP}

                # Add Journal Test
                recoverpoint_add_journal_volume ${rpxio_tgt_varray} ${rpxio_tgt_varray} ${rpxio_tgt_vpool} ${RP_XIO_CONSISTENCY_GROUP}
                
                # Snapshot/Bookmark Test
                if [ "${RP_CLR}" = "1" ]; then
                    recoverpoint_auto_snapshot_cleanup_test ${rpxiovolume}${volume_name_modifier} ${rpxiovolumetarget}${volume_name_modifier} ${rpxio_tgt_varray} ${rpxiovolume}-target-${RECOVERPOINT_VARRAY1}
                else
                    recoverpoint_auto_snapshot_cleanup_test ${rpxiovolume}${volume_name_modifier} ${rpxiovolumetarget}${volume_name_modifier} ${rpxio_tgt_varray}
                fi

                # Failover Test
                recoverpoint_failover_test ${rpxiovolume}${volume_name_modifier} ${rpxiovolumetarget}${volume_name_modifier}

                # CG Failover/Swap Test
                recoverpoint_cg_failover_test ${RP_XIO_CONSISTENCY_GROUP} ${rpxiovolume}${volume_name_modifier} ${rpxio_src_varray} ${rpxio_tgt_volumes} ${rpxio_tgt_varrays}

                # Change Replication Mode Test
                secho 'RP change vpool - replication mode, not supported on RP/XIO yet'
                #recoverpoint_vpool_change_replication_mode_test ${rpxiovolume}${volume_name_modifier} ${rpxio_src_vpool_sync}
                #recoverpoint_vpool_change_replication_mode_test ${rpxiovolume}${volume_name_modifier} ${rpxio_src_vpool}

                # RP Export Bookmark Test
                recoverpoint_export_bookmark_tests ${rpxiovolume} $RECOVERPOINT_VARRAY1 ${rpxio_tgt_varrays} ${rpxio_src_vpool} "xio"

            fi
        fi

        ### End RecoverPoint test cases ###

        secho 'RP cleanup'

        if [ "$RP_TESTS" = "1" ]; then
            secho 'RP volume cleanup...'
            recoverpoint_cleanup_volumes ${rpvolume} $RP_VOLUME_COUNT
        fi

        if [ "$RPVPLEX_TESTS" = "1" ]; then
            secho 'RP+VPLEX volume cleanup...'
            recoverpoint_cleanup_volumes ${rpvplexvolume} $RP_VOLUME_COUNT
        fi

        if [ "$RPMP_TESTS" = "1" ]; then
            secho 'MP volume cleanup...'
            recoverpoint_cleanup_volumes ${rpmpvolume} $RP_VOLUME_COUNT
        fi
        if [ "$RPXIO_TESTS" = "1" ]; then
            secho 'XIO volume cleanup...'
            recoverpoint_cleanup_volumes ${rpxiovolume} $RP_VOLUME_COUNT
        fi

        secho 'CG cleanup...'
        recoverpoint_cleanup_cgs

        secho 'RP tests done'
    fi
}

#
#
#
######################### End of RecoverPoint ############################

hds_cos_setup()
{
    run cos create block $COS_HDS               \
                      --description 'Virtual-Pool-for-HDS' true  \
                      --protocols FC \
                      --numpaths 1              \
                      --provisionType 'Thin'        \

    run cos allow $COS_HDS block $TENANT
}

hds_setup()
{
    hds_setup_once
}

hds_setup_once()
{

    # Discovery the storage systems 
    storageprovider show $HDS_PROVIDER &> /dev/null && return $?
    storageprovider create $HDS_PROVIDER $HDS_PROVIDER_IP $HDS_PROVIDER_PORT $HDS_PROVIDER_USER "$HDS_PROVIDER_PASSWD" $HDS_PROVIDER_INTERFACE_TYPE --usessl false
    storagedevice discover_all
    storagedevice list

    hds_cos_setup
}

#
# hds tests
#
hds_tests()
{
    echo "**** Done hds"
}


#
# add an isilon storage device with a default pool that supports NFS and N+2:1 protection
#
isilon_setup_once()
{
    # do this only once
    discoveredsystem show $ISI_NATIVEGUID &> /dev/null && return $?

    discoveredsystem create $ISI_DEV isilon $ISI_IP 8080 $ISI_USER $ISI_PASSWD --serialno=$ISI_SN
    
    isilon_cos_setup

    storagepool   update $ISI_NATIVEGUID --type file
    storageport   update $ISI_NATIVEGUID IP --tzone $NH/$IP_ZONE

    run cos update file $COS_ISIFILE --storage $ISI_NATIVEGUID
    #isilon_cos_setup
}

isilon_setup()
{
    isilon_setup_once
    run cos allow $COS_ISIFILE file $TENANT
}

driversystem_setup()
{
    echo "driversystem setup"
    discoveredsystem create driverManagedSystem $DRIVER_SYSTEM_TYPE  $DRIVER_SYSTEM_IP $DRIVER_SYSTEM_PORT $DRIVER_SYSTEM_USER $DRIVER_SYSTEM_PASSWORD
}


driversystem_tests()
{
   echo "driversystem tests"
   PROJECT=erProject
   vol=erVolTest006-1
#   run blocksnapshot create $PROJECT/${vol} erSnap001
}
#
# add ECS storage device and storage pool
#
ecs_setup_once()
{
    #discoveredsystem show $ECS_DEV &> /dev/null && return $?
    discoveredsystem create $ECS_DEV ecs $ECS_IP 4443 $ECS_USER $ECS_PASSWD

    storagepool update $ECS_NATIVEGUID --nhadd $NH
    storageport update $ECS_NATIVEGUID IP --addvarrays $NH

    ecs_cos_setup
    
    tenant update_namespace $TENANT  $ECS_NAMESPACE
    project create $PROJECT --tenant $TENANT 
}

ecs_setup()
{
    echo "Performing ecs_setup"
    ecs_setup_once
}

#
# add vnx file device with a storage pool that supports NFS and COS_VNX_PROTECTION protection
# add server_{2,3,4} data mover storage ports to this device, connected to the IP transport zone
#
netapp_setup_once()
{
    #do this only once
    storagedevice show $NETAPPF_NATIVEGUID &> /dev/null && return $?

    discoveredsystem create $NETAPPF_DEV netapp $NETAPPF_IP $NETAPPF_PORT $NETAPPF_USER $NETAPPF_PW --serialno=$NETAPPF_SN
    netapp_cos_setup

    storagepool update $NETAPPF_NATIVEGUID --type file
    
    storageport update $NETAPPF_NATIVEGUID IP --tzone nh/iptz
    run cos update file $COS_NETAPP --storage $NETAPPF_NATIVEGUID

   # netapp_cos_setup
}

netapp_setup()
{
    netapp_setup_once
    run cos allow $COS_NETAPP file $TENANT
}

netappc_setup_once()
{
    #do this only once
    storagedevice show $NETAPPCF_NATIVEGUID &> /dev/null && return $?

    discoveredsystem create $NETAPPCF_DEV netappc $NETAPPCF_IP $NETAPPCF_PORT $NETAPPCF_USER $NETAPPCF_PW --serialno=$NETAPPCF_SN
    netappc_cos_setup

    storagepool update $NETAPPCF_NATIVEGUID --type file

    storageport update $NETAPPCF_NATIVEGUID IP --tzone nh/iptz
    run cos update file $COS_NETAPPC --storage $NETAPPCF_NATIVEGUID

   # netapp_cos_setup
}

netappc_setup()
{
    netappc_setup_once
    run cos allow $COS_NETAPPC file $TENANT
}

#
# add datadomain file device with a storage pool that supports NFS and CIFS
# add server_{2,3,4} data mover storage ports to this device, connected to the IP transport zone
#
datadomainfile_setup_once()
{
    #do this only once

    # Discover the storage systems 
    storageprovider show $DATADOMAINF_DEV &> /dev/null && return $?
    echo "Starting storageprovider create"
    storageprovider create $DATADOMAINF_DEV $DATADOMAINF_DEV_IP $DATADOMAINF_PORT $DATADOMAINF_USER "$DATADOMAINF_PW" $DATADOMAINF_PROVIDER_INTERFACE 
    echo "Storageprovider create done"
    storagedevice discover_all
    storagedevice list
    storageport list $DATADOMAINF_NATIVEGUID
    storagedevice show $DATADOMAINF_NATIVEGUID &> /dev/null && return $?

    datadomainfile_cos_setup

    storagepool update $DATADOMAINF_NATIVEGUID --type file

    storageport update $DATADOMAINF_NATIVEGUID IP --tzone nh/iptz
    run cos update file $COS_DDFILE --storage $DATADOMAINF_NATIVEGUID

}

datadomainfile_setup()
{
    datadomainfile_setup_once
    run cos allow $COS_DDFILE file $TENANT
}

#
# add vnx file device with a storage pool that supports NFS and COS_VNX_PROTECTION protection
# add server_{2,3,4} data mover storage ports to this device, connected to the IP transport zone
#
vnxfile_setup_once()
{

    vnxfile_discovery

    storagepool update $VNXF_NATIVEGUID --type file

    storageport update $VNXF_NATIVEGUID IP --tzone nh/iptz
    run cos update file $COS_VNXFILE --storage $VNXF_NATIVEGUID

   # vnxfile_cos_setup
}

vnxfile_setup()
{
    vnxfile_setup_once
    run cos allow $COS_VNXFILE file $TENANT
}

vnxfile_discovery()
{
    #do this only once
    storagedevice show $VNXF_NATIVEGUID &> /dev/null && return $?

    discoveredsystem create $VNXF_DEV vnxfile $VNXF_IP $VNXF_PORT $VNXF_USER $VNXF_PW \
                         --smisip=$VNXF_SMIS_IP --smisport=$VNXF_SMIS_PORT --smisuser=nasadmin \
                         --smispw=nasadmin --smisssl=true --serialno=$VNXF_SN
    vnxfile_cos_setup
}

vnxfile_flex_varray_setup_once()
{
    vnxfile_discovery

    neighborhood create $NH3
    neighborhood show $NH3

    transportzone create2 $IP_ZONE3 IP --endpoints $VNXF_IP_ENDPOINT1,$VNXF_IP_ENDPOINT2,client1.emc.com,client2.emc.com

    # update the varray to add some ports
    storageport update $VNXF_NATIVEGUID IP --name 'spvnx1' --addvarrays $NH3
    storageport update $VNXF_NATIVEGUID IP --name 'spvnx2' --addvarrays $NH3
}

vnxfile_flex_varray_setup()
{
    vnxfile_flex_varray_setup_once
    run cos allow $COS_VNXFILE file $TENANT
}

vnxe_setup_once()
{

    vnxe_discovery

    vnxe_cos_setup

    storagepool update $VNXE_NATIVEGUID --type file

    storageport update $VNXE_NATIVEGUID IP --tzone nh/iptz
    run cos update file $COS_VNXE --storage $VNXE_NATIVEGUID
    
    storagepool update $VNXE_NATIVEGUID --type block --volume_type THIN_AND_THICK
    echo "vnxe block cos update"
    run cos update block $COS_VNXEBLOCK_CG --storage $VNXE_NATIVEGUID
   # run cos update block $COS_VNXEBLOCK_FC --storage $VNXE_NATIVEGUID
    run cos update block $COS_VNXEBLOCK_ISCSI --storage $VNXE_NATIVEGUID
   
}

vnxe_setup()
{
    vnxe_setup_once
    run cos allow $COS_VNXE file $TENANT
}

vnxe_discovery()
{
    #do this only once
    storagedevice show $VNXE_NATIVEGUID &> /dev/null && return $?

    discoveredsystem create $VNXE_DEV vnxe $VNXE_IP $VNXE_PORT $VNXE_USER $VNXE_PW \
                         --serialno=$VNXE_SN

}

ui_setup()
{
    echo "Nothing to do for ui setup"
}

namespace_test() {
    cos=$1; shift
    kpname=$(python -c 'import uuid; print uuid.uuid1()')
#    run namespace sanity --tenant $TENANT $NAMESPACE
}

retentionclass_test() {

    run retentionclass sanity $NAMESPACE "class_"
}

s3_baseurl_setup() {
    create_if_not_present=$1;
    if [ -n "$create_if_not_present"  ]; then
        is_present=$(run baseurl list | (grep 's3.amazonaws.com' || echo ''))
        if [ "$is_present" == '' ]; then
            echo "Default base URL entry not present, hence inserting it"
            run baseurl create "DefaultBaseUrl" "s3.amazonaws.com" false
        fi
    else
        run baseurl create "DefaultBaseUrl" "s3.amazonaws.com" false
    fi
}

s3_bucket_tests() {


    cos=$1; uid=$2; secret=$3; shift
    kpname=bucketsanitytest-$cos-$(python -c 'import uuid; print uuid.uuid1()')
    run bucket sanity $NAMESPACE $kpname --uid $uid --secret=$secret
}

s3_versioning_tests() {
    cos=$1; uid=$2; secret=$3; shift
    kpname=$(python -c 'import uuid; print uuid.uuid1()')
    run bucket create $NAMESPACE $kpname --uid $uid --secret=$secret
    run versioning sanity $NAMESPACE $kpname --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $kpname --uid $uid --secret=$secret
}

s3_multipart_upload_tests() {
    cos=$1; uid=$2; secret=$3; shift
    kpname=$(python -c 'import uuid; print uuid.uuid1()')
    keyname=key-$cos
    run bucket create $NAMESPACE $kpname --uid $uid --secret=$secret
    run s3_multipart_upload.py sanity $NAMESPACE $kpname $keyname --uid $uid --secret=$secret

    # copy part tests
    keysuffix=$(python -c 'import uuid; print uuid.uuid1()')
    srckpname=mpusrcbucket-$keysuffix
    # create source bucket
    run bucket create $NAMESPACE $srckpname --uid $uid --secret=$secret
    # create source object
    srckeyname=mpusrckey-$keysuffix
    run bucketkey create $NAMESPACE $srckpname $srckeyname copy-key-value-$cos --uid $uid --secret=$secret
    # copy source object as part to dest
    run s3_multipart_upload.py sanity_copy $NAMESPACE $kpname $keyname --srcbucket $srckpname --srckey $srckeyname --uid $uid --secret=$secret

    run bucketkey clean $NAMESPACE $srckpname --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $srckpname --uid $uid --secret=$secret

    run bucketkey clean $NAMESPACE $kpname --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $kpname --uid $uid --secret=$secret
}

swift_container_tests() {
    cos=$1; uid=$2; password=$3; shift
    kpname=contsanitytest-$cos-$(python -c 'import uuid; print uuid.uuid1()')
    run swift_container.py sanity $NAMESPACE $kpname --uid $uid --password=$password
}

swift_setup_uid_password() {
    uid=$1; password=$2; shift
    run password.py create $uid $password s3 --groups="admin"
}

swift_cleanup_uid_password() {
    uid=$1; shift
    run password.py remove $uid
}

atmos_subtenant_tests() {
    cos=$1; uid=$2; secret=$3; shift
    run atmossubtenant sanity $NAMESPACE $PROJECT $cos --uid $uid --secret=$secret
}

s3_key_tests(){
    cos=$1; uid=$2; secret=$3; testUid=$4; shift
    kpname=$(python  -c 'import uuid; print uuid.uuid1()')
    run bucket create $NAMESPACE $kpname --uid $uid --secret=$secret
    run bucketkey sanity $NAMESPACE $kpname key-$cos value-$cos  --uid $uid --secret=$secret --testUser=$testUid
    run bucket delete $NAMESPACE $kpname --uid $uid --secret=$secret

    run bucket create $NAMESPACE $kpname --uid $uid --secret=$secret --fileSystemEnabled=true
    run bucketkey filesystemsanity $NAMESPACE $kpname key-$cos value-$cos  --uid $uid --secret=$secret --testUser=$testUid
    run bucket delete $NAMESPACE $kpname --uid $uid --secret=$secret

}

s3_key_copy_tests(){
    cos=$1; uid=$2; secret=$3; shift
    sourcekpname=$(python  -c 'import uuid; print uuid.uuid1()')
    destkpname=$(python  -c 'import uuid; print uuid.uuid1()')
    run bucket create $NAMESPACE $sourcekpname --uid $uid --secret=$secret
    run bucket create $NAMESPACE $destkpname --uid $uid --secret=$secret

    keysuffix=$(python  -c 'import uuid; print uuid.uuid1()')

    # create source object
    sourcekeyname=copy-source-key-$keysuffix
    run bucketkey create $NAMESPACE $sourcekpname $sourcekeyname copy-key-value-$cos --uid $uid --secret=$secret
    # copy source object to dest
    run bucketkey copy $NAMESPACE $sourcekpname $sourcekeyname --destbucket $destkpname --destkey copy-dest-key-$keysuffix --uid $uid --secret=$secret

    # create source object with chars that need to be URL-encoded in copy-source header
    sourcekeyname2=copy,source:key/$keysuffix
    run bucketkey create $NAMESPACE $sourcekpname $sourcekeyname2 copy-key-value-$cos --uid $uid --secret=$secret
    # copy source object to dest
    run bucketkey copy $NAMESPACE $sourcekpname $sourcekeyname2 --destbucket $destkpname --destkey copy-dest-key2-$keysuffix --uid $uid --secret=$secret
    run bucketkey clean $NAMESPACE $sourcekpname --uid $uid --secret=$secret
    run bucketkey clean $NAMESPACE $destkpname --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $sourcekpname --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $destkpname --uid $uid --secret=$secret
}

s3_key_version_tests(){
    cos=$1; uid=$2; secret=$3; shift
    kpname=$(python  -c 'import uuid; print uuid.uuid1()')
    run bucket create $NAMESPACE $kpname --uid $uid --secret=$secret
    run versioning put $NAMESPACE $kpname Enabled --uid $uid --secret=$secret
    run bucketkey sanity $NAMESPACE $kpname key-$cos value-$cos  --uid $uid --secret=$secret --testUser=$testUid
    run versioning put $NAMESPACE $kpname Suspended --uid $uid --secret=$secret
    run bucketkey sanity $NAMESPACE $kpname key-$cos value-$cos  --uid $uid --secret=$secret --testUser=$testUid
    run bucketkey clean_ver $NAMESPACE $kpname --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $kpname --uid $uid --secret=$secret
}

s3_fileaccess_tests() {
    cos=$1; uid=$2; secret=$3; shift

    kpname1="$(python  -c 'import uuid; print uuid.uuid1()')-S3"
    run bucket create $NAMESPACE $kpname1 --uid $uid --secret=$secret
    run fileaccesssanity.py s3 $NAMESPACE $kpname1 $cos $uid $secret

    kpname2="$(python  -c 'import uuid; print uuid.uuid1()')-S3"
    run bucket create $NAMESPACE $kpname2 --uid $uid --secret=$secret
    run fileaccesssanity.py s3 $NAMESPACE $kpname2 $cos $uid $secret True

    fskpname1="$(python  -c 'import uuid; print uuid.uuid1()')-S3"
    run bucket create $NAMESPACE $fskpname1 --uid $uid --secret=$secret --fileSystemEnabled=True
    run fileaccesssanity.py s3 $NAMESPACE $fskpname1 $cos $uid $secret

    fskpname2="$(python  -c 'import uuid; print uuid.uuid1()')-S3"
    run bucket create $NAMESPACE $fskpname2 --uid $uid --secret=$secret --fileSystemEnabled=True
    run fileaccesssanity.py s3 $NAMESPACE $fskpname2 $cos $uid $secret True

    run bucketkey clean $NAMESPACE $kpname1 --uid $uid --secret=$secret
    run bucketkey clean $NAMESPACE $kpname2 --uid $uid --secret=$secret
    run bucketkey clean $NAMESPACE $fskpname1 --uid $uid --secret=$secret
    run bucketkey clean $NAMESPACE $fskpname2 --uid $uid --secret=$secret

    run bucket delete $NAMESPACE $kpname1 --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $kpname2 --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $fskpname1 --uid $uid --secret=$secret
    run bucket delete $NAMESPACE $fskpname2 --uid $uid --secret=$secret
}

swift_object_tests(){
    cos=$1; uid=$2; password=$3; shift
    kpname=$(python  -c 'import uuid; print uuid.uuid1()')
    run swift_container.py create $NAMESPACE $kpname --uid $uid --password=$password
    if [ "$BOURNE_IP" != "127.0.0.1" ]; then
        run swift_object.py sanity $NAMESPACE $kpname key-$cos value-$cos --uid $uid --password=$password
    fi
    run swift_container.py delete $NAMESPACE $kpname --uid $uid --password=$password
}

swift_fileaccess_tests() {
    cos=$1; uid=$2; secret=$3; shift
    kpname="$(python  -c 'import uuid; print uuid.uuid1()')-SWIFT"
    run swift_container.py create $NAMESPACE $kpname --uid $uid --password=$password
    run fileaccesssanity.py swift $NAMESPACE $kpname $cos $uid $password
    run swift_container.py delete $NAMESPACE $kpname --uid $uid --password=$password
}

atmos_object_tests(){
    cos=$1; uid=$2; secret=$3; testUid=$4; testUidSecret=$5; shift
    subtenantstr=`run atmossubtenant create $NAMESPACE $PROJECT $cos --uid $uid --secret=$secret`
    subtenant=${subtenantstr##*subtenant=}    
    run atmoskey sanity $NAMESPACE $PROJECT $subtenant key-$cos value-$cos --uid $uid --secret=$secret --testUid=$testUid --testUidSecret=$testUidSecret
    run atmossubtenant delete $NAMESPACE $subtenant --uid $uid --secret=$secret
}

object_create_secretkey() {
    echo "remove all secret keys before creating one"
    secretkey deleteuser $WS_UID
    secretkey deleteuser $WS_TEST_UID
    object_set_user_scope

    objectuser add $WS_UID $NAMESPACE
    objectuser add $WS_TEST_UID $NAMESPACE

    #object_set_user_scope_test
    WS_SECRET=`secretkeyuser add $WS_UID |tail -1`
    WS_SECRET_TEST_UID=`secretkeyuser add $WS_TEST_UID |tail -1`   
    
    #wait in case datasvc has already cache secret key from previous sanity running
    #echo "wait 3 minutes so that the secret key can be refreshed: secret key $WS_SECRET"
    #sleep 180
}

object_set_user_scope(){

    # disable set user scope  temporarily
    # to be enabled in the next patch
    SCRIPTDIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
    #echo "setting user scope as global"
    #result_global=`python $SCRIPTDIR/userscope set GLOBAL`
    #echo "setting user scope as global done"

    echo "get user scope"
    result=`python $SCRIPTDIR/userscope get`

    echo "get user scope result: $result"

    #if [ "$result_global" == "$result" ]
    #then
    #    echo "User scope is set to GLOBAL"
    #else
    #    echo "Unable to set user space as GLOBAL. Response is:"
    #    echo "$result_global"
    #fi
 
}

object_set_user_scope_test(){
    
    #validate the user scope again by trying to set it as NAMESPACE
    #and it should fail
    echo "setting user scope as namespace"
    result_namespace=`python $SCRIPTDIR/userscope set NAMESPACE`

    echo "get user scope"
    result=`python $SCRIPTDIR/userscope get`

    echo "get user scope result : $result"
   
    #compare the results and it should not be same
    if [ "$result_namespace" != "$result" ]
    then 
        echo "Set user scope as namespace failed as expected"
    else
        echo "Unexpected user scope shouldnt be allowed to change!!!"
    fi

}

object_delete_secretkey() {

    #login with webstorage user so that secret key can be created
    # security login $WS_UID $WS_PASSWORD

    #remove all secret keys
    secretkeyuser delete $WS_UID --secretKey=$WS_SECRET
    objectuser delete $WS_UID
    objectuser delete $WS_TEST_UID

    #relogin with sanity testing user
    security login $SYSADMIN $SYSADMIN_PASSWORD
    if [ "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
}



s3_baseurl_tests(){
    run baseurl sanity $NAMESPACE $WS_BUCKET1 --uid $WS_UID --secret=$WS_SECRET
}

SSH(){
    ip=$1
    cmd=$2
    opt=$3
    sshpass -p ${SYSADMIN_PASSWORD} ssh $opt -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@${ip} ${cmd}
}

SCP(){
    local ip=$1
    shift
    local args=("${@}")
    local len=${#args[@]}
    local src=${args[@]:0:${len}-1}
    local dst=${args[@]:${len}-1}
    sshpass -p ${SYSADMIN_PASSWORD} scp -q -r -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null $src root@$ip:$dst
}

ui_tests()
{
    run platform-ui kickstart
}

#
# add vnx device and use its storage
#
vnxblock_setup_once()
{
    # do this only once
    smisprovider show $VNX_SMIS_DEV &> /dev/null && return $?

    if [ $QUICK -eq 0 ]; then
       run smisprovider create $VNX_SMIS_DEV $VNX_SMIS_IP $VNX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VNX_SMIS_SSL
    else
       run smisprovider create $VNX_SMIS_DEV $SIMULATOR_SMIS_IP 5988 $SMIS_USER "$SMIS_PASSWD" false
    fi

    run storagedevice discover_all --ignore_error

    sleep 15
    vnxblock_cos_setup

    secho "VNX Block storagepools update"
    run storagepool update $VNXB_NATIVEGUID --type block --volume_type THIN_AND_THICK
    run storagepool update $VNXB_NATIVEGUID --type block --volume_type THICK_ONLY

    if [ $QUICK -eq 0 ]; then
       secho "VNX Block storageports update"
       if [ $DISCOVER_SAN -eq 0 ]; then
           run storageport update $VNXB_NATIVEGUID FC --tzone $NH/$FC_ZONE_A --group SP_A
           run storageport update $VNXB_NATIVEGUID FC --tzone $NH/$FC_ZONE_B --group SP_B
       fi
       run storageport update $VNXB_NATIVEGUID IP --tzone nh/iptz            
    fi

    run cos update block $COS_VNXBLOCK --storage $VNXB_NATIVEGUID
    run cos update block $COS_VNXBLOCK_FC --storage $VNXB_NATIVEGUID
    run cos update block $COS_VNXBLOCK_ISCSI --storage $VNXB_NATIVEGUID
    run cos update block $COS_VNXBLOCK_THIN --storage $VNXB_NATIVEGUID
    run cos update block $COS_VNXBLOCK_THICK --storage $VNXB_NATIVEGUID    
}

vnxblock_setup()
{
    secho "VNX Block Setup"
    vnxblock_setup_once
    run cos allow $COS_VNXBLOCK block $TENANT
    run cos allow $COS_VNXBLOCK_FC block $TENANT
    run cos allow $COS_VNXBLOCK_ISCSI block $TENANT
    run cos allow $COS_VNXBLOCK_THIN block $TENANT
    run cos allow $COS_VNXBLOCK_THICK block $TENANT
}


vmaxblock_setup_once()
{
    # do this only once
    smisprovider show $VMAX_SMIS_DEV &> /dev/null && return $?

    # If this is using the simulator, the vnxblock already took care of the SMIS provider
    if [ $QUICK -eq 0 ]; then
       run smisprovider create $VMAX_SMIS_DEV $VMAX_SMIS_IP $VMAX_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX_SMIS_SSL
    fi

    run storagedevice discover_all --ignore_error

    vmax_cos_setup
    mirrorblock_cos_setup

    run storagepool update $VMAX_NATIVEGUID --type block --volume_type THIN_ONLY
    run storagepool update $VMAX_NATIVEGUID --type block --volume_type THICK_ONLY
    run storagepool update $VMAX_NATIVEGUID --nhadd $NH --type block
    
    if [ $QUICK -eq 0 ]; then
        secho "VMAX Block storageports update"
        if [ $DISCOVER_SAN -eq 0 ]; then
      
           for porta in ${VMAX_PORTS_A}
           do
	      run storageport update $VMAX_NATIVEGUID FC --tzone $FCTZ_A --group ${porta}
           done

	   for portb in ${VMAX_PORTS_B}
           do
              run storageport update $VMAX_NATIVEGUID FC --tzone $FCTZ_B --group ${portb}
           done
       fi
       run storageport update $VMAX_NATIVEGUID IP --tzone nh/iptz
    fi

    run cos update block $COS_VMAXBLOCK --storage $VMAX_NATIVEGUID
    run cos update block $COS_VMAXBLOCK_FC --storage $VMAX_NATIVEGUID
    run cos update block $COS_VMAXBLOCK_ISCSI --storage $VMAX_NATIVEGUID
    run cos update block $COS_VMAXBLOCK_THIN --storage $VMAX_NATIVEGUID
}

vmaxblock_setup()
{
    secho "VMAX Block setup"

    vmaxblock_setup_once
    run cos allow $COS_VMAXBLOCK block $TENANT
    run cos allow $COS_VMAXBLOCK_FC block $TENANT
    run cos allow $COS_VMAXBLOCK_ISCSI block $TENANT
    run cos allow $COS_VMAXBLOCK_THIN block $TENANT
}

vnxblock_flex_varray_setup()
{
    vnxblock_setup

    # do manual assignment of ports
    run transportzone update $FC_ZONE_A --remNeighborhoods $NH
    run transportzone update $FC_ZONE_B --remNeighborhoods $NH
    run transportzone update $IP_ZONE --remNeighborhoods $NH

    run storageport update $VNXB_NATIVEGUID FC --addvarrays $NH --group SP_A
    run storageport update $VNXB_NATIVEGUID FC --addvarrays $NH --group SP_B
    run storageport update $VNXB_NATIVEGUID IP --addvarrays $NH
}


vmaxblock_flex_varray_setup()
{
    vmaxblock_setup

    # remove network assignment and assign ports manually
    run transportzone update $FC_ZONE_A --remNeighborhoods $NH
    run transportzone update $FC_ZONE_B --remNeighborhoods $NH
    run transportzone update $IP_ZONE --remNeighborhoods $NH
    for porta in ${VMAX_PORTS_A}
    do
        run storageport update $VMAX_NATIVEGUID FC --addvarrays $NH --group ${porta}
    done
    for portb in ${VMAX_PORTS_B}
    do
        run storageport update $VMAX_NATIVEGUID FC --addvarrays $NH --group ${portb}
    done
    run storageport update $VMAX_NATIVEGUID IP --addvarrays $NH
}

#
# VMAX and VNX export group tests
#
combined_block_setup()
{
    vnxblock_setup
    vmaxblock_setup
}

# Block Mirror setup
# 
mirrorblock_setup()
{
    vmaxblock_setup
    #vnxblock_setup

    run cos allow $COS_MIRROR block $TENANT
    run cos update block $COS_MIRROR --storage $VMAX_NATIVEGUID

    run cos allow $COS_MIRROR_WITH_OPTIONAL block $TENANT
    run cos update block $COS_MIRROR_WITH_OPTIONAL --storage $VMAX_NATIVEGUID

    run cos allow $COS_MIRROR_WITH_2_MIRRORS block $TENANT
    run cos update block $COS_MIRROR_WITH_2_MIRRORS --storage $VMAX_NATIVEGUID

    run cos allow $COS_MIRROR_BEFORE_CHANGE block $TENANT
    run cos update block $COS_MIRROR_BEFORE_CHANGE --storage $VMAX_NATIVEGUID

    run cos allow $COS_MIRROR_AFTER_CHANGE block $TENANT
    run cos update block $COS_MIRROR_AFTER_CHANGE --storage $VMAX_NATIVEGUID

    #run cos allow $COS_MIRROR_VNX block $TENANT
    #run cos update block $COS_MIRROR_VNX --storage $VNXB_NATIVEGUID    

    run cos allow $COS_VMAX_CG_MIRROR block $TENANT
    run cos update block $COS_VMAX_CG_MIRROR --storage $VMAX_NATIVEGUID
}

#
# syssvc tests
#
syssvc_setup()
{
    echo "syssvc setup, do nothing."
}

security_setup()
{
    echo "Enable root ssh permit"
    syssvc $CONFIG_FILE $BOURNE_IPADDR set_prop system_permit_root_ssh yes
}

all_setup()
{
#    ui_setup
#    webstorage_setup
    isilon_setup
#    vplex_setup
#    vnxfile_setup
    netapp_setup
#    datadomainfile_setup
    vnxblock_setup
    vmaxblock_setup
    mirrorblock_setup
    syssvc_setup
}

#
# Login, Configure SMTP, and add controller and object licenses
#
login_nd_configure_smtp_nd_add_licenses()
{
    security login $SYSADMIN $SYSADMIN_PASSWORD
    root_tenant=`tenant root|tail -1`
    echo "Verifying bulk POST request before license"
    tenant bulk_post "$root_tenant"
    echo "Finished verifying bulk POST request before license"
    echo "Configuring smtp and adding object and controller licenses."
    syssvc $CONFIG_FILE "$BOURNE_IP" setup
    echo "Finished Configuring smtp and adding licenses."
}

#
# setup cos, zone, project and add storage devices to bourne for tests
#
common_setup()
{
    sec_start_ldap_server
    login_nd_configure_smtp_nd_add_licenses

    run syssvc $CONFIG_FILE "$BOURNE_IP" set_prop system_proxyuser_encpassword ${SYSADMIN_PASSWORD}
    tenant_setup

    if [ "${RP_INGESTTESTS}" = "0" -o "${RP_INGESTTESTS}" = "" ]; then

        # If we're running physical, DISCOVER_SAN needs to be on for physical configurations.
	if [ $QUICK -eq 0 ]; then
	    secho "Turning on DISCOVER_SAN because we are testing physical"
	    DISCOVER_SAN=1
	fi

	zone_setup
	run transportzone add $NH/$IP_ZONE $BLK_CLIENT_iSCSI
    fi

    project_setup
    projectid=$(project query $PROJECT)
    secho "Project id of $PROJECT is $projectid."

    ROOT_TENANT=`tenant root|tail -1`
    if [ "${RP_INGESTTESTS}" = "0" ]; then
	secho "Setup ACLs on neighborhood for $TENANT"
	run neighborhood allow $NH $TENANT
	run neighborhood allow $NH2 $TENANT
	run neighborhood allow $NH $ROOT_TENANT
	run neighborhood allow $NH2 $ROOT_TENANT
	secho "Setup hosts and clusters for $TENANT"
	host_setup
    fi
}

blocksnapshot_setup()
{
    vnxblock_setup
    vmaxblock_setup
}

blocksnapshot_single_vnx()
{
    snaptest_vol="${VNX_VOLUME}-snaptest"
    run volume create ${snaptest_vol} $PROJECT $NH $COS_VNXBLOCK 1280000000

    # VMAX snap tests
    vnx_snap1_label=vnx_snap1-${HOSTNAME}-${RANDOM}
    vnx_snap2_label=vnx_snap2-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${snaptest_vol} $vnx_snap1_label
    if [ "$EXTRA_PARAM" = "search" ] ; then
        blocksnapshot search $(echo $snaptest_vol| head -c 2)
        blocksnapshot search $(echo $snaptest_vol | head -c 2) --project $projectid

        blocksnapshot tag $PROJECT/${snaptest_vol}/${vnx_snap1_label} $TAG
        blocksnapshot search $SEARCH_PREFIX --scope $TENANT --tag true
    fi

    run blocksnapshot create $PROJECT/${snaptest_vol} $vnx_snap2_label
    run blocksnapshot list $PROJECT/${snaptest_vol}
    run blocksnapshot show $PROJECT/${snaptest_vol}/${vnx_snap1_label}
    run blocksnapshot show $PROJECT/${snaptest_vol}/${vnx_snap2_label}
    run blocksnapshot restore $PROJECT/${snaptest_vol}/${vnx_snap2_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}/${vnx_snap1_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}/${vnx_snap2_label}
    run volume delete $PROJECT/${snaptest_vol} --wait

    # ----------------------------------
    # Run tests with activate operations
    # ----------------------------------
    activate_vol="${snaptest_vol}-ac"
    run volume create ${activate_vol} $PROJECT $NH $COS_VNXBLOCK 1280000000

    # Snapshot without create_inactive operation specified ==> Use default
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    run blocksnapshot create ${PROJECT}/${activate_vol} $snap1_label
    run blocksnapshot show ${PROJECT}/${activate_vol}/${snap1_label}
    run blocksnapshot activate ${PROJECT}/${activate_vol}/${snap1_label}
    run blocksnapshot delete ${PROJECT}/${activate_vol}/${snap1_label}

    # Snapshot with create_inactive=true
    inactive_snap_label=inactive-snap-${HOSTNAME}-${RANDOM}
    run blocksnapshot create ${PROJECT}/${activate_vol} $inactive_snap_label --create_inactive=true
    run blocksnapshot show ${PROJECT}/${activate_vol}/${inactive_snap_label}
    run blocksnapshot activate ${PROJECT}/${activate_vol}/${inactive_snap_label}
    run blocksnapshot delete ${PROJECT}/${activate_vol}/${inactive_snap_label}

    # Snapshot with create_inactive=false
    active_snap_label=active-snap-${HOSTNAME}-${RANDOM}
    run blocksnapshot create ${PROJECT}/${activate_vol} $active_snap_label --create_inactive=false
    run blocksnapshot show ${PROJECT}/${activate_vol}/${active_snap_label}
    run blocksnapshot activate ${PROJECT}/${activate_vol}/${active_snap_label}
    run blocksnapshot restore ${PROJECT}/${activate_vol}/${active_snap_label}
    run blocksnapshot delete ${PROJECT}/${activate_vol}/${active_snap_label}

    # Cleanup
    run volume delete ${PROJECT}/${activate_vol} --wait
}

blocksnapshot_single_vmax()
{
    snaptest_vol="${VMAX_VOLUME}-snaptest"

    run volume create ${snaptest_vol} $PROJECT $NH $COS_VMAXBLOCK 1280000000 --thinVolume true

    # VMAX snap tests
    vmx_snap1_label=vmx_snap1-${HOSTNAME}-${RANDOM}
    vmx_snap2_label=vmx_snap2-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${snaptest_vol} $vmx_snap1_label
    run blocksnapshot create $PROJECT/${snaptest_vol} $vmx_snap2_label
    run blocksnapshot list $PROJECT/${snaptest_vol}
    run blocksnapshot show $PROJECT/${snaptest_vol}/${vmx_snap1_label}
    run blocksnapshot show $PROJECT/${snaptest_vol}/${vmx_snap2_label}
    run blocksnapshot restore $PROJECT/${snaptest_vol}/${vmx_snap2_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}/${vmx_snap1_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}/${vmx_snap2_label}
    run volume delete $PROJECT/${snaptest_vol} --wait

    # ----------------------------------
    # Run tests with activate operations
    # ----------------------------------
    activate_vol="${snaptest_vol}-ac"
    run volume create ${activate_vol} $PROJECT $NH $COS_VMAXBLOCK 1280000000 --thinVolume true

    # Snapshot without create_inactive operation specified ==> Use default
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    run blocksnapshot create ${PROJECT}/${activate_vol} $snap1_label
    run blocksnapshot show ${PROJECT}/${activate_vol}/${snap1_label}
    run blocksnapshot activate ${PROJECT}/${activate_vol}/${snap1_label}
    run blocksnapshot delete ${PROJECT}/${activate_vol}/${snap1_label}

    # Snapshot with create_inactive=true
	# ---------------------------------------------------------------------------------------------
	# Creating inactive snapshot is not applicable for V3 array. hence commenting out this section.
	# ---------------------------------------------------------------------------------------------
    # inactive_snap_label=inactive-snap-${HOSTNAME}-${RANDOM}
    # run blocksnapshot create ${PROJECT}/${activate_vol} $inactive_snap_label --create_inactive=true
    # run blocksnapshot show ${PROJECT}/${activate_vol}/${inactive_snap_label}
    # run blocksnapshot activate ${PROJECT}/${activate_vol}/${inactive_snap_label}
    # run blocksnapshot delete ${PROJECT}/${activate_vol}/${inactive_snap_label}
	# ---------------------------------------------------------------------------------------------

    # Snapshot with create_inactive=false
    active_snap_label=active-snap-${HOSTNAME}-${RANDOM}
    run blocksnapshot create ${PROJECT}/${activate_vol} $active_snap_label --create_inactive=false
    run blocksnapshot show ${PROJECT}/${activate_vol}/${active_snap_label}
    run blocksnapshot activate ${PROJECT}/${activate_vol}/${active_snap_label}
    run blocksnapshot restore ${PROJECT}/${activate_vol}/${active_snap_label}
    run blocksnapshot delete ${PROJECT}/${activate_vol}/${active_snap_label}

    # Cleanup
    run volume delete ${PROJECT}/${activate_vol} --wait
}

blocksnapshot_consistency_group_vnx()
{
    snaptest_vol="${VNX_VOLUME}-cg"
    consistency_group=`openssl passwd "$RANDOM" | cut -c1-8`

    # Create consistency group
    run blockconsistencygroup create $PROJECT $consistency_group

    # Create volumes
    run volume create ${snaptest_vol}1 $PROJECT $NH $COS_VNXBLOCK 1280000000 --consistencyGroup consistency_group
    run volume create ${snaptest_vol}2 $PROJECT $NH $COS_VNXBLOCK 1280000000 --consistencyGroup consistency_group
    run volume create ${snaptest_vol}3 $PROJECT $NH $COS_VNXBLOCK 1280000000 --consistencyGroup consistency_group
    # Create snaps
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    snap2_label=snap2-${HOSTNAME}-${RANDOM}
    snap3_label=snap3-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${snaptest_vol}1 $snap1_label
    run blocksnapshot create $PROJECT/${snaptest_vol}1 $snap2_label
    run blocksnapshot create $PROJECT/${snaptest_vol}1 $snap3_label
    # Show it
    run blocksnapshot list $PROJECT/${snaptest_vol}1
    run blocksnapshot list $PROJECT/${snaptest_vol}2
    run blocksnapshot list $PROJECT/${snaptest_vol}3
    # Restore
    run blocksnapshot restore $PROJECT/${snaptest_vol}2/${snap2_label}
    # Clean up - delete one snap, will delete all in snapset
    run blocksnapshot delete $PROJECT/${snaptest_vol}1/${snap1_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}1/${snap2_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}1/${snap3_label}
    # Delete
    run volume delete $PROJECT/${snaptest_vol}1 --wait
    run volume delete $PROJECT/${snaptest_vol}2 --wait
    run volume delete $PROJECT/${snaptest_vol}3 --wait
    run blockconsistencygroup delete $consistency_group

    # --------------------------------------------
    # Consistency Group snap tests with activation
    # --------------------------------------------
    consistency_group=`openssl passwd "$RANDOM" | cut -c1-8`

    # Create consistency group
    run blockconsistencygroup create $PROJECT $consistency_group

    volumename="${VNX_VOLUME}-CG-VOL-TO-SNAP"
    cg_vol1=${volumename}1
    cg_vol2=${volumename}2
    run volume create ${cg_vol1} $PROJECT $NH $COS_VNXBLOCK 1280000000 --consistencyGroup consistency_group
    run volume create ${cg_vol2} $PROJECT $NH $COS_VNXBLOCK 1280000000 --consistencyGroup consistency_group

    # Create CG snap with create_inactive=true
    inactive_snap_label=inactive-snap--${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${cg_vol1} $inactive_snap_label --create_inactive=true
    # This requires an OPT to be fixed for it work
    # run blocksnapshot activate $PROJECT/${cg_vol1}/${inactive_snap_label}
    # run blocksnapshot restore $PROJECT/${cg_vol1}/${inactive_snap_label}
    run blocksnapshot show $PROJECT/${cg_vol1}/${inactive_snap_label}

    # Create CG snap with create_inactive=false
    active_snap_label=active-snap-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${cg_vol1} $active_snap_label --create_inactive=false
    run blocksnapshot show $PROJECT/${cg_vol1}/${active_snap_label}
    run blocksnapshot activate $PROJECT/${cg_vol1}/${active_snap_label}
    run blocksnapshot restore $PROJECT/${cg_vol1}/${active_snap_label}

    # Create CG snap without specifying create_inactive value
    snap_label=snap-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${cg_vol1} $snap_label
    run blocksnapshot show $PROJECT/${cg_vol1}/${snap_label}
    run blocksnapshot activate $PROJECT/${cg_vol1}/${snap_label}
    run blocksnapshot restore $PROJECT/${cg_vol1}/${snap_label}
    run blocksnapshot delete $PROJECT/${cg_vol1}/${snap_label}

    # Clean up
    run blocksnapshot delete $PROJECT/${cg_vol1}/${inactive_snap_label}
    run blocksnapshot delete $PROJECT/${cg_vol1}/${active_snap_label}
    run volume delete $PROJECT/${cg_vol1} --wait
    run volume delete $PROJECT/${cg_vol2} --wait
    run blockconsistencygroup delete $consistency_group
}

blocksnapshot_consistency_group_vmax()
{
    snaptest_vol="${VMAX_VOLUME}-cg"
    consistency_group=`openssl passwd "$RANDOM" | cut -c1-8`
	
    # Create volumes
    run volume create ${snaptest_vol}1 $PROJECT $NH $COS_VMAXBLOCK_FC 1280000000 --consistencyGroup consistency_group
    run volume create ${snaptest_vol}2 $PROJECT $NH $COS_VMAXBLOCK_FC 1280000000 --consistencyGroup consistency_group
    # Create snaps
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    snap2_label=snap2-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${snaptest_vol}1 $snap1_label 
    run blocksnapshot create $PROJECT/${snaptest_vol}1 $snap2_label
    # Show it
    run blocksnapshot list $PROJECT/${snaptest_vol}1
    run blocksnapshot list $PROJECT/${snaptest_vol}2
    # Restore
    # run blocksnapshot restore $PROJECT/${snaptest_vol}2/${snap1_label}
    # Clean up
    run blocksnapshot delete $PROJECT/${snaptest_vol}1/${snap1_label}
    run blocksnapshot delete $PROJECT/${snaptest_vol}1/${snap2_label}
    # Delete
    run volume delete $PROJECT/${snaptest_vol}1 --wait
    run volume delete $PROJECT/${snaptest_vol}2 --wait

    # --------------------------------------------
    # Consistency Group snap tests with activation
    # --------------------------------------------
    volumename="${VMAX_VOLUME}-CG-VOL-TO-SNAP"
    cg_vol1=${volumename}1
    cg_vol2=${volumename}2
    consistency_group=`openssl passwd "$RANDOM" | cut -c1-8`
    
    run volume create ${cg_vol1} $PROJECT $NH $COS_VMAXBLOCK 1280000000 --consistencyGroup consistency_group
    run volume create ${cg_vol2} $PROJECT $NH $COS_VMAXBLOCK 1280000000 --consistencyGroup consistency_group

    # Create CG snap with create_inactive=true
    nactive_snap_label=inactive-snap--${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${cg_vol1} $inactive_snap_label --create_inactive=true
    # This requires an OPT to be fixed for it work
    # run blocksnapshot activate $PROJECT/${cg_vol1}/${inactive_snap_label}
    # run blocksnapshot restore $PROJECT/${cg_vol1}/${inactive_snap_label}
    run blocksnapshot show $PROJECT/${cg_vol1}/${inactive_snap_label}

    # Create CG snap with create_inactive=false
    active_snap_label=active-snap-${HOSTNAME}-${RANDOM}
    run blocksnapshot create $PROJECT/${cg_vol1} $active_snap_label --create_inactive=false
    run blocksnapshot show $PROJECT/${cg_vol1}/${active_snap_label}
    run blocksnapshot activate $PROJECT/${cg_vol1}/${active_snap_label}
    run blocksnapshot restore $PROJECT/${cg_vol1}/${active_snap_label}

    # Create CG snap without specifying create_inactive value
    #snap_label=snap-${HOSTNAME}-${RANDOM}
    #run blocksnapshot create $PROJECT/${cg_vol1} $snap_label
    #run blocksnapshot show $PROJECT/${cg_vol1}/${snap_label}
    #run blocksnapshot activate $PROJECT/${cg_vol1}/${snap_label}
    #run blocksnapshot restore $PROJECT/${cg_vol1}/${snap_label}
    #run blocksnapshot delete $PROJECT/${cg_vol1}/${snap_label}

    # Clean up
    run blocksnapshot delete $PROJECT/${cg_vol1}/${inactive_snap_label}
    run blocksnapshot delete $PROJECT/${cg_vol1}/${active_snap_label}
    run volume delete $PROJECT/${cg_vol1} --wait
    run volume delete $PROJECT/${cg_vol2} --wait
}

blocksnapshot_tests()
{
    blocksnapshot_single_vnx
    blocksnapshot_single_vmax

#    blocksnapshot_consistency_group_vnx
#    blocksnapshot_consistency_group_vmax
}


#
# fileshare tests
#
file_tests()
{
    echo "File tests started"
    cos=$1; shift
    perms=${@}
    datetime=`date +%m%d%y%H%M%S`
    fsname=fs-$cos-$macaddr-$datetime
    fsname=$(echo $fsname | sed s/-/_/g)
    fsname=$(echo $fsname | sed s/:/_/g)
    echo $fsname
    
	if [ $cos = $COS_VNXFILE ] ; then
	    run vnas list
	fi
    
    run fileshare create $fsname $PROJECT $NH $cos $FS_SIZEMB
    if [ "$EXTRA_PARAM" = "search" ] ; then
        run fileshare search --name $(echo $fsname | head -c 2)
        run fileshare search --name $(echo $fsname | head -c 2) --project $projectid

        run fileshare tag $PROJECT/${fsname} $TAG
        run fileshare search --tag $SEARCH_PREFIX 
    fi

    run fileshare show $PROJECT/$fsname
    run fileshare expand $PROJECT/$fsname $FS_EXPAND_SIZE
    run fileshare show $PROJECT/$fsname
    if [ $cos = $COS_ISIFILE ] ; then
        run snapshot create $PROJECT/$fsname $fsname-$datetime
        if [ "$EXTRA_PARAM" = "search" ] ; then
            snapshot search $(echo $fsname| head -c 2)
            snapshot search $(echo $fsname| head -c 2) --project $projectid

            snapshot tag $PROJECT/$fsname-$datetime $PROJECT/${fsname} $TAG
            snapshot search $SEARCH_PREFIX --scope $TENANT --tag true
        fi
        run snapshot create $PROJECT/$fsname $fsname-$datetime-2
        run bulkapi filesnapshots $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    for p in $perms
    do
        case $p in
        ro)
            exp_args="$FSEXP_RO_EPS --perm ro --comments TESTCOMMENTS"
            exp_upd_args="--readonlyhosts $FSEXP1"
            snap_exp_args="$FSEXP_RO_EPS --perm ro"
            snap_share_args="--perm read"
            ;;
        rw)
            exp_args="$FSEXP_RW_EPS --perm rw --comments TESTCOMMENTS"
            exp_upd_args="--readwritehosts $FSEXP2"
            snap_exp_args="$FSEXP_RW_EPS --perm ro"
            snap_share_args="--perm read"
            ;;
        root)
            exp_args="$FSEXP_ROOT_EPS --perm root --rootuser root --comments TESTCOMMENTS"
            exp_upd_args="--roothosts $FSEXP3"
            snap_exp_args="$FSEXP_ROOT_EPS --perm ro --rootuser root"
            snap_share_args="--perm read"
            ;;
        default)
            exp_args="$FSEXP_DEFAULT_EPS --comments TESTCOMMENTS"
            exp_upd_args="--readwritehosts $FSEXP4"
            snap_exp_args="$FSEXP_DEFAULT_EPS"
            snap_share_args="--perm read"
            ;;
        esac
        echo "exp args = $exp_args"
        run fileshare export $PROJECT/$fsname $exp_args
        run fileshare show $PROJECT/$fsname

        echo "exp upd args = $exp_upd_args"
        run fileshare export_update $PROJECT/$fsname $exp_upd_args --operation modify --securityflavor sys 

        if [ $cos != $COS_VNXE ] ; then
            let "FS_EXPAND_SIZE = $FS_SIZE + $FS_EXPAND_SIZE"
            run fileshare expand $PROJECT/$fsname $FS_EXPAND_SIZE
        fi

        if [ $cos = $COS_VNXFILE ] ; then
            run snapshot create $PROJECT/$fsname $fsname-$datetime
            run snapshot export $PROJECT/$fsname-$datetime $PROJECT/$fsname $snap_exp_args
            run snapshot show $PROJECT/$fsname-$datetime $PROJECT/$fsname
            run snapshot restore $PROJECT/$fsname-$datetime $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime $PROJECT/$fsname
            run snapshot delete $PROJECT/$fsname-$datetime $PROJECT/$fsname
            run snapshot create $PROJECT/$fsname $fsname-$datetime-2
            run snapshot export $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $SNAPEXP_DEFAULT_EPS
            run snapshot show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot create $PROJECT/$fsname $fsname-$datetime-3
            run snapshot export $PROJECT/$fsname-$datetime-3 $PROJECT/$fsname $SNAPEXP_SECKRP_EPS
            run snapshot show $PROJECT/$fsname-$datetime-3 $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime-3 $PROJECT/$fsname
            run snapshot show $PROJECT/$fsname-$datetime-3 $PROJECT/$fsname
            run snapshot delete $PROJECT/$fsname-$datetime-3 $PROJECT/$fsname
        fi
        run fileshare unexport $PROJECT/$fsname

    	if [ $cos = $COS_NETAPP ] ; then
            run snapshot create $PROJECT/$fsname $fsname-$datetime-2
            run snapshot export $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $snap_exp_args
            run snapshot show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot restore $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot share $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE1 --description 'New_SNAPSHOT_SMB_Share1' --perm 'read'
	        run snapshot share $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE2 --description 'New_SNAPSHOT_SMB_Share2' --perm 'read'
            echo "Snapshot Share ACL testing for NetApp7 Started===========>>>"
			run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE1 --user 'Everyone' --permission 'read' --operation add
            run snapshot share_acl_show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE1
	        run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE1 --user 'Everyone' --permission 'read' --operation modify
            run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE1 --user 'Everyone' --permission 'read' --operation delete
	        run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE2 --user 'Everyone' --permission 'read' --operation add
    	    run snapshot share_acl_delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname  $NETAPPF_SMBSNAPSHARE2
			echo "Snapshot Share ACL testing for NetApp7 Finished===========<<<"
            run snapshot unshare $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE1 
	        run snapshot unshare $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPF_SMBSNAPSHARE2 
            run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
        fi

        if [ $cos = $COS_NETAPPC ] ; then
            run snapshot create $PROJECT/$fsname $fsname-$datetime-2
            run snapshot show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname	    
           run snapshot share $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE1 --description 'New_SNAPSHOT_SMB_Share1' --perm 'read'
	        run snapshot share $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE2 --description 'New_SNAPSHOT_SMB_Share2' --perm 'read'
			echo "Snapshot Share ACL testing for NetApp Cluster Started===========>>>"
            run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE1 --user 'Everyone' --permission 'read' --operation add
            run snapshot share_acl_show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE1
	        run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE1 --user 'Everyone' --permission 'read' --operation modify
            run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE1 --user 'Everyone' --permission 'read' --operation delete
	        run snapshot share_acl $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE2 --user 'Everyone' --permission 'read' --operation add
    	    run snapshot share_acl_delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname  $NETAPPCF_SMBSNAPSHARE2
			echo "Snapshot Share ACL testing for NetApp Cluster done===========<<<"
            run snapshot unshare $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE1 
	        run snapshot unshare $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $NETAPPCF_SMBSNAPSHARE2
		    run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
        fi
        
        if [ $cos = $COS_VNXE ] ; then
            run snapshot create $PROJECT/$fsname $fsname-$datetime-2
            run snapshot restore $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot export $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname $snap_exp_args
            run snapshot show $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
            run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
        fi
        
    done

    # Run only for Isilon (temporary until vnx implements smb)
    if [ $cos = $COS_ISIFILE ] ; then

        run fileshare share $PROJECT/$fsname  $ISI_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare share $PROJECT/$fsname  $ISI_SMBFILESHARE2 --description 'New_SMB_Share2'
		echo "File Share ACL testing for ISILON Started"
        run fileshare share_acl $PROJECT/$fsname  $ISI_SMBFILESHARE1 --user 'Everyone' --permission 'change' --operation add
	    run fileshare share_acl_show $PROJECT/$fsname  $ISI_SMBFILESHARE1
	    run fileshare share_acl $PROJECT/$fsname  $ISI_SMBFILESHARE1 --user 'Everyone' --permission 'read' --operation modify
	    run fileshare share_acl $PROJECT/$fsname  $ISI_SMBFILESHARE1 --user 'Everyone' --permission 'read' --operation delete
	    run fileshare share_acl $PROJECT/$fsname  $ISI_SMBFILESHARE2 --user 'Everyone' --permission 'read' --operation add
    	run fileshare share_acl_delete $PROJECT/$fsname  $ISI_SMBFILESHARE2
		echo "File Share ACL testing for ISILON done"
        run fileshare show $PROJECT/$fsname

	    # Deleting the FileSystem Shares.
        run fileshare unshare $PROJECT/$fsname $ISI_SMBFILESHARE1
        run fileshare unshare $PROJECT/$fsname $ISI_SMBFILESHARE2
    fi

    # Run only for Data Domain
    if [ $cos = $COS_DDFILE ] ; then
        echo "DD SMB share tests started...create 2 fileshares"
        run fileshare share $PROJECT/$fsname  $DATADOMAINF_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare share $PROJECT/$fsname  $DATADOMAINF_SMBFILESHARE2 --description 'New_SMB_Share2'
        echo "DD SMB share tests started...create 2 fileshares done"
        run fileshare show $PROJECT/$fsname

        # Deleting the FileSystem Shares.
        echo "Delete 1st fileshare..."
        run fileshare unshare $PROJECT/$fsname $DATADOMAINF_SMBFILESHARE1
        echo "Delete 1st fileshare done...deleting 2nd fileshare"
        run fileshare unshare $PROJECT/$fsname $DATADOMAINF_SMBFILESHARE2
        echo "Delete fileshares done"

    fi
    
    # Run only for Netapp 
    if [ $cos = $COS_NETAPP ] ; then
        run fileshare share $PROJECT/$fsname  $NETAPPF_SMBFILESHARE1 --description 'New_SMB_Share'
		run fileshare share $PROJECT/$fsname  $NETAPPF_SMBFILESHARE2 --description 'New_SMB_Share2'
        run fileshare show $PROJECT/$fsname
		echo "File Share ACL testing for NetApp7 Started"
		run fileshare share_acl $PROJECT/$fsname  $NETAPPF_SMBFILESHARE1 --user 'Everyone' --permission 'change' --operation add
	    run fileshare share_acl_show $PROJECT/$fsname  $NETAPPF_SMBFILESHARE1
	    run fileshare share_acl $PROJECT/$fsname  $NETAPPF_SMBFILESHARE1 --user 'Everyone' --permission 'read' --operation modify
	    run fileshare share_acl $PROJECT/$fsname  $NETAPPF_SMBFILESHARE1 --user 'Everyone' --permission 'read' --operation delete
	    run fileshare share_acl $PROJECT/$fsname  $NETAPPF_SMBFILESHARE2 --user 'Everyone' --permission 'read' --operation add
    	run fileshare share_acl_delete $PROJECT/$fsname  $NETAPPF_SMBFILESHARE2
		echo "File Share ACL testing for NetApp7 Finished"
        run fileshare unshare $PROJECT/$fsname $NETAPPF_SMBFILESHARE1
		run fileshare unshare $PROJECT/$fsname $NETAPPF_SMBFILESHARE2
		
    fi

    if [ $cos = $COS_NETAPPC ] ; then
        run fileshare share $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE1 --description 'New_SMB_Share'
		run fileshare share $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE2 --description 'New_SMB_Share2'
        run fileshare show $PROJECT/$fsname
		echo "File Share ACL testing for NetApp Cluster Started"
		run fileshare share_acl $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE1 --user 'Everyone' --permission 'change' --operation add
	    run fileshare share_acl_show $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE1
	    run fileshare share_acl $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE1 --user 'Everyone' --permission 'read' --operation modify
	    run fileshare share_acl $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE1 --user 'Everyone' --permission 'read' --operation delete
	    run fileshare share_acl $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE2 --user 'Everyone' --permission 'read' --operation add
    	run fileshare share_acl_delete $PROJECT/$fsname  $NETAPPCF_SMBFILESHARE2
		echo "File Share ACL testing for NetApp Cluster Finished"
        run fileshare unshare $PROJECT/$fsname $NETAPPCF_SMBFILESHARE1
		run fileshare unshare $PROJECT/$fsname $NETAPPCF_SMBFILESHARE2
    fi
    
    if [ $cos = $COS_VNXFILE ] ; then
        fscifs=filesysCifs-$datetime
        run fileshare create $fscifs $PROJECT $NH $cos $FS_SIZEMB
        run fileshare share $PROJECT/$fscifs  $VNXF_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare show $PROJECT/$fscifs
        run fileshare unshare $PROJECT/$fscifs $VNXF_SMBFILESHARE1
        run fileshare delete $PROJECT/$fscifs
    fi
    
    if [ $cos = $COS_VNXE ] ; then
        run fileshare share $PROJECT/$fsname  $VNXE_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare show $PROJECT/$fsname
        run fileshare unshare $PROJECT/$fsname $VNXE_SMBFILESHARE1 
    fi
    
    if [ $cos = $COS_ISIFILE ] ; then
        run snapshot delete $PROJECT/$fsname-$datetime $PROJECT/$fsname
        run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
    fi
   
    # create some fileshares for bulk get test
     if [ $cos != $COS_VNXE ] ; then
	    fs1=sanityFs1-$datetime
	    fs2=sanityFs2-$datetime
	    run fileshare create ${fs1} $PROJECT $NH $cos $FS_SIZE
	    run fileshare create ${fs2} $PROJECT $NH $cos $FS_SIZE
	    run fileshare bulkget
	    run fileshare delete $PROJECT/${fs1}
	    run fileshare delete $PROJECT/${fs2}
    fi

    run fileshare delete $PROJECT/$fsname --forceDelete true

    if [ $cos = $COS_VNXFILE ] ; then
        fsForceDelete="forceDeleteTestVNX"
        echo "--Force Delete tests for VNXFile--"
        run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        run fileshare export $PROJECT/$fsForceDelete $exp_args
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime-2
        run snapshot export $PROJECT/$fsForceDelete-$datetime-2 $PROJECT/$fsForceDelete $SNAPEXP_DEFAULT_EPS
        run fileshare delete $PROJECT/$fsForceDelete --forceDelete true
        # duplicate label Test - Should fail
        # run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        # run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE

    fi


    if [ $cos = $COS_NETAPP ] ; then
        fsForceDelete="forceDeleteTestNETAPP"
        echo "--Force Delete tests for NETAPP--"
        run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime-2
        run fileshare delete $PROJECT/$fsForceDelete --forceDelete true
        # duplicate label Test - Should fail
        # run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        # run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE

    fi

    if [ $cos = $COS_NETAPPC ] ; then
        fsForceDelete="forceDeleteTestNETAPPC"
        echo "--Force Delete tests for NETAPPC--"
        run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime-2
        run fileshare delete $PROJECT/$fsForceDelete --forceDelete true
        # duplicate label Test - Should fail
        # run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        # run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE

    fi

    if [ $cos = $COS_ISIFILE ] ; then
        fsForceDelete="forceDeleteTestISILON"
        echo "--Force Delete tests for ISILON--"
        run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime
        run snapshot create $PROJECT/$fsForceDelete $fsForceDelete-$datetime-2
        run fileshare delete $PROJECT/$fsForceDelete --forceDelete true
	# duplicate label Test - Should fail
	# run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
	# run fileshare create $fsForceDelete $PROJECT $NH $cos $FS_SIZE
    fi

    # Force Delete test for Data Domain
    if [ $cos = $COS_DDFILE ] ; then
        fsForceDelete="forceDeleteTestDATADOMAIN"
        #File systems (mtrees) are not deleted from DDMC, just marked inactive
        #until the GC runs.  To avoid conflict with the previous run, we attach
        #a timestamp to FS name.
        fsForceDeleteTimed=$fsForceDelete-$datetime
        echo "--Force Delete tests for DATADOMAIN--"
        run fileshare create $fsForceDeleteTimed $PROJECT $NH $cos $FS_SIZE
        run fileshare delete $PROJECT/$fsForceDeleteTimed --forceDelete true
    fi

    # File System Quota directory ops.
    if [ $cos = $COS_NETAPP ] ; then
        echo "--Quota directory operations for NETAPP--"
        fsQuotaDir="FileSystemQuotaDirTest"--$datetime
        quotaDir1="NetAppQuotaDirTest1"
        quotaDir2="NetAppQuotaDirTest1234" 
        run fileshare create $fsQuotaDir $PROJECT $NH $cos $FS_SIZE
        run fileshare create_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "unix" --size $FS_SIZE --oplock true
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare update_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "ntfs" 
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 
        run fileshare delete_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --forceDelete true
        run fileshare delete $PROJECT/$fsQuotaDir --forceDelete true
    fi
    
    # File System Quota directory ops.
    if [ $cos = $COS_NETAPPC ] ; then
        echo "--Quota directory operations for NETAPP Cluster--"
        fsQuotaDir="FileSystemQuotaDirTest"--$datetime
        quotaDir1="NetAppQuotaDirTest1"
        quotaDir2="NetAppQuotaDirTest1234" 
        run fileshare create $fsQuotaDir $PROJECT $NH $cos $FS_SIZE
        run fileshare create_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "unix" --size $FS_SIZE --oplock true
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare update_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "ntfs" 
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 
        run fileshare delete_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --forceDelete true
    fi
    
    # File System Quota directory ops.
    if [ $cos = $COS_ISIFILE ] ; then
        echo "--Quota directory operations for Isilon--"
        fsQuotaDir="FileSystemQuotaDirTest"--$datetime
        quotaDir1="IsilonQuotaDirTest1"
        quotaDir2="IsilonQuotaDirTest2" 
        run fileshare create $fsQuotaDir $PROJECT $NH $cos $FS_SIZE
        run fileshare create_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "unix" --size $FS_SIZE --oplock true
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare update_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "ntfs" 
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 
        run fileshare delete_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --forceDelete true
        run fileshare delete $PROJECT/$fsQuotaDir --forceDelete true
    fi
    
    # File System Quota directory ops.
    if [ $cos = $COS_VNXFILE ] ; then
        echo "--Quota directory operations for VNXFile--"
        fsQuotaDir="FileSystemQuotaDirTest"--$datetime
        quotaDir1="VNXQuotaDir1"
        quotaDir2="VNXQuotaDir2"
        quotafs_exp_args="$FSEXP_SHARED_VARRAY_RW_EPS --perm rw  --comments TESTCOMMENTS"
        run fileshare create $fsQuotaDir $PROJECT $NH $cos $FS_SIZE
        run fileshare export $PROJECT/$fsQuotaDir $quotafs_exp_args
        run fileshare create_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "unix" --size $FS_SIZE --oplock true
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare update_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --sec "ntfs"
        run fileshare show_quota_dir $PROJECT/$fsQuotaDir $quotaDir2
        run fileshare delete_quota_dir $PROJECT/$fsQuotaDir $quotaDir2 --forceDelete true
        run fileshare delete $PROJECT/$fsQuotaDir --forceDelete true
    fi

}

quick_file_tests()
{
    cos=$1; shift
    perms=${@}
    datetime=`date +%m%d%y%H%M%S`
    fsname=fs-$cos-$macaddr-$datetime
    fsname=$(echo $fsname | sed s/-/_/g)
    fsname=$(echo $fsname | sed s/:/_/g)
    echo $fsname

    run fileshare create $fsname $PROJECT $NH $cos $FS_SIZE
    if [ $cos = $COS_ISIFILE ] ; then
        run snapshot create $PROJECT/$fsname $fsname-$datetime
        run snapshot create $PROJECT/$fsname $fsname-$datetime-2
    fi

    if [ $cos = $COS_DDFILE ] ; then
        echo "DD snapshot create"
        run snapshot create $PROJECT/$fsname $fsname-$datetime
        run snapshot create $PROJECT/$fsname $fsname-$datetime-2
    fi

    for p in $perms
    do
        case $p in
        ro)
            exp_args="$FSEXP_RO_EPS --perm ro"
            snap_share_args="--perm read"
            ;;
        rw)
            exp_args="$FSEXP_RW_EPS --perm rw"
            snap_share_args="--perm read"
            ;;
        root)
            exp_args="$FSEXP_ROOT_EPS --perm root --rootuser root"
            snap_share_args="--perm read"
            ;;
        default)
            exp_args="$FSEXP_DEFAULT_EPS"
            snap_share_args="--perm read"
            ;;
        esac

        run fileshare export $PROJECT/$fsname   $exp_args
        run fileshare show $PROJECT/$fsname
        if [ $cos = $COS_VNXFILE ] ; then
            run snapshot create $PROJECT/$fsname $fsname-$datetime
            run snapshot export $PROJECT/$fsname-$datetime $PROJECT/$fsname $exp_args
            run snapshot show $PROJECT/$fsname-$datetime $PROJECT/$fsname
            run snapshot restore $PROJECT/$fsname-$datetime $PROJECT/$fsname
            run snapshot unexport $PROJECT/$fsname-$datetime $PROJECT/$fsname 
            run snapshot delete $PROJECT/$fsname-$datetime $PROJECT/$fsname
        fi
        run fileshare unexport $PROJECT/$fsname

       # if [ $cos = $COS_ISIFILE ] ; then
       #     run snapshot export $PROJECT/$fsname-$datetime $PROJECT/$fsname $exp_args
       #     run snapshot show $PROJECT/$fsname-$datetime $PROJECT/$fsname
       #     run snapshot unexport $PROJECT/$fsname-$datetime $PROJECT/$fsname 
       # fi
    done

    # Run only for Isilon (temporary until vnx implements smb)
    if [ $cos = $COS_ISIFILE ] ; then
        run fileshare share $PROJECT/$fsname  $ISI_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare show $PROJECT/$fsname
        run fileshare unshare $PROJECT/$fsname $ISI_SMBFILESHARE1 
        #
        run snapshot share $PROJECT/$fsname-$datetime $PROJECT/$fsname  $ISI_SMBSNAPSHARE1  --description 'New_SMB_Share_For_Snapshot' $snap_share_args
        run snapshot share_list $PROJECT/$fsname-$datetime $PROJECT/$fsname
        run snapshot show $PROJECT/$fsname-$datetime $PROJECT/$fsname
        run snapshot unshare $PROJECT/$fsname-$datetime $PROJECT/$fsname $ISI_SMBSNAPSHARE1 
        run snapshot share_list $PROJECT/$fsname-$datetime $PROJECT/$fsname
        run snapshot show $PROJECT/$fsname-$datetime $PROJECT/$fsname
    fi

    if [ $cos = $COS_ISIFILE ] ; then
        run snapshot delete $PROJECT/$fsname-$datetime $PROJECT/$fsname
        run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
    fi
    # Data Domain share tests for both fileshare and snapshot
    if [ $cos = $COS_DDFILE ] ; then
        echo "DD tests for fileshare and snapshot"
        run fileshare share $PROJECT/$fsname  $DATADOMAINF_SMBFILESHARE1 --description 'New_SMB_Share'
        run fileshare show $PROJECT/$fsname
        run fileshare unshare $PROJECT/$fsname $DATADOMAINF_SMBFILESHARE1
    fi

    if [ $cos = $COS_DDFILE ] ; then
        echo "DD snapshot delete 2"
        run snapshot delete $PROJECT/$fsname-$datetime $PROJECT/$fsname
        run snapshot delete $PROJECT/$fsname-$datetime-2 $PROJECT/$fsname
    fi 

    run fileshare delete $PROJECT/$fsname
}

#
# do fileshare tests using isilon specific cos to exercise Isilon tests
#
isilon_tests()
{
    file_tests $COS_ISIFILE default ro rw root

    if [ "$AUTH" != 'local' && "$AUTH" != 'ipv6' ] ; then
        bulkapi tenants $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi projects $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi neighborhoods $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        # hosts, clusters and vcenters will need to ran through a setup/create process before beeing called
        #bulkapi hosts $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        #bulkapi clusters $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        #bulkapi vcenters $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1

        bulkapi storage-systems $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi storage-ports $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi storage-pools $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi transport-zones $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi filecos $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi fileshares $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1 $NH $COS_ISIFILE $FS_SIZE
    fi
}

ecs_tests()
{
    echo "Performing ecs_tests"
    bucket create $ECS_BUCKET $PROJECT $NH $COS_ECS $ECS_SOFT_QUOTA $ECS_HARD_QUOTA $ECS_BUCKET_OWNER
    #bucket delete bkt_id  #currently create itself deletes bucket
}


#
# do fileshare tests using isilon specific cos to exercise Isilon tests
#
vnxfile_tests()
{
    file_tests $COS_VNXFILE default
}

#
# do fileshare tests using manually assigned ports to varray
#
vnxfile_flex_varray_tests () {
    cos=$COS_VNXFILE
    fsname=fs-$cos-$macaddr
    exp_args="$FSEXP_SHARED_VARRAY_RW_EPS --perm rw"
    NH=$NH3
    FSEXP_RW_EPS=$FSEXP_SHARED_VARRAY_RW_EPS
    file_tests $COS_VNXFILE rw
}

netapp_tests()
{
    file_tests $COS_NETAPP default
}

netappc_tests()
{
    file_tests $COS_NETAPPC default
}

vnxe_tests()
{
	FS_SIZEMB=$FS_VNXE_SIZE;
	FS_SIZE=$FS_VNXE_SIZE;
	FS_EXPAND_SIZE=$FS_VNXE_EXPAND_SIZE;
	file_tests $COS_VNXE default
	
	echo "vnxe block tests begin"
	vnxe_block_tests
}

vnxe_block_tests() {
	vol1=vnxe1-${RANDOM};
	vol2=vnxe-cg-${RANDOM};
    host=$PROJECT.lss.emc.com
    hostLbl=$PROJECT
	iqn1=iqn.1998-01.com.vmware:lgly6193-7ae20d76
	consistency_group=cg-${RANDOM}
	snap1_label=snap1-${RANDOM}
	snap2_label=snap2-${RANDOM}
	eg=eg-${RANDOM}
    
    run transportzone add $NH/$IP_ZONE $iqn1
    run volume create $vol1 $PROJECT $NH $COS_VNXEBLOCK_ISCSI $BLK_SIZE
    run volume expand $PROJECT/$vol1 $BLK_SIZE_EXPAND

    run hosts create $hostLbl $TENANT Windows $host --port 8111 --username user --password 'password' --osversion 1.0
    run initiator create $hostLbl iSCSI $iqn1

    run export_group create $PROJECT $eg $NH --type Host --volspec $PROJECT/$vol1 --hosts $hostLbl
	
    run blocksnapshot create $PROJECT/$vol1 $snap1_label
    run blocksnapshot list $PROJECT/$vol1
    run blocksnapshot restore $PROJECT/$vol1/${snap1_label}
    run blocksnapshot delete $PROJECT/$vol1/${snap1_label}
    run export_group delete $PROJECT/$eg
    run volume delete $PROJECT/$vol1 --wait
    run hosts delete $hostLbl
    
    echo "vnxe consistency group tests"
    
    run blockconsistencygroup create $PROJECT $consistency_group
    run volume create $vol2 $PROJECT $NH $COS_VNXEBLOCK_CG 1280000000 --consistencyGroup $consistency_group
    run blocksnapshot create $PROJECT/$vol2 $snap2_label
    run blocksnapshot delete $PROJECT/$vol2/${snap2_label}
    run volume delete $PROJECT/$vol2 --wait
    run blockconsistencygroup delete $consistency_group
    echo "**** Done vnxe"
}

datadomainfile_tests()
{
    echo "DD file tests started"
    file_tests $COS_DDFILE default
    echo "DD file tests completed succesfully"
}

#
# do vmax block tests using network assignment to varray
#
vmaxblock_tests()
{
    block_tests $VMAXEXPORT_GROUP $VMAXEXPORT_GROUP_HOST $VMAX_VOLUME $COS_VMAXBLOCK $VMAX_VOLUME $COS_VMAXBLOCK
    # meta_volume_block_tests $VMAXEXPORT_GROUP $VMAXEXPORT_GROUP_HOST $VMAX_META_VOLUME $COS_VMAXBLOCK $VMAX_META_VOLUME $COS_VMAXBLOCK
    volume_expand_test $VMAX_VOLUME $COS_VMAXBLOCK_THIN   # work only for concatenate, stripe tbd

    if [ "${AUTH}" != "local" -a "${AUTH}" != "ipv6" ]
    then
        bulkapi tenants $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi projects $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi neighborhoods $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        # hosts, clusters and vcenters will need to ran through a setup/create process before beeing called
        #bulkapi hosts $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        #bulkapi clusters $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        #bulkapi vcenters $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1

        bulkapi smis-providers $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi storage-systems $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi storage-ports $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi storage-pools $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi transport-zones $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
        bulkapi blockcos $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    # ---------------------------------------------------------
    # Thick/Thin volume testing
    # ---------------------------------------------------------
    # The following tests depend on the VMAX array type.
    # Some arrays only support thin pools, so the thick test
    # cases would not work against them.
    TkOnTk=vmax-thick-on-thick-${seed}
    TnOnTn=vmax-thin-on-thin-${seed}

#    run volume create $TkOnTk $PROJECT $NH $COS_VMAXBLOCK_THICK $BLK_SIZE
#    run volume create $TnOnTn $PROJECT $NH $COS_VMAXBLOCK_THIN $BLK_SIZE --thinVolume true

#    run volume delete $PROJECT/${TkOnTk} --wait &
#    run volume delete $PROJECT/${TnOnTn} --wait &
#    wait
}

#
# do vmax block tests using using manual port assignments tp varray
#

# Before proceeding with ingestion, we need to substitute volumes & host details here.
ingestblock_setup() {
    secho "Ingest Block Tests Setup"

    xtremio_setup

    run storagedevice discover_namespace $XTREMIO_NATIVEGUID 'UNMANAGED_VOLUMES'
    run hosts create $XTREMIO_INGEST_HOST_LABEL $TENANT Windows $XTREMIO_INGEST_HOST --port 8111 --username user --password 'password' --osversion 1.0 

    PWWN1=10:00:00:E0:E0:E0:E0:E0
    WWNN1=20:00:00:E0:E0:E0:E0:E0
    PWWN2=10:00:00:E0:E0:E0:E0:E1
    WWNN2=20:00:00:E0:E0:E0:E0:E1

    run initiator create 'ingesthost' FC ${PWWN1} --node ${WWNN1}
    run initiator create 'ingesthost' FC ${PWWN2} --node ${WWNN2}
}

ingestblock_tests() {
    secho "VMAX Ingest Block Tests BEGIN"
    vol1=ingestvol1

    run unmanagedvolume ingest_export --host $XTREMIO_INGEST_HOST_LABEL $NH $XTREMIO_COS_FC $PROJECT --volspec $vol1

#   unmanagedvolume ingest_unexport $NH $XTREMIO_COS_FC $PROJECT --volspec "vol1,vol2"
#   unmanagedvolume ingest_unexport $NH $vpool $PROJECT --volspec $vol1
   
    secho "VMAX Ingest Block Tests END"
}

vmaxblock_flex_varray_tests() {
    # run the usual tests
    vmaxblock_tests

    # restore network assignment and remove manual port assignment
    transportzone update $FC_ZONE_A --addNeighborhoods $NH
    transportzone update $FC_ZONE_B --addNeighborhoods $NH
    transportzone update $IP_ZONE --addNeighborhoods $NH

    for porta in ${VMAX_PORTS_A}
    do
        storageport update $VMAX_NATIVEGUID FC --tzone $FCTZ_A --group ${porta}
    done
    for portb in ${VMAX_PORTS_B}
    do
        storageport update $VMAX_NATIVEGUID FC --tzone $FCTZ_B --group ${portb}
    done
    storageport update $VMAX_NATIVEGUID IP --tzone nh/iptz
}

#
# do vnx block tests using network assignment to varray
#
vnxblock_tests()
{
    block_tests $VNXEXPORT_GROUP $VNXEXPORT_GROUP_HOST $VNX_VOLUME $COS_VNXBLOCK $VNX_VOLUME $COS_VNXBLOCK
    volume_expand_test $VNX_VOLUME $COS_VNXBLOCK_THIN

    # Thick/Thin volume testing
    TkOnTn=vnx-thick-on-thin-${seed}
    TkOnTk=vnx-thick-on-thick-${seed}
    TnOnTn=vnx-thin-on-thin-${seed}

    run volume create $TkOnTn $PROJECT $NH $COS_VNXBLOCK $BLK_SIZE
    run volume create $TkOnTk $PROJECT $NH $COS_VNXBLOCK_THICK $BLK_SIZE
    run volume create $TnOnTn $PROJECT $NH $COS_VNXBLOCK $BLK_SIZE --thinVolume true

    run volume delete $PROJECT/${TkOnTn} --wait &
    run volume delete $PROJECT/${TkOnTk} --wait &
    run volume delete $PROJECT/${TnOnTn} --wait &
    wait
}

#
# do vnx block tests using using manual port assignments tp varray
#
vnxblock_flex_varray_tests()
{
    vnxblock_tests

    # undo manual assignment of ports
    transportzone update $FC_ZONE_A --addNeighborhoods $NH
    transportzone update $FC_ZONE_B --addNeighborhoods $NH
    transportzone update $IP_ZONE --addNeighborhoods $NH

    storageport update $VNXB_NATIVEGUID FC --rmvarrays $NH --group SP_A
    storageport update $VNXB_NATIVEGUID FC --rmvarrays $NH --group SP_B
    storageport update $VNXB_NATIVEGUID IP --rmvarrays $NH
}

#
# do block tests that spans arrays
#
combined_block_tests()
{
    block_tests $BLOCKEXPORT_GROUP $VMAX_VNXEXPORT_GROUP_HOST $VNX_VOLUME $COS_VNXBLOCK $VMAX_VOLUME $COS_VMAXBLOCK
}

block_tests()
{
    export_name=$1
    export_host=$2
    v1=${3}1
    cos1=$4
    v2=${5}2
    cos2=$6

    run volume create ${v1} $PROJECT $NH $cos1 $BLK_SIZE --thinVolume true
    if [ "$EXTRA_PARAM" = "search" ] ; then
        run volume search --name $(echo ${v1}| head -c 2)
        run volume search --name $(echo ${v1}| head -c 2) --project $projectid

        run volume tag $PROJECT/"$v1" $TAG
        run volume search --tag $SEARCH_PREFIX 
    fi

    run volume create ${v2} $PROJECT $NH $cos2 $BLK_SIZE --thinVolume true

#    export_test_1 ${export_name}1 ${export_host}1 ${v1} ${v2}
#    export_test_2 ${export_name}2 ${export_host}2 ${v1} ${v2}
#    export_test_3 ${export_name}3 ${export_host}3 ${v1} ${v2}
#    export_test_4 ${export_name}4 ${export_host}4 ${v1} ${v2}
    export_initiator ${export_name}1 ${v1} ${v2}
#    export_host ${export_name}2 ${v1} ${v2}
#    export_cluster ${export_name}3 ${v1} ${v2}

    run volume bulkget
    
    run volume delete $PROJECT/${v1} --wait
    run volume delete $PROJECT/${v2} --wait
}


meta_volume_block_tests()
{
#    size=1099511627776    # 1TB
    size=322122547200      # 300GB
    export_name=$1
    export_host=$2
    v1=${3}1
    cos1=$4
    v2=${5}2
    cos2=$6

    run volume create ${v1} $PROJECT $NH $cos1 $size --thinVolume true
    run volume create ${v2} $PROJECT $NH $cos2 $size --thinVolume true
    sleep 15 # sleep to make sure that volume binding completed

#    export_test_1 ${export_name}1 ${export_host}1 ${v1} ${v2}
#    export_test_2 ${export_name}2 ${export_host}2 ${v1} ${v2}
#    export_test_3 ${export_name}3 ${export_host}3 ${v1} ${v2}
#    export_test_4 ${export_name}4 ${export_host}4 ${v1} ${v2}
    
    run volume delete $PROJECT --project --wait
}

quick_block_tests()
{
    export_name=$1
    export_host=$2
    v=$3
    cos1=$4

    v1=${3}-1
    v2=${3}-2

    run volume create $v $PROJECT $NH $cos1 $BLK_SIZE --thinVolume true --count=2
    export_initiator_quick ${export_name}Q ${v1} ${v2}
    run volume delete $PROJECT --project --wait
}

volume_expand_test()
{
    ev=$1-${seed}
    cos=$2

    run volume create ${ev} $PROJECT $NH $cos $BLK_SIZE
    run volume expand $PROJECT/$ev $BLK_SIZE_EXPAND
    run volume show $PROJECT/$ev
    run volume expand $PROJECT/$ev $BLK_SIZE_EXPAND_2
    run volume show $PROJECT/$ev
    run volume expand $PROJECT/$ev $BLK_SIZE_EXPAND_3
    run volume show $PROJECT/$ev

    run volume delete $PROJECT/${ev} --wait
}

# =======================================================================================
# Export Group 1
# =======================================================================================
export_test_1()
{
    expname=$1
    host=$2
    vol1=$PROJECT/$3
    vol2=$PROJECT/$4

    proj=$PROJECT
    exp=$proj/$expname
    NWWN=$BLK_CLIENT_FC_NODE
    PWWN1=`pwwn A1`
    PWWN2=`pwwn A2`

    run export_group create $proj $expname $NH
    run export_group add_volume $exp "$vol1+1"
    run export_group add_volume $exp "$vol2+2"
    run export_group show $exp
    run export_group remove_volume $exp "$vol1"
    run export_group show $exp
    run export_group add_volume $exp "$vol1+1"

    run export_group add_initiator $exp "FC+$NWWN+$PWWN1+$host"
    run export_group show $exp

    run export_group add_initiator $exp "FC+$NWWN+$PWWN2+$host"
    run export_group remove_initiator $exp "FC+$PWWN1"
    run export_group show $exp

    run export_group add_initiator $exp "FC+$NWWN+$PWWN1+$host"
    run export_group remove_initiator $exp "FC+$PWWN2"
    run export_group show $exp

    run export_group remove_volume $exp "$vol1"
    run export_group show $exp

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi exportgroups $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group delete $exp
    run export_group show $exp
}

# =======================================================================================
# Export Exclusive Type
# =======================================================================================
export_initiator()
{   
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    vol1=$PROJECT/$2
    vol2=$PROJECT/$3
    snap1=${vol1}/${snap1_label}
    proj=$PROJECT
    tenant=$TENANT
    c=1
    h=1
    expname=$1
    hostname=$hostbase$tenant$c$h
    echo $vol1 $vol2 $proj $tenant $hostname $expname
    
    exp=$proj/$expname
    nwwn=`nwwn $i$j`
    k=`wwnIdx $c $h`
    pwwn1=`pwwn A$k`
    pwwn2=`pwwn B$k`
    pwwn3=`pwwn C$k`
    pwwn4=`pwwn D$k`

    run blocksnapshot create $vol1 ${snap1_label}

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi blocksnapshots $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group create $proj $expname $NH --volspec "$vol1+1,$snap1+2" --inits "$hostname/$pwwn1"
    run export_group show $exp
    run export_group update $exp --addVolspec "$vol2+3" --remVols $vol1
    run export_group show $exp
    run export_group update $exp --addInits "$hostname/$pwwn3"
    run export_group show $exp
    run export_group update $exp --remInits "$hostname/$pwwn1"
    run export_group show $exp
    run export_group update $exp --remVols $vol2,$snap1
    run export_group show $exp

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi exportgroups $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group delete $exp
    run blocksnapshot delete $snap1
}

# =======================================================================================
# Export Initiator Type (Quick)
# =======================================================================================
export_initiator_quick()
{
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    vol1=$PROJECT/$2
    vol2=$PROJECT/$3
    snap1=${vol1}/${snap1_label}
    proj=$PROJECT
    tenant=$TENANT
    c=1
    h=1
    expname=$1
    hostname=$hostbase$tenant$c$h
    echo $vol1 $vol2 $proj $tenant $hostname $expname

    exp=$proj/$expname
    nwwn=`nwwn $i$j`
    k=`wwnIdx $c $h`
    pwwn1=`pwwn A$k`
    pwwn2=`pwwn B$k`
    pwwn3=`pwwn C$k`
    pwwn4=`pwwn D$k`

    run blocksnapshot create $vol1 ${snap1_label}

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi blocksnapshots $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group create $proj $expname $NH --volspec "$vol1+1,$snap1+2" --inits "$hostname/$pwwn1"
    run export_group show $exp

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi exportgroups $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group delete $exp
    run blocksnapshot delete $snap1
    secho "Export initiator quick complete"
}

# =======================================================================================
# Export Host Type
# =======================================================================================
export_host()
{
    vol1=$PROJECT/$2
    vol2=$PROJECT/$3
    proj=$PROJECT
    tenant=$TENANT
    expname=$1
    exp=$proj/$expname
    
    c=1
    h=1
    hostname=$hostbase$tenant$c$h
    nwwn1=`nwwn $i$j`
    k=`k $c $h`
    pwwn11=`pwwn A$k`
    pwwn12=`pwwn B$k`
    pwwn13=`pwwn C$k`
    pwwn14=`pwwn D$k`
    
    h=2
    hostname2=$hostbase$tenant$c$h
    nwwn2=`nwwn $i$j`
    k=`k $c $h`
    pwwn21=`pwwn A$k`
    pwwn22=`pwwn B$k`
    pwwn23=`pwwn C$k`
    pwwn24=`pwwn D$k`

    run export_group create --type Host $proj $expname $NH --volspec "$vol1+1" --hosts "$hostname1"
    run export_group show $exp
    run export_group update $exp --addVolspec "$vol2+1" --remVols $vol1
    run export_group show $exp
    run export_group update $exp --addHosts "$hostname2" --remHosts "$hostname1"
    run export_group show $exp
    run export_group update $exp --remInits "$hostname2/pwwn21,$hostname2/pwwn23"
    run export_group show $exp
    run export_group update $exp --remVols $vol2
    run export_group show $exp

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi exportgroups $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group delete $exp
    run export_group show $exp
}

# =======================================================================================
# Export Cluster Type
# =======================================================================================
export_cluster()
{
    vol1=$PROJECT/$2
    vol2=$PROJECT/$3
    proj=$PROJECT
    tenant=$TENANT
    expname=$1
    exp=$proj/$expname
    
    cluster1=${proj}Cluster1
    cluster2=${proj}Cluster2

    c=1
    h=1
    hostname=$hostbase$tenant$c$h
    nwwn1=`nwwn $i$j`
    k=`k $c $h`
    pwwn11=`pwwn A$k`
    pwwn12=`pwwn B$k`
    pwwn13=`pwwn C$k`
    pwwn14=`pwwn D$k`
    
    c=2
    h=2
    hostname2=$hostbase$tenant$c$h
    nwwn2=`nwwn $i$j`
    k=`k $c $h`
    pwwn21=`pwwn A$k`
    pwwn22=`pwwn B$k`
    pwwn23=`pwwn C$k`
    pwwn24=`pwwn D$k`

    run export_group create --type Cluster $proj $expname $NH --volspec "$vol1+1" --clusters "$cluster1"
    run export_group show $exp
    run export_group update $exp --addVolspec "$vol2+1" --remVols $vol1
    run export_group show $exp
    run export_group update $exp --addClusters "$cluster2" --remClusters "$cluster1"
    run export_group show $exp
    run export_group update $exp --remHosts "$hostname2"
    run export_group show $exp
    run export_group update $exp --addHosts "$hostname2"
    run export_group show $exp
    run export_group update $exp --remInits "$hostname2/pwwn21,$hostname2/pwwn23"
    run export_group show $exp
    run export_group update $exp --remVols $vol2
    run export_group show $exp

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi exportgroups $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group delete $exp
    run export_group show $exp
}

# =======================================================================================
# - create empty export group
# - add initiators, try to add duplicate initiators
# - add/remove volumes with initiator list non-empty
# =======================================================================================
export_test_2()
{
    expname=$1
    host=$2
    vol1=$PROJECT/$3
    vol2=$PROJECT/$4

    proj=$PROJECT
    exp=$proj/$expname
    NWWN=$BLK_CLIENT_FC_NODE
    PWWN1=`pwwn A3`
    PWWN2=`pwwn A4`

    run export_group create $proj $expname $NH
    run export_group show $exp

    run export_group add_initiator $exp "FC+$NWWN+$PWWN1+$host"
    run export_group add_initiator $exp "FC+$NWWN+$PWWN2+$host"
    run export_group show $exp

    run export_group add_volume $exp "$vol1+1"
    run export_group show $exp

    run export_group add_volume $exp "$vol2+2"
    run export_group show $exp

    run export_group remove_volume $exp "$vol1"
    run export_group show $exp

    run export_group remove_volume $exp "$vol2"
    run export_group show $exp

    run export_group add_volume $exp "$vol2+2"
    run export_group show $exp

    run export_group delete $exp
    run export_group show $exp
}

#
# create block export
#
export_test_3()
{
    expname=$1
    host=$2
    vol1=$PROJECT/$3
    vol2=$PROJECT/$4

    proj=$PROJECT
    exp=$proj/$expname
    NWWN=$BLK_CLIENT_FC_NODE
    PWWN1=`pwwn A1`
    PWWN2=`pwwn A2`

    run export_group create $proj $expname $NH --volspec "$vol1+1,$vol2+2" --initspec "FC+$NWWN+$PWWN1+$host,FC+$NWWN+$PWWN2+$host,iSCSI++$BLK_CLIENT_iSCSI+$host"
    run export_group show $exp

    run export_group remove_volume $exp $vol2
    run export_group show $exp

    run export_group remove_initiator $exp "FC+$PWWN1"
    run export_group show $exp

    run export_group delete $exp
    run export_group show $exp

    expname=${expname}2
    exp=$proj/$expname
    run export_group create $proj $expname $NH --volspec "$vol1,$vol2" --initspec "FC+$NWWN+$PWWN2+$host,iSCSI++$BLK_CLIENT_iSCSI+$host"
    run export_group remove_volume $exp $vol2
    run export_group show $exp

    run export_group add_initiator $exp "FC+$NWWN+$PWWN1+$host"
    run export_group delete $exp
    run export_group show $exp
}

#
# Test blocksnapshot and volume exports
#
export_test_4()
{
    expname=$1
    host=$2
    vol1=$PROJECT/$3
    vol2=$PROJECT/$4
    snap1_label=snap1-${HOSTNAME}-${RANDOM}
    snap1="${vol1}/${snap1_label}"

    proj=$PROJECT
    exp=$proj/$expname
    NWWN=$BLK_CLIENT_FC_NODE
    PWWN1=`pwwn A1`
    PWWN2=`pwwn A2`
    
    
    run blocksnapshot create $vol1 $snap1_label 

    if [ "$BOURNE_SECURITY_DISABLED" != '1' ] ; then
        run bulkapi blocksnapshots $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD $LOCAL_LDAP_USER_PASSWORD_1
    fi

    run export_group create $proj $expname $NH --volspec "$vol1,$vol2,$snap1" --initspec "FC+$NWWN+$PWWN2+$host"
    run export_group show $exp
    run volume exports $vol2
    run export_group remove_volume $exp $vol2
    run volume exports $vol2
    run export_group show $exp
    run blocksnapshot exports $snap1
    run export_group remove_volume $exp $snap1
    run blocksnapshot exports $snap1
    run export_group add_volume $exp $snap1
    run blocksnapshot exports $snap1

    run export_group add_initiator $exp "FC+$NWWN+$PWWN1+$host"
    run export_group delete $exp
    run export_group show $exp

    run blocksnapshot delete $snap1
}

syssvc_tests()
{
    syssvc $CONFIG_FILE "$BOURNE_IP"
}

security_tests()
{
    if [ "$AUTH" = 'local' ] ; then
        echo 'no security tests for local security'
        return
    fi

    # done in setupe: security login $SYSADMIN $SYSADMIN_PASSWORD
    security test_firewall
    security test_proxy_token
    security test_formlogin $SYSADMIN $SYSADMIN_PASSWORD
    security test_vulnerability $SYSADMIN $SYSADMIN_PASSWORD
    security login $SYSADMIN $SYSADMIN_PASSWORD
    security test_logout $SYSADMIN
    security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    
    security update_authn_provider $LOCAL_LDAP_AUTHN_PROVIDER_NEWNAME
    balance run security add_tenant_role subject_id $LOCAL_LDAP_TENANTADMIN_USERNAME TENANT_ADMIN
    balance run security add_tenant_role subject_id $LOCAL_LDAP_TENANTADMIN_USERNAME TENANT_APPROVER
    balance run security login $LOCAL_LDAP_TENANTADMIN_USERNAME $LOCAL_LDAP_TENANTADMIN_PASSWORD
    balance run security add_tenant_role group $LOCAL_LDAP_TENANT_PROJECT_ADMINS_GROUP PROJECT_ADMIN
    balance run security login $LOCAL_LDAP_PROJECT_ADMIN_USERNAME $LOCAL_LDAP_PROJECT_ADMIN_PASSWORD
    balance run project create $PROJECT.securitytest
    balance run security login $LOCAL_LDAP_MAXGROUPSUSER_USERNAME $LOCAL_LDAP_MAXGROUPSUSER_PASSWORD
    for(( i=0; i<${#BOURNE_IP_ARRAY[@]};i++ ));
    do
        run security verify_user $LOCAL_LDAP_MAXGROUPSUSER_USERNAME --ip=${BOURNE_IP_ARRAY[$i]}
    done

    #run security logout
    #Test the ldaps provider
    balance run security login $SYSADMIN $SYSADMIN_PASSWORD
    balance security add_authn_provider $LOCAL_LDAP_AUTHN_MODE $LOCAL_SECURE_LDAP_AUTHN_URLS $LOCAL_SECURE_LDAP_AUTHN_MANAGER_DN $LOCAL_SECURE_LDAP_AUTHN_MANAGER_PWD $LOCAL_SECURE_LDAP_AUTHN_SEARCH_BASE $LOCAL_LDAP_AUTHN_SEARCH_FILTER $LOCAL_LDAP_AUTHN_AUTHN_GROUP_ATTR "$LOCAL_SECURE_LDAP_AUTHN_NAME" $LOCAL_SECURE_LDAP_AUTHN_DOMAINS "$LOCAL_SECURE_LDAP_AUTHN_WHITELIST" $LOCAL_LDAP_AUTHN_SEARCH_SCOPE --group_object_classes "$LOCAL_LDAP_AUTHN_GROUP_OBJECT_CLASSES" --group_member_attributes "$LOCAL_LDAP_AUTHN_GROUP_MEMBER_ATTRIBUTES"
    balance tenant add_attribute $LOCAL_SECURE_LDAP_AUTHN_DOMAINS $LOCAL_SECURE_LDAP_TENANT_ATTRIBUTE_KEY $LOCAL_SECURE_LDAP_TENANT_ATTRIBUTE_VALUE
    for(( i=0; i<${#BOURNE_IP_ARRAY[@]};i++ ));
    do
        run security login $LOCAL_SECURE_LDAP_USER_USERNAME $LOCAL_SECURE_LDAP_USER_PASSWORD --ip=${BOURNE_IP_ARRAY[$i]} 
    done
    
    #Test the LDAP provider
    balance run security login $SYSADMIN $SYSADMIN_PASSWORD
    balance tenant add_attribute $LOCAL_LDAP_AUTHN_DOMAINS $LOCAL_LDAP_TENANT_ATTRIBUTE_KEY $LOCAL_LDAP_TENANT_ATTRIBUTE_ROOT_SUBTENANT1_VALUE
        
    balance run security login $SYSADMIN $SYSADMIN_PASSWORD
    for(( i=0; i<${#BOURNE_IP_ARRAY[@]};i++ ));
    do
        run security verify_user $SYSADMIN --ip=${BOURNE_IP_ARRAY[$i]}
    done
    
    balance run security add_tenant_role subject_id $LOCAL_LDAP_USER_USERNAME_1 TENANT_ADMIN
    balance run security add_tenant_role subject_id $LOCAL_LDAP_USER_USERNAME_2 PROJECT_ADMIN
    
    # end of proxy token test
    
    # Leave the security test suite logged in as the super user
    balance tenant add_attribute $LOCAL_LDAP_AUTHN_DOMAINS $LOCAL_LDAP_TENANT_ATTRIBUTE_KEY $LOCAL_LDAP_TENANT_ATTRIBUTE_ROOT_TENANT_VALUE
    balance run security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD 
    security test_tenant_access_permissions $LOCAL_SECURE_LDAP_USER_USERNAME $LOCAL_SECURE_LDAP_USER_PASSWORD
    
    # test the domain name with spaces at the beginning and at the end CQ 603992
    # Tenant update with Whitespace before/after domain name and Group Name returns 400 before this change
    security login $SYSADMIN $SYSADMIN_PASSWORD
    security test_tenant_domain_update $SYSADMIN $SYSADMIN_PASSWORD $(toLower ${LOCAL_SECURE_LDAP_AUTHN_DOMAINS}) $LOCAL_SECURE_LDAP_TENANT_ATTRIBUTE_KEY $LOCAL_SECURE_LDAP_TENANT_ATTRIBUTE_VALUE
    
    # test adding and removing a group with spaces before and after the group's name CQ 603992
    tenant add_group $LOCAL_LDAP_AUTHN_DOMAINS "$LOCAL_LDAP_VIPR_USER_GROUP"
    tenant remove_group $LOCAL_LDAP_AUTHN_DOMAINS "$LOCAL_LDAP_VIPR_USER_GROUP"
    
    # test adding role with subject ID with spaces at the beginning and at the end and removing it for roles
    security add_tenant_role subject_id $LOCAL_SECURE_LDAP_USER_USERNAME_WITH_SPACES TENANT_ADMIN
    security remove_tenant_role subject_id $LOCAL_SECURE_LDAP_USER_USERNAME_WITH_SPACES TENANT_ADMIN
    
    # test the sequence: create a subtenant (TENANT_ID), get the URI of that tenant, 
    # deactivate the tenant and then try to create that tenant again. In the reply we should have the tenant ID like:
    # duplicated in another tenant (urn:storageos:TenantOrg:TENANT_ID)
    # The argument "true" means that we expect the tenant ID be reported in the error
    security test_tenant_duplicate_message $LOCAL_LDAP_AUTHN_DOMAINS "TEST_SUBTENANT_$$" "true"
    
    # The argument "false" means that we do not expect the tenant ID to be reported in the error
    security test_tenant_duplicate_message $LOCAL_LDAP_AUTHN_DOMAINS "TEST_SUBTENANT_$$" "false"
    
    # test that the password change with the same value returns the error 400
    security login $SYSADMIN $SYSADMIN_PASSWORD
    security test_password_change $SVCUSER $SYSADMIN_PASSWORD

    #Test the ad provider
	if [ "$TEST_AD_PROVIDER" = 'yes' ] ; then
		balance run security login $SYSADMIN $SYSADMIN_PASSWORD
		balance security add_authn_provider $AD_AUTHN_MODE $AD_AUTHN_URLS $AD_AUTHN_MANAGER_DN $AD_AUTHN_MANAGER_PWD $AD_AUTHN_SEARCH_BASE $AD_AUTHN_SEARCH_FILTER $AD_AUTHN_AUTHN_GROUP_ATTR "$AD_AUTHN_NAME" $AD_AUTHN_DOMAINS "$AD_AUTHN_WHITELIST" $AD_AUTHN_SEARCH_SCOPE 
		balance tenant add_attribute $AD_AUTHN_DOMAINS $AD_TENANT_ATTRIBUTE_KEY $AD_TENANT_ATTRIBUTE_VALUE
		for(( i=0; i<${#BOURNE_IP_ARRAY[@]};i++ ));
		do
			run security login $AD_USER_USERNAME $AD_USER_PASSWORD --ip=${BOURNE_IP_ARRAY[$i]} 
		done
	fi

	# verify java support ssl3 and don't have keysize restriction
	verify_java_security_file $BOURNE_IPADDR
}

verify_java_security_file() {
    VIP=$1
    OPT=""
    if [[ $VIP == \[* ]];
    then
        tmp=${VIP#*[}
        VIP=${tmp%]*}
        OPT="-6"
    fi
    echo "VIP=${VIP} OPT=${OPT}"

    # make sure line jdk.tls.disabledAlgorithms=SSLv3 was comented out
    cmd="find /usr/lib64 -name java.security -exec grep jdk.tls.disabledAlgorithms {} \; | grep SSLv3 | grep -v '#' > /dev/null || echo ok"
    echo "CMD=${cmd}"
    result=`SSH ${VIP} "$cmd" ${OPT}`
    if [[ $result != "ok" ]];
    then
        echo "***"
        echo "*** FAILED! vipr java.security disabled ssl3"
        echo "***"
        exit 1
    fi

    # make sure line jdk.certpath.disabledAlgorithms don't contains 'keySize' restriction
    cmd="find /usr/lib64 -name java.security -exec grep jdk.certpath.disabledAlgorithms {} \; | grep keySize | grep -v '#' > /dev/null || echo ok"
    echo "CMD=${cmd}"
    result=`SSH ${VIP} "$cmd" ${OPT}`
    if [[ $result != "ok" ]];
    then
        echo "***"
        echo "*** FAILED! vipr java.security contains keySize restriction"
        echo "***"
        exit 1
    fi
}

##### full_copy tests 
#######################

VMAX3_FC_SMIS_DEV=VMAX3_FC_SMIS_DEV
VMAX3_FC_PORTS_A="FA-1D FA-3D"
VMAX3_FC_PORTS_B="FA-2D FA-4D"
COS_VMAX3BLOCK_FC=COS_VMAX3BLOCK_FC

vmax3_fc_setup_once()
{
    secho "VMAX3 FC Setup"

    smisprovider show $VMAX3_FC_SMIS_DEV &> /dev/null && return $?

    run smisprovider create $VMAX3_FC_SMIS_DEV $VMAX3_FC_SMIS_IP $VMAX3_FC_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX3_FC_SMIS_SSL

    run storagedevice discover_all --ignore_error
    secho "Waiting 5 minutes after storage discovery"
    sleep 300

    run storagepool update $VMAX3_FC_NATIVEGUID --type block --volume_type THIN_ONLY
    run storagepool update $VMAX3_FC_NATIVEGUID --type block --volume_type THICK_ONLY
    run storagepool update $VMAX3_FC_NATIVEGUID --nhadd $NH --type block

    if [ $QUICK -eq 0 ]; then     
        echo "vmax3 fc storageports update"
        if [ $DISCOVER_SAN -eq 0 ]; then
           for porta in ${VMAX3_FC_PORTS_A}
           do
	      run storageport update $VMAX3_FC_NATIVEGUID FC --tzone $FCTZ_A --group ${porta}
           done
	    for portb in ${VMAX3_FC_PORTS_B}
           do
              run storageport update $VMAX3_FC_NATIVEGUID FC --tzone $FCTZ_B --group ${portb}
           done
       fi
       run storageport update $VMAX3_FC_NATIVEGUID IP --tzone nh/iptz
    fi

    run cos create block $COS_VMAX3BLOCK_FC false \
			 --description 'Virtual-Pool for VMAX3 block FC' \
                      --protocols FC 			\
                      --numpaths 2 \
                      --max_snapshots 10 \
	               --system_type vmax \
                      --provisionType 'Thin' \
			 --neighborhoods $NH


    run cos update block $COS_VMAX3BLOCK_FC --storage $VMAX3_FC_NATIVEGUID
    run cos allow $COS_VMAX3BLOCK_FC block $TENANT
}

vmax_fc_setup()
{
    secho "VMAX FC Setup"
    vmaxblock_setup
    # This vpool is used for the standard vmax tests and we
    # want to make sure the vmax3 is not used for those tests,
    # so we make sure to use only assigned pools, which are
    # those for the standard vmax array.
    run cos update block $COS_VMAXBLOCK_FC --use_matched false
    # vmax3_fc_setup_once
}

full_copy_setup()
{
    secho "Tenant is $TENANT"
    secho "Project is $PROJECT"
    vmax_fc_setup
    vnxblock_setup
}

full_copy_single_volume_vnx()
{
    echo "Finished vnx full-copy"

    full_copy_source="${FULL_COPY_VOLUME}-source${RANDOM}"
    full_copy_clone="${FULL_COPY_VOLUME}-clone${RANDOM}"

    echo "Creating source volume for VNX"
    run volume create ${full_copy_source} $PROJECT $NH $COS_VNXBLOCK_FC 1073741825 --thinVolume true

    echo "Creating 1 full copy"
    # TODO sanity_utils does not yet handle the count arg.  If you change this, perform manual cleanup of +1 copies
    run volume full_copy ${full_copy_clone} $PROJECT/${full_copy_source} --count=1

    echo "Listing all full copies for source volume"
    run volume full_copy_list $PROJECT/${full_copy_source}

    echo "Resynchronizing full copy from source"
    run volume full_copy_resync $PROJECT/${full_copy_clone}

    echo "Restoring source from full copy"
    run volume full_copy_restore $PROJECT/${full_copy_clone}

    echo "Detaching full copy..."
    run volume detach ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Deactivating full copy volume"
    run volume delete $PROJECT/${full_copy_clone} --wait

    echo "Deactivating source volume"
    run volume delete $PROJECT/${full_copy_source} --wait
    
    echo "Finished vnx full-copy"
}

full_copy_single_volume_vmax()
{
    echo "Started vmax full-copy"

    full_copy_source="${FULL_COPY_VOLUME}-source${RANDOM}"
    full_copy_clone="${FULL_COPY_VOLUME}-clone${RANDOM}"

    echo "Creating source volume"
    run volume create ${full_copy_source} $PROJECT $NH $COS_VMAXBLOCK_FC 1073741825 --thinVolume true

    echo "Creating 1 full copy"
    # TODO sanity_utils does not yet handle the count arg.  If you change this, perform manual cleanup of +1 copies
    run volume full_copy ${full_copy_clone} $PROJECT/${full_copy_source} --count=1

    echo "Listing all full copies for source volume"
    run volume full_copy_list $PROJECT/${full_copy_source}

    echo "Resynchronizing full copy from source"
    run volume full_copy_resync $PROJECT/${full_copy_clone}

    echo "Restoring source from full copy"
    run volume full_copy_restore $PROJECT/${full_copy_clone}

    echo "Detaching full copy..."
    run volume detach ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Deactivating full copy volume"
    run volume delete $PROJECT/${full_copy_clone} --wait

    echo "Deactivating source volume"
    run volume delete $PROJECT/${full_copy_source} --wait

    echo "Finished vmax full-copy"
}

full_copy_single_volume_vmax_inactive()
{
    echo "Started vmax inactive full-copy"

    full_copy_source="${FULL_COPY_VOLUME}-source${RANDOM}"
    full_copy_clone="${FULL_COPY_VOLUME}-clone${RANDOM}"

    echo "Creating source volume"
    run volume create ${full_copy_source} $PROJECT $NH $COS_VMAXBLOCK_FC 1073741825 --thinVolume true

    echo "Creating 1 full copy"
    # TODO sanity_utils does not yet handle the count arg.  If you change this, perform manual cleanup of +1 copies
    run volume full_copy ${full_copy_clone} $PROJECT/${full_copy_source} --count=1 --create_inactive true

    echo "Checking synchronization progress"
    run volume full_copy_check_progress ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Activating full copy..."
    sleep 10
    run volume activate ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Checking synchronization progress"
    run volume full_copy_check_progress ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}
    echo "Checking synchronization progress"
    run volume full_copy_check_progress ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}
    echo "Checking synchronization progress"
    run volume full_copy_check_progress ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Detaching full copy..."
    sleep 10
    run volume detach ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Listing all full copies for source volume"
    run volume full_copy_list $PROJECT/${full_copy_source}

    echo "Deactivating source volume"
    run volume delete $PROJECT/${full_copy_source} --wait

    echo "Deactivating full copy volume"
    run volume delete $PROJECT/${full_copy_clone} --wait

    echo "Finished vmax inactive full-copy"
}

full_copy_single_volume_vmax3()
{
    echo "Started vmax3 full-copy"

    full_copy_source="${FULL_COPY_VOLUME}-source${RANDOM}"
    full_copy_clone="${FULL_COPY_VOLUME}-clone${RANDOM}"

    echo "Creating source volume"
    run volume create ${full_copy_source} $PROJECT $NH $COS_VMAX3BLOCK_FC 1073741825 --thinVolume true

    echo "Creating 1 full copy"
    # TODO sanity_utils does not yet handle the count arg.  If you change this, perform manual cleanup of +1 copies
    run volume full_copy ${full_copy_clone} $PROJECT/${full_copy_source} --count=1

    echo "Listing all full copies for source volume"
    run volume full_copy_list $PROJECT/${full_copy_source}

    echo "Resynchronizing full copy from source"
    run volume full_copy_resync $PROJECT/${full_copy_clone}

    echo "Restoring source from full copy"
    run volume full_copy_restore $PROJECT/${full_copy_clone}

    echo "Detaching full copy..."
    run volume detach ${PROJECT}/${full_copy_source} ${PROJECT}/${full_copy_clone}

    echo "Deactivating full copy volume"
    run volume delete $PROJECT/${full_copy_clone} --wait

    echo "Deactivating source volume"
    run volume delete $PROJECT/${full_copy_source} --wait

    echo "Finished vmax3 full-copy"
}

full_copy_tests()
{
    echo "Started full-copy tests"
    full_copy_single_volume_vmax_inactive
    full_copy_single_volume_vmax
    #full_copy_single_volume_vmax3
    full_copy_single_volume_vnx
    echo "Finished full-copy tests"
}

##### end of full_copy tests

##### blockmirror tests 
#######################


blockmirror_setup()
{
    secho "Tenant is $TENANT"
    secho "Project is: $PROJECT"
    mirrorblock_setup
}

blockmirror_single_mirror()
{
    mirrortest_vol="${MIRROR_VOLUME}-single"

    echo "Creating source volume"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR 1073741825 --thinVolume true

    echo "Attaching a single mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 1 

    echo "Listing active mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}

    echo "Deactivating source volume"
    run volume delete $PROJECT/${mirrortest_vol} --wait
}

blockmirror_attach_mirror_with_optional()
{
    mirrortest_vol="${MIRROR_VOLUME}-mirrorcos"

    echo "Creating source volume"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR_WITH_OPTIONAL 1073741825 --thinVolume true

    echo "Attaching a mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 1 

    echo "Listing active mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}
    
    echo "Deactivating source volume"
    run volume delete $PROJECT/${mirrortest_vol} --wait
}

blockmirror_attach_2mirrors()
{
    mirrortest_vol="${MIRROR_VOLUME}-test-attach2"

    echo "Creating source volume"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR_WITH_2_MIRRORS 1073741825 --thinVolume true

    echo "Attaching two mirrors"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 2 

    echo "Listing active mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}
    
    echo "Deactivating source volume and all mirrors"
    run volume delete $PROJECT/${mirrortest_vol} --wait
}

blockmirror_detach_mirror_all()
{
    mirrortest_vol="${MIRROR_VOLUME}-test-detach"

    echo "Creating source volume"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR_WITH_2_MIRRORS 1073741825 --thinVolume true

    echo "Listing active mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}

    echo "Attaching first mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 1 

    echo "Attaching second mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "bar" 1 

    echo "Listing active mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}

    echo "Detaching all mirrors"
    run blockmirror detach $PROJECT/${mirrortest_vol}
    
    echo "Deactivating source volume and all mirrors"
    run volume delete $PROJECT/${mirrortest_vol} --wait
}

blockmirror_pause_resume_all()
{
    mirrortest_vol="${MIRROR_VOLUME}-test-pause-resume-all"

    echo "Creating source volume"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR 1073741825 --thinVolume true 

    echo "Attaching first mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 1 

    echo "Pausing all mirrors"
    run blockmirror pause $PROJECT/${mirrortest_vol}

    echo "Listing paused mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}

    echo "Resuming all mirrors"
    run blockmirror resume $PROJECT/${mirrortest_vol}

    echo "Listing resumed mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}

    echo "Deactivating source volume and all mirrors"
    run volume delete $PROJECT/${mirrortest_vol} --wait
}

blockmirror_vpool_change()
{
    mirrortest_vol="${MIRROR_VOLUME}-test-vpool-change"

    echo "Creating source volume with no mirrors explicitly"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR_BEFORE_CHANGE 1073741825 --thinVolume true 

    echo "Change virtual pool to one that has 1 explicit maximum mirror"
    run volume change_cos $PROJECT/${mirrortest_vol} $COS_MIRROR_AFTER_CHANGE

    echo "Attaching a mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 1 

    echo "Listing mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}

    echo "Deactivating source volume and all mirrors"
    run volume delete $PROJECT/${mirrortest_vol} --wait
}

blockmirror_vnx()
{
    mirrortest_vol="${MIRROR_VOLUME_VNX}-test-vnx"

    echo "Creating source volume on VNX"
    run volume create ${mirrortest_vol} $PROJECT $NH $COS_MIRROR_VNX 1073741825 --thinVolume true 

    echo "Attaching a mirror"
    run blockmirror attach $PROJECT/${mirrortest_vol} "foo" 1 

    echo "Pausing all mirrors"
    run blockmirror pause $PROJECT/${mirrortest_vol}

    echo "Listing paused mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}

    echo "Resuming all mirrors"
    run blockmirror resume $PROJECT/${mirrortest_vol}

    echo "Listing resumed mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}

    echo "Deactivating source volume and all mirrors"
    run volume delete $PROJECT/${mirrortest_vol} --wait
}

blockmirror_group_mirrors()
{
    mirrortest_vol="${MIRROR_VOLUME}-group"
    consistency_group=`openssl passwd "$RANDOM" | cut -c1-8`
    group_mirror="sanitytest"
    run blockconsistencygroup create $PROJECT $consistency_group

    # Create source volumes
    echo "Creating source volumes"
    run volume create ${mirrortest_vol}1 $PROJECT $NH $COS_VMAX_CG_MIRROR 1073741825 --thinVolume true --consistencyGroup $consistency_group
    run volume create ${mirrortest_vol}2 $PROJECT $NH $COS_VMAX_CG_MIRROR 1073741825 --thinVolume true --consistencyGroup $consistency_group

    run blockconsistencygroup show $consistency_group
    run volume list $PROJECT

    echo "Attaching group mirrors"
    run blockmirror attach $PROJECT/${mirrortest_vol}1 $group_mirror 1

    echo "Listing active mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}1
    run blockmirror list $PROJECT/${mirrortest_vol}2

    echo "Pausing group mirrors"
    run blockmirror pause $PROJECT/${mirrortest_vol}1

    echo "Listing group mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}1
    run blockmirror list $PROJECT/${mirrortest_vol}2

    echo "Resuming group mirrors"
    run blockmirror resume $PROJECT/${mirrortest_vol}1

    echo "Listing group mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}1
    run blockmirror list $PROJECT/${mirrortest_vol}2

    echo "Deactivating group mirrors"
    run blockmirror deactivate $PROJECT/${mirrortest_vol}1 ${mirrortest_vol}1-${group_mirror}

    echo "Listing group mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}1
    run blockmirror list $PROJECT/${mirrortest_vol}2

    echo "Attaching group mirrors"
    run blockmirror attach $PROJECT/${mirrortest_vol}1 ${group_mirror}1 1

    echo "Listing group mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}1
    run blockmirror list $PROJECT/${mirrortest_vol}2

    echo "Detaching group mirrors"
    run blockmirror detach $PROJECT/${mirrortest_vol}1

    echo "Listing group mirrors"
    run blockmirror list $PROJECT/${mirrortest_vol}1
    run blockmirror list $PROJECT/${mirrortest_vol}2

    echo "Deleting promoted volumes"
    run volume delete $PROJECT/${mirrortest_vol}1-${group_mirror}1 --wait
    run volume delete $PROJECT/${mirrortest_vol}2-${group_mirror}1 --wait

    echo "Deactivating source volumes"
    run volume delete $PROJECT/${mirrortest_vol}1 --wait
    echo "delete $PROJECT/${mirrortest_vol}2"
    run volume delete $PROJECT/${mirrortest_vol}2 --wait
}

blockmirror_tests()
{
    # quick_file_tests $COS_VNXFILE default
    # quick_block_tests $VMAXEXPORT_GROUP $VMAXEXPORT_GROUP_HOST $VMAX_VOLUME $COS_VMAXBLOCK
    # quick_block_tests $VNXEXPORT_GROUP $VNXEXPORT_GROUP_HOST $VNX_VOLUME $COS_VNXBLOCK
    blockmirror_single_mirror
    blockmirror_attach_mirror_with_optional
    blockmirror_attach_2mirrors
    blockmirror_detach_mirror_all
    blockmirror_pause_resume_all
    blockmirror_vpool_change

    #blockmirror_vnx
    blockmirror_group_mirrors
}

#### end of blockmirror test section

##### errorhandling tests 
#################################

errorhandling_setup()
{
    masa=`date +%s | cut -c5-10`
    mainvalue=value"$masa"
    mainkey=key"$masa"
    MAINERRORHANDLING=mainerrorhandling"$masa"
    PROJECTERRORHANDLING=prjcterrorhandling"$masa"
}

errorhandling_tests()
{
    ## CoS ErrorHandling tests
    cos errorhandling file $COS_VNXFILE 				\
	--description 'Virtual-Pool-for-VNX-file' false 	\
                         --protocols NFS CIFS --provisionType 'Thin'
}

#### end of errorhandling test section

##### blockconsistencygroup tests
#################################

blockconsistencygroup_setup()
{
    # Run setup
    init_setup

    # Create MultiVolumeConsistency CoS
    consistencygroup_block_cos_setup

    # Create a Second Project
    date=`date +%s | cut -c5-10`
    PROJECT_GROUP_OTHER=cgProject"$date"
    project create $PROJECT_GROUP_OTHER --tenant $TENANT

    # Create Consistency Group
    blockconsistencygroup_create_test

    # Print environment
    secho "Tenant is $TENANT"
    secho "Project is $PROJECT"
    secho "Project Other is $PROJECT_GROUP_OTHER"
    secho "SMIS IP is $VNX_SMIS_IP"
    secho "VNX CG CoS is $VNX_COS_GROUP"
    secho "VMAX CG CoS is $VMAX_COS_GROUP"
    secho "CoS without MultiVolumeConsistency $COS_GROUP_INVALID"
    secho "Consistency Group is $CONSISTENCY_GROUP"
    secho "Consistency Group Snapshot is $CONSISTENCY_GROUP_SNAPSHOT"
}

blockconsistencygroup_create_test()
{
    ### create blockconsistencygroup
    echo "Creating consistency group"
    run blockconsistencygroup create $PROJECT $CONSISTENCY_GROUP
}

blockconsistencygroup_show_test()
{
    echo "Getting consistency group"
    run blockconsistencygroup show $CONSISTENCY_GROUP
}

blockconsistencygroup_bulk_test()
{
    echo "Getting bulk data for consistency groups"
    run blockconsistencygroup bulk
}

blockconsistencygroup_delete_test()
{
    ### delete blockconsistencygroup
    echo "Deleting consistency group"
    run blockconsistencygroup delete $CONSISTENCY_GROUP
}

blockconsistencygroup_add_volume_test()
{
    ### Create Volume
    echo "Adding volume to consistency group"
    run volume create  volume-${CONSISTENCY_GROUP} $PROJECT $NH $VNX_COS_GROUP 1280000000 --consistencyGroup $CONSISTENCY_GROUP

    ### Check that volume is inside the group
    echo "Checking volume is part of consistency group"
    run blockconsistencygroup check_volume  $PROJECT volume-${CONSISTENCY_GROUP} $CONSISTENCY_GROUP --expected

    ### Check that consistencygroup cannot be deleted at this point
    echo "Checking consistency group cannot be deleted with active volumes"
    run blockconsistencygroup delete_with_volumes $CONSISTENCY_GROUP
}

blockconsistencygroup_add_volume_invalid_project(){
    echo "Checking Volume creation fails when the consistency group project and the volume project don't match"

    # Create Volume in a different project: this should fail
    blockconsistencygroup check_volume_error volume-${CONSISTENCY_GROUP} $PROJECT_GROUP_OTHER $NH $VNX_COS_GROUP 1280000000 'Objects should all be in the project $projid' --consistencyGroup $CONSISTENCY_GROUP --consistencyGroupProject $PROJECT
}

blockconsistencygroup_add_volume_invalid_CoS(){
    echo "Checking volume creation fails when consistency group is provided but MultiVolumeConsistency attribute in VirtualPool is false"

    # Create Volume in a different project: this should fail
    blockconsistencygroup check_volume_error volume-${CONSISTENCY_GROUP} $PROJECT $NH $COS_GROUP_INVALID 1280000000 'Consistency group $cgid was provided but multi_volume_consistency attribute in the virtual pool $cosid is false' --consistencyGroup $CONSISTENCY_GROUP
}

blockconsistencygroup_add_volume_no_consistency_group(){
    echo "Checking volume creation fails when MultiVolumeConsistency attribute in VirtualPool is true but consistency group is not provided"

    # Create Volume in a different project: this should fail
    blockconsistencygroup check_volume_error volume-${CONSISTENCY_GROUP} $PROJECT $NH $VNX_COS_GROUP 1280000000 'Required parameter consistencyGroup was missing or empty' --servicecode 1005
}

blockconsistencygroup_create_snapshot()
{
    ### create blockconsistencygroup snapshot
    echo "Creating consistency group snapshot"
    run blockconsistencygroup create_snapshot $1 $2 --createInactive 
}

blockconsistencygroup_create_active_snapshot ()
{
    ### create blockconsistencygroup snapshot
    echo "Creating consistency group snapshot"
    run blockconsistencygroup create_snapshot $1 $2

}

blockconsistencygroup_activate_snapshot()
{
    ### activate blockconsistencygroup snapshot
    echo "Activating consistency group snapshot"
    run blockconsistencygroup activate_snapshot $1 $2
}

blockconsistencygroup_deactivate_snapshot()
{
    ### deactivate blockconsistencygroup snapshot
    echo "Deactivating consistency group snapshot"
    run blockconsistencygroup deactivate_snapshot $1 $2
}

blockconsistencygroup_restore_snapshot()
{
    ### restore blockconsistencygroup snapshot
    echo "Restoring consistency group snapshot"
    run blockconsistencygroup restore_snapshot $1 $2
}

blockconsistencygroup_show_snapshot()
{
    ### show blockconsistencygroup snapshot
    echo "Showing consistency group snapshot"
    run blockconsistencygroup show_snapshot $1 $2
}

blockconsistencygroup_list_snapshot()
{
    ### list blockconsistencygroup snapshot
    echo "Listing consistency group snapshot"
    run blockconsistencygroup list_snapshots $1 $2
}

blockconsistencygroup_cleanup(){
    echo "Cleaning up after running blockconsistencygroup test"

    ### Delete volume from consistency group
    echo "Deleting volume from consistency group"
    run volume delete $PROJECT/volume-${CONSISTENCY_GROUP} --wait

    # Remove Invalid CoS
    cos delete $COS_GROUP_INVALID block

    # Delete the project
    project delete $PROJECT_GROUP_OTHER

    # Remove Consistency Group
    blockconsistencygroup_delete_test
}

blockconsistencygroup_setup_snapshot()
{
    echo "Creating setup for Consistency Group Snapshot tests"

    # Creating Consisstency Group for the snapshot
    run blockconsistencygroup create $PROJECT $1

    # Adding a Volume to the Consistency Group
    run volume create  volume-$1 $PROJECT $NH $2 1280000000 --consistencyGroup $1
}

blockconsistencygroup_snapshot_tests()
{    
    cg_name=$1
    cg_cos=$2
    cg_snapshot=$3
	
	
    
    echo "Running Consistency Group Snapshot tests"
    echo "Consistency Group: $cg_name"
    echo "CoS: $cg_cos"
    echo "CG Snapshot: $cg_snapshot" 
    
    blockconsistencygroup_setup_snapshot $cg_name $cg_cos
	blockconsistencygroup_create_active_snapshot $cg_name $cg_snapshot
	blockconsistencygroup_restore_snapshot $cg_name $cg_snapshot
    blockconsistencygroup_show_snapshot $cg_name $cg_snapshot
    blockconsistencygroup_list_snapshot $cg_name $cg_snapshot
    blockconsistencygroup_deactivate_snapshot $cg_name $cg_snapshot   
}

blockconsistencygroup_update_tests()
{
    cg1=$1
    cg1_vpool=$2

    cg2=$3
    cg2_vpool=$4

    run volume create vnx-vol $PROJECT $NH cosvnxb 1GB
    run volume create vmax-vol $PROJECT $NH cosvmaxb 1GB

    blockconsistencygroup create $PROJECT $cg1
    blockconsistencygroup create $PROJECT $cg2

    secho "Trying to update CG that is not on array"
    blockconsistencygroup update $cg1 --add $PROJECT/vnx-vol
    blockconsistencygroup update $cg2 --add $PROJECT/vmax-vol

    secho "Creating volumes and adding them to the CG"
    run volume create vnx-vol-cg $PROJECT $NH $cg1_vpool 1GB --count=2 --consistencyGroup=$cg1
    run volume create vmax-vol-cg $PROJECT $NH $cg2_vpool 1GB --count=2 --consistencyGroup=$cg2

    blockconsistencygroup show $cg1
    blockconsistencygroup show $cg2

    secho "Creating volumes to update"
    run volume create vnx-update $PROJECT $NH $cg1_vpool 1GB --count=3
    run volume create vmax-update $PROJECT $NH $cg2_vpool 1GB --count=3

    secho "Adding volumes to CG"
    run blockconsistencygroup update $cg1 --add $PROJECT/vnx-update-1,$PROJECT/vnx-update-2,$PROJECT/vnx-update-3
    run blockconsistencygroup update $cg2 --add $PROJECT/vmax-update-1,$PROJECT/vmax-update-2,$PROJECT/vmax-update-3

    secho "Remove volumes from CG"
    run blockconsistencygroup update $cg1 --remove $PROJECT/vnx-update-1
    run blockconsistencygroup update $cg2 --remove $PROJECT/vmax-update-1

    secho "Add and Remove volumes from CG"
    run blockconsistencygroup update $cg1 --remove $PROJECT/vnx-update-2 --add $PROJECT/vnx-update-3
    run blockconsistencygroup update $cg2 --remove $PROJECT/vmax-update-2 --add $PROJECT/vmax-update-3

    secho "Cleaning up after CG update tests"
    volume delete $PROJECT --project --wait
    run blockconsistencygroup delete $cg1
    run blockconsistencygroup delete $cg2
}

blockconsistencygroup_tests()
{
    blockconsistencygroup_show_test
    blockconsistencygroup_add_volume_test
    blockconsistencygroup_add_volume_invalid_project
    blockconsistencygroup_add_volume_invalid_CoS
#    blockconsistencygroup_add_volume_no_consistency_group
    blockconsistencygroup_snapshot_tests vnx-$CONSISTENCY_GROUP $VNX_COS_GROUP vnx-$CONSISTENCY_GROUP_SNAPSHOT 
    blockconsistencygroup_snapshot_tests vmax-$CONSISTENCY_GROUP $VMAX_COS_GROUP vmax-$CONSISTENCY_GROUP_SNAPSHOT
    blockconsistencygroup_bulk_test
    blockconsistencygroup_cleanup
    blockconsistencygroup_update_tests vnx-update $VNX_COS_GROUP vmax-update $VMAX_COS_GROUP
}

#### end of blockconsistencygroup test section

init_setup()
{
#    vnxfile_setup
    vnxblock_setup
    vmaxblock_setup
}

# ------------------------------------------------------------------------------------
# The 'init' operation is way to set up Bourne with all the configuration, but not run
# any tests. This would be useful for setting up for running manual test cases.
# ------------------------------------------------------------------------------------
init_tests()
{
    secho "Tenant is $TENANT"
    secho "Project is $PROJECT"
}

quick_setup()
{
    secho "quick setup"
#    vnxfile_setup
    VNXB_NATIVEGUID=$SIMULATOR_VNX_NATIVEGUID
    VMAX_NATIVEGUID=$SIMULATOR_VMAX_NATIVEGUID
    vnxblock_setup
#    datadomainfile_setup
    vmaxblock_setup
    errorhandling_setup

}

quick_tests()
{
#    quick_file_tests $COS_NETAPP default
#    quick_file_tests $COS_VNXFILE default
    # TODO Add isilon simulator WJEIV
    quick_block_tests $VMAXEXPORT_GROUP $VMAXEXPORT_GROUP_HOST $VMAX_VOLUME $COS_VMAXBLOCK
    quick_block_tests $VNXEXPORT_GROUP $VNXEXPORT_GROUP_HOST $VNX_VOLUME $COS_VNXBLOCK
    #quick_file_tests $COS_DDFILE default
    errorhandling_tests
}

all_tests()
{
#    ui_tests
#    webstorage_tests
    isilon_tests
#    vplex_tests
#    vnxfile_tests
#    datadomainfile_tests
    vnxblock_tests
    vmaxblock_tests
    blocksnapshot_tests
    blockmirror_tests
    blockconsistencygroup_tests
    syssvc_tests
    security_tests
    errorhandling_tests	
    ingestblock_tests
}

# query auditlogs happened in specific timeslot and return them in desired language
audit_setup()
{
    echo "Nothing to do for audit setup"
}

audit_tests()
{
    language="en_US"
    if [ -z ${EXTRA_PARAM} ]; then
        timeslot=`date -u +%Y-%m-%dT%H`
    else 
        timeslot=${EXTRA_PARAM}
    fi
    audit query $timeslot $language
}

# query monitorlogs happened in specific timeslot and return them in desired language
monitor_setup()
{
    echo "Nothing to do for monitor setup"
}

monitor_tests()
{
    language="en_US"
    if [ -z ${EXTRA_PARAM} ]; then
        timeslot=`date -u +%Y-%m-%dT%H`
    else
        timeslot=${EXTRA_PARAM}
    fi
    monitor query $timeslot $language
}

dr_setup()
{
    if [ -z "$DR_SITE_B_IP" ] ; then
        echo -e "\nFail: Usage of dr sanity test is like:\n ./sanity 255.254.253.site1 dr 255.254.253.site2\nOr multi site configuration can be tested:\n ./sanity 255.254.253.site1 dr 255.254.253.site2 255.254.253.site3"
        exit 1
    fi

    vdc_common_setup

    #setting pipefail option to pipe exit codes
    set -o pipefail

    DR_SITE_A_IP=$BOURNE_IPADDR
    security login $SYSADMIN $SYSADMIN_PASSWORD 10 600
    LOCAL_LDAP_GROUPUSER_USERNAME='ldapvipruser1@viprsanity.com'

}

# method to wait for all sites that are given as arguments to be stable
# each one wait 600 seconds and check every 10 seconds
wait_for_site_stable()
{
    local old_bourne_ipaddr=$BOURNE_IPADDR
    for ip in "$@"
    do
        echo "Waiting for $ip to be stable ..."
        BOURNE_IPADDR=$ip
        security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
        dr waitforstable --ip "${ip}" 10 600
    done
    BOURNE_IPADDR=$old_bourne_ipaddr
}

dr_tests()
{
    if [ -z "$DR_SITE_C_IP" ]; then
        dr_basic_tests
    else
        dr_multisite_tests
    fi
}

dr_basic_tests()
{
    projectid=$(project query $PROJECT)
    echo "Project id of $PROJECT is $projectid."
    
    active_site=$(dr find state ACTIVE|tail -1) || { echo "get dr active site failed."; echo $active_site; exit 1; }
    echo "active uuid is $active_site"
    
    echo "---ADD STANDBY SITE---"
    echo "Adding new standby into current vipr system"
    standby_site=$(dr add $DR_SITE_B_NAME "$DR_SITE_B_DESCRIPTION" "${DR_SITE_B_IP}" $SYSADMIN $SYSADMIN_PASSWORD|tail -1) || { echo "Add dr site failed."; echo $standby_site; exit 1; }
    echo "Adding dr done"
    echo "stanby uuid is $standby_site"    

    echo "---GET SITES---"
    dr list

    echo "---GET SITE $standby_site---"
    dr get $standby_site

    echo "---VERIFY SITE $standby_site---"
    name=$(dr get $standby_site name|tail -1)
    description=$(dr get $standby_site description|tail -1)
    vip=$(dr get $standby_site vip_endpoint|tail -1)

    if [ "$vip" != "$DR_SITE_B_IP" -a "[$vip]" != "$DR_SITE_B_IP" ] ; then
        echo -e "Fail: site vip \"$vip\" isn't equal to \"$DR_SITE_B_IP\""
        exit 1
    fi

    if [ "$name" != "$DR_SITE_B_NAME" ] ; then
        echo -e "Fail: site name \"$name\" isn't equal to \"$DR_SITE_B_NAME\""
        exit 1
    fi

    if [ "$description" != "$DR_SITE_B_DESCRIPTION" ] ; then
        echo -e "Fail: site description \"$description\" isn't equal to \"$DR_SITE_B_DESCRIPTION\""
        exit 1
    fi

    
    echo "---WAIT FOR SYNC $standby_site---"
    dr waitforstate $standby_site STANDBY_SYNCED 30 1200
    echo "Syncing dr done"

    echo "---WAIT 1m FOR $standby_site TO BOOTSTRAP---"
    sleep 60

    # login to standby with auth
    echo "---VERIFY PROJECT SYNCED $standby_site---"
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    
    echo "---WAIT FOR STABLE $standby_site---"
    dr waitforstable 30 1200

    echo "---VERIFY SITE DETAILS $standby_site---"
    state=$(dr details $standby_site clusterState|tail -1)
    latency=$(dr details $standby_site networkLatencyInMs|tail -1)

    if [ "$state" != "STABLE" ] ; then
        echo -e "Fail: site clusterState \"$state\" isn't equal to STABLE"
        exit 1
    fi

    if (( $(echo "$latency < 0" | bc -l) )) ; then
        echo -e "Fail: site latency \"$latency\" isn't calculated correctly, should be 0 or greater"
        exit 1
    fi

    # verify project
    project show $projectid

    # verify sites in standby
    echo "---VERIFY sites from standby $standby_site---"
    dr get $standby_site
    dr get $active_site
    dr checkstandby "${DR_SITE_A_IP}"

    # wait for both sites stable so able to pause standby
    wait_for_site_stable "${DR_SITE_A_IP}" "${DR_SITE_B_IP}"

    # pause standby
    echo "---PAUSE SITE $standby_site---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr pause $standby_site || { echo "pause dr standby site failed."; exit 1; }

    echo "---WAIT 30s FOR $active_site TO STABILIZE---"
    sleep 30

    echo "---WAIT FOR PAUSE $standby_site---"
    dr waitforstate $standby_site STANDBY_PAUSED 30 1200
    echo "Pausing dr done"

    echo "---CREATE A NEW PROJECT ON ACTIVE---"
    PROJECT_NEW=${PROJECT}_new
    project create ${PROJECT_NEW} --tenant $TENANT
    projectid_new=$(project query ${PROJECT_NEW})
    echo "Project id of ${PROJECT_NEW} is ${projectid_new}."

    echo "---RESUME SITE $standby_site---"
    wait_for_site_stable "${DR_SITE_A_IP}" "${DR_SITE_B_IP}"
    BOURNE_IPADDR="${DR_SITE_A_IP}"
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr resume $standby_site || { echo "resume dr standby site failed."; exit 1; }

    echo "---WAIT FOR SYNC $standby_site---"
    dr waitforstate $standby_site STANDBY_SYNCED 30 1200
    echo "Syncing dr done"

    echo "---VERIFY NEW PROJECT FROM $standby_site---"
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    echo "wait for $standby_site to become stable"
    dr waitforstable 30 1200
    project show ${projectid_new}

    echo "---PREPARING STANDBY_DEGRADED TEST---"
    syssvc $CONFIG_FILE "$DR_SITE_A_IP" set_prop system_permit_root_ssh yes
    dr waitforstable 10 600
    tune_dr_config ${DR_SITE_A_IP} degrade_standby_threshold_millis 0
    stop_vdc_services "$DR_SITE_B_IP"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    echo "---WAIT FOR $standby_site TO BECOME DEGRADED---"
    dr waitforstate $standby_site STANDBY_DEGRADED 30 1200

    echo "---PREPARING STANDBY REJOIN TEST---"
    start_vdc_services "$DR_SITE_B_IP"

    echo "---WAIT FOR RESYNC $standby_site---"
    dr waitforstate $standby_site STANDBY_SYNCED 30 1200

    # set degrade_standby_threshold_millis to default value: 15 minutes
    tune_dr_config ${DR_SITE_A_IP} degrade_standby_threshold_millis 900000

    # wait both active and standby to be stable, ready to do switchover
    echo "---WAIT BOTH ACTIVE AND STANBY TO BE STABLE, READY FOR SWITCHOVER---"
    # there's no point in updating the property on both active and standby since they are shared
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 600

    # switchover to standby
    echo "---SWITCHOVER FROM TO $active_site TO $standby_site---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitfornetworkhealth $standby_site GOOD 30 1200
    dr switchover $standby_site || { echo "swichover dr standby site failed."; exit 1; }

    echo "---WAIT ACTIVE AND STANDBY TO BE STABLE AFTER SWITCHOVER---"
    sleep 120
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 600
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 600

    # verify switchover status
    echo "---WAIT FOR SWITCHOVER FROM $active_site TO $standby_site DONE---"
    dr waitforstate $standby_site ACTIVE 10 1200 && echo "standby has been switched into active"
    dr waitforstate $active_site STANDBY_SYNCED 10 1200 && echo "active has been switched into standby"

    # NOTE: active site and standby site has been switched, site A is standby and site B is active now

    # failover test part
    echo "Stopping active site $DR_SITE_B_NAME services to prepare for failover test ..."
    stop_vdc_services "$DR_SITE_B_IP"
    echo "Sleep 1m to wait standby site (VIP: ${DR_SITE_A_IP}) ZK switch to participant"
    sleep 60
    echo "Wait for site ${DR_SITE_A_NAME} to be stable ..."
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 800
    dr waitforstable --ip "${DR_SITE_A_IP}" 10 600

    echo "sleep 2m until active site disappears"
    sleep 120

    echo "Failover to promote site $DR_SITE_A_NAME as active ..."
    dr failover --ip "${DR_SITE_A_IP}" $active_site || { echo "failover to site $DR_SITE_A_NAME failed."; exit 1; }
    echo "Sleep 1m to wait failover ..."
    sleep 60
    dr waitforstable --ip "${DR_SITE_A_IP}"  10 800

    echo "Wait for site ${DR_SITE_A_NAME} to be stable ..."
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_A_IP}" 10 600

    echo "Wait site state to become ACTIVE after failover ..."
    dr waitforstate $active_site ACTIVE 30 600 --ip "${DR_SITE_A_IP}"

    # failback test part
    echo "Starting old active site $DR_SITE_B_NAME services ..."
    start_vdc_services "$DR_SITE_B_IP"
    echo "Sleep 300s to wait old site to become ACTIVE_DEGRADED ..."
    sleep 300
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_B_IP}" 10 600
    dr waitforstate $standby_site ACTIVE_DEGRADED 30 600 --ip "${DR_SITE_B_IP}"

    # resume ACTIVE_DEGRADED site to prepare next standby-removing test
    echo "Waiting $DR_SITE_A_NAME and $DR_SITE_B_NAME to be stable ..."
    wait_for_site_stable "${DR_SITE_A_IP}" "${DR_SITE_B_IP}"
    echo "---RESUME OLD ACTIVE SITE ${DR_SITE_B_NAME} (ACTIVE_DEGRADED) AFTER FAILOVER---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr resume --ip "${DR_SITE_A_IP}" $standby_site || { echo "resume dr standby site failed."; exit 1; }
    echo "Resuming done"
    echo "Resuming site uuid is $standby_site"

    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    echo "Wait standby resuming done ..."
    dr waitforstate $standby_site STANDBY_SYNCED 30 1200

    # NOTE: after switchover and failover (then resume old active), site A is active and site B is standby (STANDBY_SYNCED state)

    # delete standby
    echo "---DELETE STANDBY $standby_site---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr delete $standby_site

    echo "---WAIT FOR REMOVED $standby_site---should throw exceptions---"
    dr waitforstate $standby_site STANDBY_REMOVING 30 600 -n|| echo "standby removed"
    echo "Removing dr done"

    # verify deleted
    echo "---VERIFY STANDBY DELETED $standby_site---should throw exceptions---"
    dr list $DR_SITE_B_NAME || echo "standby removed from site list"
    dr get $standby_site || echo "standby uuid removed from active"
    
    # verify active still works
    TESTPROJECT=dr_project_$$
    echo "---CREATE PROJECT $TESTPROJECT ON ACTIVE $active_site---"
    project show $TESTPROJECT &> /dev/null && return $?
    project create $TESTPROJECT --tenant $TENANT
    test_projectid=$(project query $TESTPROJECT)
    echo "Project id of $TESTPROJECT is $test_projectid."

    #disabling pipefail option to pipe exit codes
    set +o pipefail
}

time_start(){
    startvar=starttime_$1
    export "$startvar"=$(date +"%s")
}
time_finish(){
    endvar=endtime_$1
    startvar=starttime_$1
    declare "$endvar"=$(date +"%s")
    runtime=$(expr "${!endvar}" - "${!startvar}")
    echo "$1=${runtime}s" >> $PERFORMANCE_FILE

    #Record difference from last run if there was one
    if [ ! -z ${!1} ] ; then
        difference=$(expr "${runtime}" - "${!1%?}")
        echo "$1_diff=${difference}s" >> $PERFORMANCE_FILE
    fi
}


dr_multisite_tests()
{
    if [ -z $PERFORMANCE_FILE ] ; then
        export PERFORMANCE_FILE=dr_performance_results
    fi
    if [ -e $PERFORMANCE_FILE ] ; then
        source $PERFORMANCE_FILE
        truncate -s 0 $PERFORMANCE_FILE
    fi
    #Time DR operations if DR_RECORD_TIME
    projectid=$(project query $PROJECT)
    echo "Project id of $PROJECT is $projectid."

    active_site=$(dr find state ACTIVE|tail -1) || { echo "get dr active site failed."; echo $active_site; exit 1; }
    echo "active uuid is $active_site"

    echo "Change system property to allow root ssh login ..."
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    syssvc $CONFIG_FILE "$DR_SITE_A_IP" set_prop system_permit_root_ssh yes
    dr waitforstable 10 600
    
    echo "---ADD STANDBY SITE B---"
    time_start add_standby_b
    echo "Adding new standby into current vipr system"
    standby_site_b=$(dr add $DR_SITE_B_NAME "$DR_SITE_B_DESCRIPTION" "${DR_SITE_B_IP}" $SYSADMIN $SYSADMIN_PASSWORD|tail -1) || { echo "Add dr site failed."; echo $standby_site_b; exit 1; }
    echo "Adding dr done"
    echo "stanby uuid is $standby_site_b"    

    echo "---GET SITES---"
    dr list

    echo "---GET SITE $standby_site_b---"
    dr get $standby_site_b

    echo "---VERIFY SITE $standby_site_b---"
    name=$(dr get $standby_site_b name|tail -1)
    description=$(dr get $standby_site_b description|tail -1)
    vip=$(dr get $standby_site_b vip_endpoint|tail -1)

    if [ "$vip" != "$DR_SITE_B_IP" -a "[$vip]" != "$DR_SITE_B_IP"  ] ; then
        echo -e "Fail: site vip \"$vip\" isn't equal to \"$DR_SITE_B_IP\""
        exit 1
    fi

    if [ "$name" != "$DR_SITE_B_NAME" ] ; then
        echo -e "Fail: site name \"$name\" isn't equal to \"$DR_SITE_B_NAME\""
        exit 1
    fi

    if [ "$description" != "$DR_SITE_B_DESCRIPTION" ] ; then
        echo -e "Fail: site description \"$description\" isn't equal to \"$DR_SITE_B_DESCRIPTION\""
        exit 1
    fi

    
    echo "---WAIT FOR SYNC $standby_site_b---"
    dr waitforstate $standby_site_b STANDBY_SYNCED 30 1200
    echo "Syncing dr done"
    time_finish add_standby_b

    echo "---WAIT 1m FOR $standby_site_b TO BOOTSTRAP---"
    sleep 60

    # login to standby with auth
    echo "---LOGIN TO STANDBY WITH SYNCED AUTH $standby_site_b---"
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    echo "---WAIT FOR STABLE $standby_site_b---"
    dr waitforstable 30 1200

    echo "---VERIFY SITE DETAILS $standby_site_b---"
    state=$(dr details $standby_site_b clusterState|tail -1)
    latency=$(dr details $standby_site_b networkLatencyInMs|tail -1)

    if [ "$state" != "STABLE" ] ; then
        echo -e "Fail: site clusterState \"$state\" isn't equal to STABLE"
        exit 1
    fi

    if (( $(echo "$latency < 0" | bc -l) )) ; then
        echo -e "Fail: site latency \"$latency\" isn't calculated correctly, should be 0 or greater"
        exit 1
    fi

    # verify project
    echo "---VERIFY PROJECT SYNCED $standby_site_b---"
    project show $projectid

    # verify sites in standby
    echo "---VERIFY sites from standby $standby_site_b---"
    dr get $standby_site_b
    dr get $active_site

    dr waitforstable 10 600

    echo "---ADD STANDBY SITE C---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    time_start add_standby_c
    echo "Adding new standby into current vipr system"
    standby_site_c=$(dr add $DR_SITE_C_NAME "$DR_SITE_C_DESCRIPTION" "${DR_SITE_C_IP}" $SYSADMIN $SYSADMIN_PASSWORD|tail -1) || { echo "Add dr site c failed."; echo $standby_site_c; exit 1; }
    echo "Adding dr site c done"
    echo "stanby uuid is $standby_site_c"

    echo "---GET SITES---"
    dr list

    echo "---GET SITE $standby_site_c---"
    dr get $standby_site_c

    echo "---VERIFY SITE $standby_site_c---"
    name=$(dr get $standby_site_c name|tail -1)
    description=$(dr get $standby_site_c description|tail -1)
    vip=$(dr get $standby_site_c vip_endpoint|tail -1)

    if [ "$vip" != "$DR_SITE_C_IP" -a "[$vip]" != "$DR_SITE_C_IP" ] ; then
        echo -e "Fail: site vip \"$vip\" isn't equal to \"$DR_SITE_C_IP\""
        exit 1
    fi

    if [ "$name" != "$DR_SITE_C_NAME" ] ; then
        echo -e "Fail: site name \"$name\" isn't equal to \"$DR_SITE_C_NAME\""
        exit 1
    fi

    if [ "$description" != "$DR_SITE_C_DESCRIPTION" ] ; then
        echo -e "Fail: site description \"$description\" isn't equal to \"$DR_SITE_C_DESCRIPTION\""
        exit 1
    fi


    echo "---WAIT FOR SYNC $standby_site_c---"
    dr waitforstate $standby_site_c STANDBY_SYNCED 30 1200
    echo "Syncing dr done"
    time_finish add_standby_c

    echo "---WAIT 1m FOR $standby_site_c TO BOOTSTRAP---"
    sleep 60

    # login to standby with auth
    echo "---LOGIN TO STANDBY WITH SYNCED AUTH $standby_site_c---"
    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    echo "---WAIT FOR STABLE $standby_site_c---"
    dr waitforstable 30 1200

    echo "---VERIFY SITE DETAILS $standby_site_c---"
    state=$(dr details $standby_site_c clusterState|tail -1)
    latency=$(dr details $standby_site_c networkLatencyInMs|tail -1)

    if [ "$state" != "STABLE" ] ; then
        echo -e "Fail: site clusterState \"$state\" isn't equal to STABLE"
        exit 1
    fi

    if (( $(echo "$latency < 0" | bc -l) )) ; then
        echo -e "Fail: site latency \"$latency\" isn't calculated correctly, should be 0 or greater"
        exit 1
    fi

    # verify project
    echo "---VERIFY PROJECT SYNCED $standby_site_c---"
    project show $projectid

    # verify sites in standby
    echo "---VERIFY sites from standby $standby_site_c---"
    dr get $standby_site_b
    dr get $standby_site_c
    dr get $active_site

    # login to standby with auth
    echo "---LOGIN TO STANDBY WITH SYNCED AUTH $standby_site_b---"
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    # verify sites in standby
    echo "---VERIFY sites from standby $standby_site_b---"
    dr get $standby_site_b
    dr get $standby_site_c
    dr get $active_site

    wait_for_site_stable "${DR_SITE_A_IP}" "${DR_SITE_B_IP}" "${DR_SITE_C_IP}"

    # pause standby b
    echo "---PAUSE SITE $standby_site_b---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    time_start pause_standby_b
    dr pause $standby_site_b || { echo "pause dr standby site b failed."; exit 1; }

    echo "---WAIT 30s FOR $active_site_b TO STABILIZE---"
    sleep 30

    echo "---WAIT FOR PAUSE $standby_site_b---"
    dr waitforstate $standby_site_b STANDBY_PAUSED 30 1200
    echo "Pausing dr done"
    time_finish pause_standby_b

    wait_for_site_stable "${DR_SITE_A_IP}" "${DR_SITE_B_IP}" "${DR_SITE_C_IP}"

    # pause standby c
    echo "---PAUSE SITE $standby_site_c---"
    time_start pause_standby_c
    dr pause $standby_site_c || { echo "pause dr standby site c failed."; exit 1; }

    echo "---WAIT 30s FOR $active_site_c TO STABILIZE---"
    sleep 30

    echo "---WAIT FOR PAUSE $standby_site_c---"
    dr waitforstate $standby_site_c STANDBY_PAUSED 30 1200
    echo "Pausing dr done"
    time_finish pause_standby_c

    echo "---CREATE A NEW PROJECT ON ACTIVE---"
    PROJECT_NEW=${PROJECT}_new
    project create ${PROJECT_NEW} --tenant $TENANT
    projectid_new=$(project query ${PROJECT_NEW})
    echo "Project id of ${PROJECT_NEW} is ${projectid_new}."

    echo "---RESUME SITE $standby_site_b---"
    time_start resume_standby_b
    dr resume $standby_site_b || { echo "resume dr standby site b failed."; exit 1; }

    echo "---WAIT FOR SYNC $standby_site_b---"
    dr waitforstate $standby_site_b STANDBY_SYNCED 30 1200
    time_finish resume_standby_b
    echo "Syncing dr done"

    echo "---WAIT 30s TO STABILIZE---"
    sleep 30

    echo "---VERIFY NEW PROJECT FROM $standby_site_b---"
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    echo "wait for $standby_site_b to be stable"
    dr waitforstable 30 1200
    project show ${projectid_new}

    echo "---RESUME SITE $standby_site_c---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    time_start resume_standby_c
    dr resume $standby_site_c || { echo "resume dr standby site c failed."; exit 1; }

    echo "---WAIT FOR SYNC $standby_site_c---"
    dr waitforstate $standby_site_c STANDBY_SYNCED 30 1200
    time_finish resume_standby_c
    echo "Syncing dr done"

    echo "---WAIT 30s TO STABILIZE---"
    sleep 30

    echo "---VERIFY NEW PROJECT FROM $standby_site_c---"
    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    echo "wait for $standby_site_b to be stable"
    dr waitforstable 30 1200
    project show ${projectid_new}

    echo "---PREPARING STANDBY_DEGRADED TEST---"
    syssvc $CONFIG_FILE "$DR_SITE_A_IP" set_prop system_permit_root_ssh yes
    dr waitforstable 10 600
    tune_dr_config ${DR_SITE_A_IP} degrade_standby_threshold_millis 0
    stop_vdc_services "$DR_SITE_B_IP"
    time_start degrade_standby_b
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600

    echo "---WAIT FOR $standby_site_b TO BECOME DEGRADED---"
    dr waitforstate $standby_site_b STANDBY_DEGRADED 30 1200
    time_finish degrade_standby_b

    echo "---PREPARING STANDBY REJOIN TEST---"
    start_vdc_services "$DR_SITE_B_IP"
    time_start rejoin_standby_b

    echo "---WAIT FOR RESYNC $standby_site_b---"
    dr waitforstate $standby_site_b STANDBY_SYNCED 30 1200
    time_finish rejoin_standby_b

    # set degrade_standby_threshold_millis to default value: 15 minutes
    tune_dr_config ${DR_SITE_A_IP} degrade_standby_threshold_millis 900000

    # switchover test part
    echo "---WAIT BOTH ACTIVE AND STANBY TO BE STABLE, READY FOR SWITCHOVER---"
    dr waitforstable 10 800
    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 800

    echo "---SWITCHOVER FROM TO $active_site TO $standby_site_c---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitfornetworkhealth $standby_site_c GOOD 30 1200
    time_start switchover_standby_c
    dr switchover $standby_site_c || { echo "switchover dr standby site c failed."; exit 1; }

    echo "---WAIT ALL 3 SITES TO BE STABLE AFTER SWITCHOVER---"
    sleep 120
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 800

    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 800

    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable 10 800

    echo "---WAIT FOR SWITCHOVER FROM $active_site TO $standby_site DONE---"
    dr waitforstate $standby_site_c ACTIVE 10 1200 && echo "standby has been switched into active"
    dr waitforstate $active_site STANDBY_SYNCED 10 1200 && echo "active has been switched into standby"
    time_finish switchover_standby_c

    # NOTE: active site and standby site has been switched, site A is standby and site C is active now

    # failover test part
    echo "Stopping active site $DR_SITE_C_NAME services to prepare for failover test ..."
    stop_vdc_services "$DR_SITE_C_IP"

    echo "Sleep 1m to wait standby ZK switch to participant"
    sleep 60
    echo "Wait for stable"
    BOURNE_IPADDR=$DR_SITE_B_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_B_IP}" 10 600

    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_A_IP}" 10 600

    echo "wait for site to become STANDBY_PAUSED before failover"
    dr waitforstate $active_site STANDBY_PAUSED 30 1200

    # COP-20918
    echo "Sleep 1m before failover"
    sleep 60

    time_start failover_standby_a
    echo "Failover to promote site $DR_SITE_A_NAME as active"
    dr failover --ip "${DR_SITE_A_IP}" $active_site || { echo "failover to site $DR_SITE_A_NAME failed."; exit 1; }
    echo "Sleep 1m to wait failover"
    sleep 60

    echo "Wait for stable"
    dr waitforstable --ip "${DR_SITE_A_IP}" 10 600

    echo "Wait site state to be PRIAMRY after failover"
    dr waitforstate $active_site ACTIVE 30 600 --ip "${DR_SITE_A_IP}"
    time_finish failover_standby_a

    # failback test part
    echo "Starting old active site $DR_SITE_C_NAME services ..."
    start_vdc_services "$DR_SITE_C_IP"
    echo "Sleep 300 seconds to wait old site to become ACTIVE_DEGRADED ..."
    sleep 300
    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_C_IP}" 10 600
    dr waitforstate $standby_site_c ACTIVE_DEGRADED 30 600 --ip "${DR_SITE_C_IP}"

    # resume old active site (in ACTIVE_DEGRADED state)
    echo "Waiting $DR_SITE_A_NAME and $DR_SITE_C_NAME to be stable ..."
    wait_for_site_stable "${DR_SITE_A_IP}" "${DR_SITE_C_IP}"
    echo "---RESUME OLD ACTIVE SITE ${DR_SITE_C_NAME} (ACTIVE_DEGRADED) AFTER FAILOVER---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    time_start failback_standby_c
    dr resume --ip "${DR_SITE_A_IP}" $standby_site_c || { echo "resume dr standby site c failed."; exit 1; }
    echo "Resuming done"
    echo "Resuming site uuid is $standby_site_c"

    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    echo "Wait standby resuming done ..."
    dr waitforstate $standby_site_c STANDBY_SYNCED 30 1200
    time_finish failback_standby_c

    # delete standby
    echo "---DELETE STANDBY $standby_site_b---"
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    time_start delete_standby_b
    dr delete $standby_site_b

    echo "---WAIT FOR REMOVED $standby_site_b---should throw exceptions---"
    dr waitforstate $standby_site_b STANDBY_REMOVING 30 600 -n|| echo "standby removed"
    echo "Removing dr done"
    time_finish delete_standby_b

    # verify deleted
    echo "---VERIFY STANDBY DELETED $standby_site_b---should throw exceptions---"
    dr list $DR_SITE_B_NAME || echo "standby removed from site list"
    dr get $standby_site_b || echo "standby uuid removed from active"

    # verify active still works
    TESTPROJECT=dr_project_$$
    echo "---CREATE PROJECT $TESTPROJECT ON ACTIVE $active_site---"
    project show $TESTPROJECT &> /dev/null && return $?
    project create $TESTPROJECT --tenant $TENANT
    test_projectid=$(project query $TESTPROJECT)
    echo "Project id of $TESTPROJECT is $test_projectid."

    # verify stanby C still syncs
    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    project show $test_projectid

    # Below codes are used to test failover to ACTIVE_DEGRADED site
    # power off site A, then failover to site C, then power on site A and wait it to be ACTIVE_DEGRADED, then failover to it, and wait it to be ACTIVE
    # step1: poweroff site A, wait site C to be stable and paused
    echo "step1: poweroff site $DR_SITE_A_NAME ..."
    stop_vdc_services "$DR_SITE_A_IP"
    echo "Sleep 1m to wait standby ZK switch to participant"
    sleep 60
    echo "Wait for stable"
    BOURNE_IPADDR=$DR_SITE_C_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_C_IP}" 10 600

    echo "wait for site to become STANDBY_PAUSED before failover"
    dr waitforstate $standby_site_c STANDBY_PAUSED 30 1200

    echo "Sleep 1m before failover"
    sleep 60

    #step2: failover to site C, wait it to be stable and ACTIVE
    echo "step2: failover to site $DR_SITE_C_NAME ..."
    dr failover --ip "${DR_SITE_C_IP}" $standby_site_c || { echo "failover to site $DR_SITE_C_NAME failed."; exit 1; }
    echo "Sleep 1m to wait failover"
    sleep 60
    echo "Wait for stable"
    dr waitforstable --ip "${DR_SITE_C_IP}" 10 600
    echo "Wait site state to be PRIAMRY after failover"
    dr waitforstate $standby_site_c ACTIVE 30 600 --ip "${DR_SITE_C_IP}"

    # step3: power on site A, wait it to become ACTIVE_DEGRADED
    echo "step3: power on old active site $DR_SITE_A_NAME ..."
    start_vdc_services "$DR_SITE_A_IP"
    echo "Sleep 300 seconds to wait old site to become ACTIVE_DEGRADED ..."
    sleep 300
    BOURNE_IPADDR=$DR_SITE_A_IP
    security login $LOCAL_LDAP_GROUPUSER_USERNAME $LOCAL_LDAP_GROUPUSER_PASSWORD 10 600
    dr waitforstable --ip "${DR_SITE_A_IP}" 10 600
    dr waitforstate $active_site ACTIVE_DEGRADED 30 600 --ip "${DR_SITE_A_IP}"

    # step4: power off site C
    echo "step4: poweroff site $DR_SITE_C_NAME to prepare failover ..."
    stop_vdc_services "$DR_SITE_C_IP"
    echo "sleep 180 seconds to wait standby site recognize the broken network ..."
    sleep 180
    # step5: failover to ACTIVE_DEGRADED site and wait it to be stable and ACTIVE
    echo "step5: failover to $DR_SITE_A_NAME ..."
    dr failover --ip "${DR_SITE_A_IP}" $active_site || { echo "failover to site $DR_SITE_A_NAME failed."; exit 1; }
    echo "Sleep 1m to wait failover"
    sleep 60

    echo "Wait for stable"
    dr waitforstable --ip "${DR_SITE_A_IP}" 10 600

    echo "Wait site state to be PRIAMRY after failover"
    dr waitforstate $active_site ACTIVE 30 600 --ip "${DR_SITE_A_IP}"
    #disabling pipefail option to pipe exit codes
    set +o pipefail
}

vdc_setup()
{
	vdc_common_setup	
	vdc_federation_setup
}

vdc_common_setup()
{
	login_nd_configure_smtp_nd_add_licenses

    syssvc $CONFIG_FILE "$BOURNE_IP" set_prop system_proxyuser_encpassword ${SYSADMIN_PASSWORD}
    tenant_setup
    
    project_setup
    projectid=$(project query $PROJECT)
    echo "Project id of $PROJECT is $projectid."
    
    export BOURNE_API_SYNC_TIMEOUT=5600
}

ipsec_setup()
{
    echo "No ipsec test setup needed"
}

ipsec_tests() 
{
    echo "--- IPSEC Key Rotation ---"
    ipsec keyrotate || { echo "key rotation failed."; exit 1; }
    echo "Key Rotation done"
    assert_ipsec_status "good"

    echo "--- Test IPSEC disable ---"
    ipsec disable || { echo "ipsec disable failed."; exit 1; }
    echo "ipsec disabled"
    assert_ipsec_status "disabled"

    echo "--- Test IPSEC enable ---"
    ipsec enable || { echo "ipsec enable failed."; exit 1; }
    echo "ipsec enabled"
    assert_ipsec_status "good"
}

assert_ipsec_status()
{
    echo "Checking IPsec status"
    local expect_status="${1}"
    local ipsec_status=""
    local result=1

    sleep 1
    for i in {1..60}; do
        echo "trying the ${i} times to see if ipsec status change to ${expect_status}."
        ipsec_status=$(./ipsec check)
        [[ "${ipsec_status}" == *"${expect_status}"* ]] && { result=0; break; }
        echo "status is: ${ipsec_status}."
        echo "sleep 5 seconds"
        sleep 5
    done

    if [[ ${result} == 1 ]]; then
        echo "ipsec status is not right, expect: ${expect_status}, actually is: ${ipsec_status}"
        exit 1
    fi

    echo "IPsec status verified, which is: ${ipsec_status}"
}


sec_start_ldap_server()
{
    echo "Starting the in memory ldap server at http://${LOCAL_LDAP_SERVER_IP}:8082."
    resp_status=0
    while (($resp_status != 200))
    do
	  exec 3>&1
	  resp_status=$(curl -sw "%{http_code}" -o >(cat >&3) -k -H "Content-Type:application/json" -X POST -d '{"listener_name":"ViPRSanityLDAP"}' http://${LOCAL_LDAP_SERVER_IP}:8082/ldap-service/start)
	  if (($resp_status != 200))
	  then
	     echo "Response received : $resp_status. Retrying. Make sure ldap simulator service is started and listening at http://${LOCAL_LDAP_SERVER_IP}:8082."
	  else
	     echo "Response received : $resp_status. In memory ldap server started successfully."
	  fi
    done
}

# after this setup, vdcs will combine a federation
vdc_federation_setup()
{
	if [ "$VDC_ENDPOINT_B" == "$VDC_DEFAULT_ENDPOINT" ] ; then
        echo -e "\nFail: Usage of vdc sanity test is like:\n ./sanity 255.254.253.vdc1 vdc 255.254.253.vdc2"
        echo -e "\nOr you can test more vdcs use:\n ./sanity 255.254.253.vdc1 vdc 255.254.253.vdc2 255.254.253.vdc3"
        exit 1
    fi

    VDC_ENDPOINT_A=$BOURNE_IPADDR
    export BOURNE_IPADDR=$VDC_ENDPOINT_B
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $SYSADMIN $SYSADMIN_PASSWORD
    fi

    echo "do Login, Configure SMTP, and add controller and object licenses in $BOURNE_IPADDR"
    BOURNE_IP=$BOURNE_IPADDR
    login_nd_configure_smtp_nd_add_licenses
    syssvc $CONFIG_FILE "$BOURNE_IP" set_prop system_permit_root_ssh yes
    sleep 60
    vdc waitforstablestate 10 600
    echo "login_nd_configure_smtp_nd_add_licenses in $BOURNE_IPADDR done"

    echo "Get security key from $VDC_ENDPOINT_B"
    VDC_ENDPOINT_B_SECRETKEY=`vdc get_key|tail -1`
    echo "Key: $VDC_ENDPOINT_B_SECRETKEY"

    echo "Get cert chain from $VDC_ENDPOINT_B"
    VDC_ENDPOINT_B_CERTCHAIN=`vdc get_certchain`
    echo "Certchain: $VDC_ENDPOINT_B_CERTCHAIN"

    # if ${4} exists, will test three vdcs' CRUD operations
    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        VDC_ENDPOINT_C_NAME=vdc_name_C_$$
        VDC_ENDPOINT_C_SECRETKEY=
        VDC_ENDPOINT_C_ID=
        VDC_ENDPOINT_C_CERTCHAIN=

        export BOURNE_IPADDR=$VDC_ENDPOINT_C
        if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
            security login $SYSADMIN $SYSADMIN_PASSWORD
        fi

        echo "do Login, Configure SMTP, and add controller and object licenses in the 3rd vdc: $BOURNE_IPADDR"
        BOURNE_IP=$BOURNE_IPADDR
        login_nd_configure_smtp_nd_add_licenses
        syssvc $CONFIG_FILE "$BOURNE_IP" set_prop system_permit_root_ssh yes
        sleep 60
        vdc waitforstablestate 10 600
        echo "login_nd_configure_smtp_nd_add_licenses in the 3rd vdc: $BOURNE_IPADDR done"

        echo "Get security key from $VDC_ENDPOINT_C"
        VDC_ENDPOINT_C_SECRETKEY=`vdc get_key|tail -1`
        echo "Key: $VDC_ENDPOINT_C_SECRETKEY"

        echo "Get cert chain from $VDC_ENDPOINT_C"
        VDC_ENDPOINT_C_CERTCHAIN=`vdc get_certchain`
        echo "Certchain: $VDC_ENDPOINT_C_CERTCHAIN"
    fi

    export BOURNE_IPADDR=$VDC_ENDPOINT_A
    export BOURNE_IP=$VDC_ENDPOINT_A
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
    
    echo "Adding new vdc into current vipr system"
    vdc add $VDC_ENDPOINT_B_NAME "$VDC_ENDPOINT_B" $VDC_ENDPOINT_B_SECRETKEY "$VDC_ENDPOINT_B_CERTCHAIN"
    if [ $? -ne 0 ]; then
        echo "Add vdc B failed."
        exit 1
    fi
    sleep 30
    vdc waitforstablestate 10 600
    echo "Adding vdc done"
    
    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        echo "Adding the 3rd vdc into current vipr system"
        vdc add $VDC_ENDPOINT_C_NAME "$VDC_ENDPOINT_C" $VDC_ENDPOINT_C_SECRETKEY "$VDC_ENDPOINT_C_CERTCHAIN"
        if [ $? -ne 0 ]; then
            echo "Add vdc C failed."
            exit 1
        fi
        sleep 30
        vdc waitforstablestate 10 600
        echo "Adding the 3rd vdc done"
    fi

	echo "Test on global resource after vdc joined"
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
    project show $VDC_TEST_PROJECT &> /dev/null && return $?
    project create $VDC_TEST_PROJECT --tenant $TENANT
    echo "Project $VDC_TEST_PROJECT created on first vdc."
    echo "Login next vdc to list the created vdc"
    VDC_ENDPOINT_A=$BOURNE_IPADDR
    export BOURNE_IPADDR=$VDC_ENDPOINT_B
    security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    project search $(echo $VDC_TEST_PROJECT | head -c 2)
    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        export BOURNE_IPADDR=$VDC_ENDPOINT_C
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
        project search $(echo $VDC_TEST_PROJECT | head -c 2)
    fi
    export BOURNE_IPADDR=$VDC_ENDPOINT_A
    echo "Test on global resource done"
    
    security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    VDC_ENDPOINT_B_ID=`vdc get_id $VDC_ENDPOINT_B_NAME|tail -1`
    echo "new vdc id: $VDC_ENDPOINT_B_ID"

    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        VDC_ENDPOINT_C_ID=`vdc get_id $VDC_ENDPOINT_C_NAME|tail -1`
        echo "the 3rd vdc id: $VDC_ENDPOINT_C_ID"
    fi
}

vdc_discon_reconn_test()
{
    echo "Disconnecting the vdc in current vipr system"
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
    echo "Disconnecting with $VDC_ENDPOINT_B"

    # Stop all the services on the vdc which we want to disconnect, make it to inaccessable.
    stop_vdc_services "$VDC_ENDPOINT_B" 
    sleep 30
    vdc disconnect $VDC_ENDPOINT_B_ID
    if [ $? -ne 0 ]; then
        echo "Disconnect vdc failed."
        exit 1
    fi
    sleep 30
    vdc waitforstablestate 10 600
    echo "Disconnecting vdc done"
    
    project show $VDC_TEST_DISCONN_RECONN_PROJECT &> /dev/null && return $?
    project create $VDC_TEST_DISCONN_RECONN_PROJECT --tenant $TENANT
    echo "Project $VDC_TEST_DISCONN_RECONN_PROJECT created on remain vdc after disconnect."
    echo "Will check above project data after reconnecting vdc."
    
    echo "Reconnecting the vdc in current vipr system"
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi

    # Restart all the services on the vdc which we disconnected, make it back online to reconnect.
    start_vdc_services "$VDC_ENDPOINT_B"
    sleep 60
    echo "Waiting for vdc stable $VDC_ENDPOINT_B"
    export BOURNE_IPADDR=$VDC_ENDPOINT_B
    security login $SYSADMIN $SYSADMIN_PASSWORD
    vdc waitforstablestate 10 600

    export BOURNE_IPADDR=$VDC_ENDPOINT_A
    security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD 
    vdc reconnect $VDC_ENDPOINT_B_ID
    if [ $? -ne 0 ]; then
        echo "Resconnect vdc failed."
        exit 1
    fi
    sleep 30
    vdc waitforstablestate 10 600
    echo "Reconnecting vdc done"
    
    echo "Start to check the Project $VDC_TEST_DISCONN_RECONN_PROJECT data."

    VDC_ENDPOINT_A=$BOURNE_IPADDR
    export BOURNE_IPADDR=$VDC_ENDPOINT_B
    security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD

    echo "Test on disconnect and reconnect vdc done"
    export BOURNE_IPADDR=$VDC_ENDPOINT_A
}
	
vdc_tests()
{
    vdc_discon_reconn_test

    echo "Updating the new vdc in current vipr system"
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
    VDC_ENDPOINT_B_NAME='VDC_B_NEW_NAME'
    vdc update $VDC_ENDPOINT_B_ID $VDC_ENDPOINT_B_NAME
    if [ $? -ne 0 ]; then
        echo "Update vdc failed."
        exit 1
    fi
    sleep 30
    vdc waitforstablestate 10 600
    echo "Updating vdc done"

    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        echo "Updating the 3rd vdc in current vipr system"
        VDC_ENDPOINT_C_NAME='VDC_C_NEW_NAME'
        vdc update $VDC_ENDPOINT_C_ID $VDC_ENDPOINT_C_NAME
        sleep 30
        vdc waitforstablestate 10 600
        echo "Updating the 3rd vdc done"
    fi

    echo "Removing the new vdc"
    vdc del $VDC_ENDPOINT_B_ID
    if [ $? -ne 0 ]; then
        echo "Remove vdc failed."
        exit 1
    fi
    sleep 30
    vdc waitforstablestate 10 600
    echo "Removing vdc done"

    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        echo "Removing the 3rd vdc"
        vdc del $VDC_ENDPOINT_C_ID
        sleep 30
        vdc waitforstablestate 10 600
        echo "Removing the 3rd vdc done"
    fi

    echo "Test on resource after vdc removed"
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
    project show $VDC_TEST_REMOVE_PROJECT &> /dev/null && return $?
    project create $VDC_TEST_REMOVE_PROJECT --tenant $TENANT
    echo "Project $VDC_TEST_REMOVE_PROJECT created on first vdc."
    echo "Login other vdc to search the created vdc"
    VDC_ENDPOINT_A=$BOURNE_IPADDR
    export BOURNE_IPADDR=$VDC_ENDPOINT_B

    security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    # finding nothing is right, and it will throw python exception, we must catch it
    VDC_REMOVE_TEST_RESULT=`project search $(echo $VDC_TEST_REMOVE_PROJECT | head -c 5)\
    >> /tmp/pythonexception 2>&1 || echo "ok"`
    if [ "$VDC_REMOVE_TEST_RESULT" != "ok" ] ; then
        echo -e "\nError Happens in Test on resource after vdc removed in $BOURNE_IPADDR"
        exit 1
    fi
    echo "Find nothing, the result is $VDC_REMOVE_TEST_RESULT"
    if [ "$VDC_ENDPOINT_C" != "$VDC_DEFAULT_ENDPOINT" ] ; then
        export BOURNE_IPADDR=$VDC_ENDPOINT_C
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
        VDC_REMOVE_TEST_RESULT=`project search $(echo $VDC_TEST_REMOVE_PROJECT | head -c 5)\
        >> /tmp/pythonexception 2>&1 || echo "ok"`
        if [ "$VDC_REMOVE_TEST_RESULT" != "ok" ] ; then
            echo -e "\nError Happens in Test on resource after vdc removed in $BOURNE_IPADDR"
            exit 1
        fi
        echo "Find nothing, the result is $VDC_REMOVE_TEST_RESULT"
    fi
    export BOURNE_IPADDR=$VDC_ENDPOINT_A
    echo "Test on resource after vdc removed done"

    echo "Delete project $VDC_TEST_PROJECT."
    if [ "$BOURNE_SECURITY_DISABLED" != '1' -a "$AUTH" != 'local' ] ; then
        security login $LOCAL_LDAP_SUPERUSER_USERNAME $LOCAL_LDAP_SUPERUSER_PASSWORD
    fi
    project delete $VDC_TEST_PROJECT

    echo "ALL VDC TEST DONE"
}

tune_dr_config()
{
    VIP=$1
    CONFIG_KEY=$2
    CONFIG_VALUE=$3
    OPT=""
    if [[ $VIP == \[* ]];
    then
        tmp=${VIP#*[}
        VIP=${tmp%]*}
        OPT="-6"
    fi

    echo "VIP=${VIP} OPT=${OPT} KEY=${CONFIG_KEY} VALUE=${CONFIG_VALUE}"
    SSH ${VIP} "/opt/storageos/bin/zkutils tune_dr_config ${CONFIG_KEY} ${CONFIG_VALUE}" ${OPT}
}

stop_vdc_services()
{
    VIP=$1
    OPT=""
    NETWORK_PATTERN='^network_.*_ipaddr='
    if [[ $VIP == \[* ]];
    then
        tmp=${VIP#*[}
        VIP=${tmp%]*}
        OPT="-6"
        NETWORK_PATTERN='^network_.*_ipaddr6='
    fi

    echo "VIP=$VIP OPT=$OPT"
    IP_ADDRESS_LIST=`SSH $VIP "/etc/systool --getprops | grep '$NETWORK_PATTERN' | cut -d= -f 2 " $OPT`

    echo "stop services on VDC($VIP) that has following IP addresses $IP_ADDRESS_LIST"
    for ip_addr in $IP_ADDRESS_LIST
    do
        SSH $ip_addr 'service nginx stop;' $OPT
        SSH $ip_addr '/etc/storageos/storageos stop;' $OPT
    done
}

start_vdc_services()
{
    VIP=$1
    NETWORK_PATTERN='^network_.*_ipaddr='
    if [[ $VIP == \[* ]];
    then
        tmp=${VIP#*[}
        VIP=${tmp%]*}
        OPT="-6"
        NETWORK_PATTERN='^network_.*_ipaddr6='
    fi

    echo "VIP=$VIP OPT=$OPT"
    IP_ADDRESS_LIST=`SSH $VIP "/etc/systool --getprops | grep '$NETWORK_PATTERN' | cut -d= -f 2 " $OPT`
    echo "start services on VDC($VIP) that has following IP addresses $IP_ADDRESS_LIST"
    for ip_addr in $IP_ADDRESS_LIST
    do
        SSH $ip_addr '/etc/storageos/storageos start;' $OPT
        SSH $ip_addr 'service nginx start;' $OPT
    done
}

#Usage: sanity <conf path> <vip> backuprestore backup-server-uri username password
# E.g.: sanity conf/sanity.conf 137.69.169.21 backuprestore ftp://X.X.X.X/foo root root-password 
backuprestore_setup()
{
    echo "Setup backup restore environment begin"
    login_nd_configure_smtp_nd_add_licenses
    vip=$BOURNE_IPADDR

    echo "Enable root ssh permit and configure backup server"
    syssvc $CONFIG_FILE "$vip" set_prop system_permit_root_ssh yes

    syssvc $CONFIG_FILE "$vip" set_prop backup_external_location_username "${BACKUP_SERVER_USERNAME}"
    syssvc $CONFIG_FILE "$vip" set_prop backup_external_location_password "${BACKUP_SERVER_PASSWORD}"
    syssvc $CONFIG_FILE "$vip" set_prop backup_external_location_url "${BACKUP_SERVER_URL}"

    echo -e "Done\nurl=${BACKUP_SERVER_URL} username=${BACKUP_SERVER_USERNAME} password=${BACKUP_SERVER_PASSWORD}"
}

backuprestore_tests()
{
    backupname='sanity'
    backupname2='abc'
    filepath='/tmp/sanity.zip'
    remote_backup_file=""

    backup_tests "${backupname}" "${backupname2}" "${filepath}"
    set +E
    restore_tests "${filepath}"
    set -E
}

backup_tests()
{
    local backupname=$1
    local backupname2=$2
    local filepath=$3

    echo "Creating backup(${backupname})"
    backup create "${backupname}"

    if [ $? -ne 0 ]; then
        echo "Create backup ${backupname} failed"
        exit 1
    fi

    echo "Create backup ${backupname} successful"

    echo "Creating another backup(${backupname2})"
    backup create "${backupname2}"

    if [ $? -ne 0 ]; then
        echo "Create backup ${backupname2} failed"
        exit 1
    fi

    echo "Create backup ${backupname2} successful"

    echo "Listing local backup(${backupname})"
    local found=$(backup list "${backupname}")
    echo "found=$found"

    if [ "$found"x != "${backupname}"x ]; then
        echo "List backup failed"
        exit 1
    fi

    echo "List local backup ${backupname} successful"

    echo "Query local backup ${backupname} info"
    backup query-info "${backupname}" "true"

    echo "Uploading backup(${backupname}) to ${BACKUP_SERVER_URL}"
    backup upload "${backupname}"

    if [ $? != 0 ]; then
        echo "Upload backup ${backupname} failed"
        exit 1
    fi

    echo "Wait for upload ${backupname} to complete"

    for(( i=0; i<60; i++ )); do
       status=$(backup query-upload "${backupname}")
       echo "status=${status}"

       if [[ "${status}" == "DONE" || "${status}" == "FAILED" ]] ; then
           break;
       fi

       sleep 10s
    done

    echo "status=${status}"
    if [[ "${status}" != "DONE" ]]; then
        echo "upload backup ${backupname} failed status=${status}"
        exit 1
    fi

    echo "Listing external backup(${backupname} from ${BACKUP_SERVER_URL})"

    found=""
    for(( i=0; i<60; i++ )); do
       found=$(backup list-external "${backupname}")
       echo "found=${found}"

       if [[ "${found}" != "" ]]; then
           break;
       fi
       sleep 10s
    done

    if [[ "${found}" == "" ]]; then
       echo "Failed to find backup ${backupname} on ${BACKUP_SERVER_URL}"
       exit 1
    fi

    echo "Query remote backup ${found} info"
    backup query-info "${found}" "false"

    echo "Pull backup(${found} from ${BACKUP_SERVER_URL})"
    backup pull "${found}"

    if [[ $? -ne 0 ]]; then
        echo "pull backup failed"
        exit 1
    fi

    echo "Start to query pull status"
    local status=""

    for (( i=0; i<60; i++ )); do
       status=$(backup query-pull "${found}" "false")
       echo "status=${status}"

       if [[ "${status}" == "DOWNLOAD_SUCCESS" ]]; then
           break;
       fi

       sleep 10s
    done

    if [[ "${status}" != "DOWNLOAD_SUCCESS" ]]; then
       echo "Failed to pull backup ${found}"
       exit 1
    fi

    remote_backup_file="${found}"

    echo "Deleting backup(${backupname2})"
    backup delete "${backupname2}"

    if [[ $? -ne 0 ]]; then
        echo "Delete backup failed"
        exit 1
    fi

    echo "Delete backup ${backupname2} successful"

    echo "Listing backup(${backupname2})"

    found=`backup list "${backupname2}"`
    echo "found=$found"

    if [[ "$found"x == "${backupname2}"x ]]; then
        echo "Backup(${backupname}-tmp) should not exist"
        exit 1
    fi

    echo "Downloading backup(${backupname}) to ${filepath}"

    backup download "${backupname}" "${filepath}"

    if [[ $? -ne 0 ||  ! -f ${filepath} ]]; then
        echo "Download backup failed"
        exit 1
    fi

    echo "Download backup successful"

    echo "Deleting backup(${backupname})"

    backup delete "${backupname}"

    if [[ $? -ne 0 ]]; then
        echo "Delete backup failed"
        exit 1
    fi

    echo "Delete backup successful"
}

restore_tests()
{
    filepath="$1"
    password=$SYSADMIN_PASSWORD

    echo "Restore from pulled backup ${remote_backup_file} password=${password}"
    backup restore "${remote_backup_file}" "false" "${password}"

    echo "Wait for stable"
    sleep 60s

    for (( i=0; i<240; i++ )); do
       security login $SYSADMIN $SYSADMIN_PASSWORD
       if [[ $? -eq 0 ]]; then
            break;
       fi
       echo "login failed retry later"
       sleep 10s
    done

    echo "Uploading backup file ${filepath} to $BOURNE_IPADDR"
    SCP $BOURNE_IPADDR ${filepath} ${filepath}

    echo "Restoring backup file ${filepath} on $BOURNE_IPADDR"    
    SSH $BOURNE_IPADDR "echo -e 'yes\n$password' | /opt/storageos/bin/restore '${filepath}'"
    ret=$?
    echo "ret=$ret"
    if [[ ${ret} -ne 0 ]]; then
        echo "Please check /opt/storageos/logs/bkutils.log for details"
        exit 1
    fi
}

recovery_setup()
{
    echo "Setup minority node corrupted cluster($BOURNE_IPADDR) begin"
    vip=$BOURNE_IPADDR
    corrupted_node_id=2

    echo "Enable root ssh permit"
    syssvc $CONFIG_FILE "$vip" set_prop system_permit_root_ssh yes
    echo "Enable root ssh permit done"
    recovery wait_for_stable 10 600

    if [[ $vip == \[* ]]; then
        tmp=${vip#*[}
        vip=${tmp%]*}
        opt="-6"
        network_patten="^network_${corrupted_node_id}_ipaddr6="
    else
        opt=""
        network_patten="^network_${corrupted_node_id}_ipaddr="
    fi

    echo "network_patten=$network_patten opt=$opt"
    node_count=`SSH $vip "/etc/systool --getprops | grep '^node_count=' | cut -d= -f 2" $opt`
    if [[ $node_count -lt 3 ]]; then
        echo "Could not do node recovery on cluster that node count($node_count) less then 3"
        exit 2
    fi

    ip_addr=`SSH $vip "/etc/systool --getprops | grep '$network_patten' | cut -d= -f 2 " $opt`
    export RECOVERY_CORRUPTED_NODE_IP=$ip_addr

    echo "Simulating vipr${corrupted_node_id}($RECOVERY_CORRUPTED_NODE_IP) crash"
    recovery_node_crash $RECOVERY_CORRUPTED_NODE_IP
    echo "Make vipr${corrupted_node_id}($RECOVERY_CORRUPTED_NODE_IP) corrupted done"
 
    sleep 60
    echo "Setup minority node corrupted cluster($BOURNE_IPADDR) successful"
}

recovery_tests()
{
    echo "Begin to run node recovery on $BOURNE_IPADDR"
    recovery trigger
 
    for(( i=0; i<10; i++ )); do
       recovery_status=`recovery get_status`
       echo "Cuttent recovery status is: $recovery_status"
       if [ recovery_status != '' ]; then
           break;
       fi
       sleep 3 
    done

    recovery_status=`recovery get_status`
    if [ recovery_status != '' ]; then
        echo "Simulating vipr${corrupted_node_id}($RECOVERY_CORRUPTED_NODE_IP) redeployment"
        recovery_node_redeploy $RECOVERY_CORRUPTED_NODE_IP
        echo "Make vipr${corrupted_node_id}($RECOVERY_CORRUPTED_NODE_IP) redeployed done"
    else
        echo "Unexcepted error"
        recovery_node_startup $RECOVERY_CORRUPTED_NODE_IP
        exit 1                         
    fi

    while true; do
        recovery_status=`recovery get_status`
        echo "Cuttent recovery status is: $recovery_status"
        if [ recovery_status == '' ]; then
            echo "Unexcepted error"
            exit 1
        elif [[ "${recovery_status[*]}" == *"DONE"* ]]; then
            echo "Node recovery successful"
            break
        elif [[ "${recovery_status[*]}" == *"FAILED"* ]]; then
            echo "Node recovery failed"
            exit 1
        elif [[ "${recovery_status[*]}" == *"REPAIRING"* ]]; then
            db_repair_status=`recovery get_db_repair_status`
            echo "Db repair status is: $db_repair_status"
            sleep 10
        else
            sleep 5
        fi
    done
}

recovery_node_crash()
{
    ip_addr=$1
    opt=$2
    SSH $ip_addr '/etc/storageos/storageos stop; rm -rf /data/db/1 /data/geodb/1 /data/zk/*' $opt
}

recovery_node_redeploy()
{
    ip_addr=$1
    opt=$2
    SSH $ip_addr 'echo startupmode=hibernate > /data/db/startupmode; echo startupmode=hibernate > /data/geodb/startupmode; chown -R storageos:storageos /data; /etc/storageos/storageos start' $opt
}

recovery_node_startup()
{
    ip_addr=$1
    opt=$2
    SSH $ip_addr '/etc/storageos/storageos start' $opt
}

objcontrolsvc_setup()
{
    echo "Nothing to do for objcontrol setup"
}

objcontrolsvc_tests()
{
    nodeobj create "datanode-001"
    echo "Create node object datanode-001 succeed."
    nodeobj create "datanode-002"
    echo "Create node object datanode-002 succeed."
    nodeobj create "datanode-003"
    echo "Create node object datanode-003 succeed."
}

# Save the latest token file to token.txt on error for debugging and remove the current one
save_token_file() {
    if [ "$BOURNE_SECURITY_DISABLED" != "1" ]; then
       [ -f ${BOURNE_TOKEN_FILE} ] && mv ${BOURNE_TOKEN_FILE} ${BOURNE_SAVED_TOKEN_FILE}
    fi
}

_failure() {
    if [ -f ${CMD_OUTPUT} ]; then
	echo "*******************************************************" >&2
	/usr/bin/cat ${CMD_OUTPUT}                                     >&2
	echo "*******************************************************" >&2
    fi
    echo "***"                                                   >&2
    secho "FAILED! $0:$1: `eval echo ${BASH_COMMAND}`"           >&2
    echo "***"                                                   >&2
    date
    run_undo_commands
    save_token_file
    exit 1
}

_success() {
    echo "***"                                                   >&2
    secho "PASSED!"                                              >&2
    echo "***"                                                   >&2
    date
    save_token_file
    finalize_undo_log
}

#counterpart for run
#executes a command that is expected to fail
fail(){
    cmd=$*
    echo === $cmd
    trap - ERR
    if [ "${HIDE_OUTPUT}" = "" -o "${HIDE_OUTPUT}" = "1" ]; then
	$cmd &> ${CMD_OUTPUT}
    else
	$cmd 2>&1
    fi

    status=$?
    if [ $status -eq 0 ] ; then
        echo '**********************************************************************'
        echo $cmd succeeded, which should not have happened
	cat ${CMD_OUTPUT}
        echo '**********************************************************************'
        trap '_failure $LINENO' ERR
        set_undo $cmd
        exit 1
    fi
    secho "$cmd failed, which is the expected ouput"
    trap '_failure $LINENO' ERR
}

trap '_failure $LINENO' ERR INT
set -E
if [ "$3" != "vdc" ] && [ "$3" != "dr" ] && [ "$3" != "backuprestore" ]; then
    common_setup	
fi

datastore_setup(){
    opname=$1
    fileCos=$2
    opsize=$3
    deviceType=$4

    echo "create datastore for device" $deviceType
    if [ "$deviceType" = "nfsexportpoints" ]; then
        run datastore create nfsexportpoints $opname $NH --size $opsize --mountpoint $opname
    fi
    if [ "$deviceType" = "filesystems" ]; then
        run datastore create filesystems $opname $NH --filecos $fileCos --size $opsize
    fi
}

output_conf(){
    file=$WS_SETUP
    echo "nh="$NH > "$file"
    echo "tenant="$TENANT >> "$file"
    echo "namespace="$NAMESPACE >> "$file"
    echo "project="$PROJECT >> "$file"
    echo "cos="${WS_SETUP_COS#,} >> "$file"
    echo "user=$WS_UID" >> "$file"
    echo "secretkey=$WS_SECRET" >> "$file"
    echo "bucket=${WS_SETUP_BUCKETS# }" >> "$file"
}

application_setup()
{
    secho "Executing volume group setup"
    vmaxblock_setup
    vnxblock_setup
    consistencygroup_block_cos_setup
}

application_crud_test()
{
    secho "Executing volume group crud test"
    appname=sanityapp-${RANDOM}

    # test and verify create volume group
    run volumegroup create ${appname} descrip COPY
    run volumegroup show ${appname}
    run volumegroup verify ${appname} description descrip
    #run volume group ${appname} roles COPY

    # test and verify list volume group
    run volumegroup list

    # test and verify volume group update
    newappname=${appname}new
    run volumegroup update ${appname} ${newappname} newdescription
    run volumegroup show ${newappname}
    run volumegroup verify ${newappname} description newdescription

    # test and verify volume group delete
    run volumegroup delete ${newappname}
    fail volumegroup show ${newappname}

    mobilityname=mobilitygroup-${RANDOM}
    run volumegroup create ${mobilityname} description MOBILITY VPLEX VOLUMES
    run volumegroup delete ${mobilityname}

    mobilityname=mobilitygroup-${RANDOM}
    run volumegroup create ${mobilityname} description MOBILITY VPLEX HOSTS
    run volumegroup delete ${mobilityname}

    mobilityname=mobilitygroup-${RANDOM}
    run volumegroup create ${mobilityname} description MOBILITY VPLEX CLUSTERS
    run volumegroup delete ${mobilityname}
 
    mobilityname=mobilitygroup-${RANDOM}
    fail volumegroup create ${mobilityname} description MOBILITY VPLEX INVALIDTYPE 

    mobilityname=mobilitygroup-${RANDOM}
    fail volumegroup create ${mobilityname} description MOBILITY INVALIDTYPE VOLUMES
}

application_createvols()
{
    #unique=12044
    unique=${RANDOM}
    basename=vgsanity-${unique}
    VG_CGNAME1=cg1${unique}
    VG_CGNAME2=cg2${unique}
    VG_COPYVG1=${basename}-copyvg1
    VG_COPYVG2=${basename}-copyvg2
    VG_MOBIVG1=${basename}-mobivg1

    run blockconsistencygroup create ${PROJECT} ${VG_CGNAME1}
    a=`blockconsistencygroup show ${VG_CGNAME1} | grep '"id":' | grep BlockConsistencyGroup | awk '{print $2}'`
    b=${a:1}
    VG_CGID1=${b%'"'*}
    echo cgid1 is ${VG_CGID1}

    run blockconsistencygroup create ${PROJECT} ${VG_CGNAME2}
    c=`blockconsistencygroup show ${VG_CGNAME2} | grep '"id":' | grep BlockConsistencyGroup | awk '{print $2}'`
    d=${c:1}
    VG_CGID2=${d%'"'*}
    echo cgid2 is ${VG_CGID2}

    run volume create ${basename}-cg $PROJECT $NH $VG_VPOOL_CG $BLK_SIZE --thinVolume true --consistencyGroup ${VG_CGNAME1} --count 3
    VG_CGVOL1="${basename}-cg-1"
    VG_CGVOL2="${basename}-cg-2"
    VG_CGVOL3="${basename}-cg-3"
    VG_CGVOLID1=`volume list ${PROJECT} | grep ${VG_CGVOL1} | awk '{print $7}'`
    VG_CGVOLID2=`volume list ${PROJECT} | grep ${VG_CGVOL2} | awk '{print $7}'`
    VG_CGVOLID3=`volume list ${PROJECT} | grep ${VG_CGVOL3} | awk '{print $7}'`
    echo cgvolid1 is ${VG_CGVOLID1}
    echo cgvolid2 is ${VG_CGVOLID2}
    echo cgvolid3 is ${VG_CGVOLID3}

    run volume create ${basename} $PROJECT $NH $VG_VPOOL $BLK_SIZE --thinVolume true --count 3
    VG_VOL1="${basename}-1"
    VG_VOL2="${basename}-2"
    VG_VOL3="${basename}-3"
    VG_VOLID1=`volume list ${PROJECT} | grep ${VG_VOL1} | awk '{print $7}'`
    VG_VOLID2=`volume list ${PROJECT} | grep ${VG_VOL2} | awk '{print $7}'`
    VG_VOLID3=`volume list ${PROJECT} | grep ${VG_VOL3} | awk '{print $7}'`
    echo volid1 is ${VG_VOLID1}
    echo volid2 is ${VG_VOLID2}
    echo volid3 is ${VG_VOLID3}

    run volumegroup create ${VG_COPYVG1} copy1 COPY
    run volumegroup create ${VG_COPYVG2} copy2 COPY
    run volumegroup create ${VG_MOBIVG1} mobility MOBILITY
}

application_deleteall()
{
    run volume delete $PROJECT/$VG_CGVOL1 --wait
    run volume delete $PROJECT/$VG_CGVOL2 --wait
    run volume delete $PROJECT/$VG_CGVOL3 --wait
    run volume delete $PROJECT/$VG_VOL1 --wait
    run volume delete $PROJECT/$VG_VOL2 --wait
    run volume delete $PROJECT/$VG_VOL3 --wait
    run blockconsistencygroup delete ${VG_CGNAME1}
    run blockconsistencygroup delete ${VG_CGNAME2}
    run volumegroup delete ${VG_COPYVG1}
    run volumegroup delete ${VG_COPYVG2}
    run volumegroup delete ${VG_MOBIVG1}
}

# adds and removes volumes from a COPY volume group
application_volume_tests()
{
    # add 3 volumes from same CG to application
    run volumegroup add-volumes ${VG_COPYVG1} ${VG_CGVOLID1},${VG_CGVOLID2},${VG_CGVOLID3}

    # add one volume not in a CG to the same CG
    run volumegroup add-volumes ${VG_COPYVG1} ${VG_VOLID1} ${VG_CGID1}

    # add 1 volume not in a CG to a different CG
    run volumegroup add-volumes ${VG_COPYVG1} ${VG_VOLID2} ${VG_CGID2}

    # add the saem volume to a mobility group
    run volumegroup add-volumes ${VG_MOBIVG1} ${VG_VOLID2}

    # remove
    run volumegroup remove-volumes ${VG_COPYVG1} ${VG_VOLID2}
    run volumegroup remove-volumes ${VG_MOBIVG1} ${VG_VOLID2}

    # add 1 volume not in a CG to a different copy vg
    run volumegroup add-volumes ${VG_COPYVG2} ${VG_VOLID2} ${VG_CGID2}

    #remove from one copy group then add to another
    run volumegroup remove-volumes ${VG_COPYVG1} ${VG_CGVOLID1}
    run volumegroup add-volumes ${VG_COPYVG2} ${VG_CGVOLID1} ${VG_CGID2}
    
}

application_tests()
{
    secho "Executing volume group tests"
    VG_VPOOL_CG=$VMAX_COS_GROUP
    VG_VPOOL=$COS_VMAXBLOCK_THIN
    application_crud_test
    application_createvols
    application_volume_tests
    application_deleteall
}

vplexsnap_setup()
{
    secho "Executing vplexsnap setup"
    vplex_setup
}

vplexsnap_tests()
{
    secho "Executing VPLEX snapshot export tests"

    hname=$(hostname)
    if [ $hname = "standalone" ]; then
        hname=$SHORTENED_HOST
    fi
    echo "hostname is $hname"

    localVolume=$hname-${RANDOM}-VPlexLocal1
    localSnapshot=$hname-${RANDOM}-VPlexLocalSnap
    host=$PROJECT.lss.emc.com
    hostLbl=$PROJECT
    PWWN1=10:00:00:E0:7E:00:00:0F
    WWNN1=20:00:00:E0:7E:00:00:0F
    PWWN2=10:00:00:90:FA:18:0E:99
    WWNN2=20:00:00:90:FA:18:0E:99

    secho "Creating source VPLEX local volume"
    run volume create $localVolume $PROJECT $NH cosvplexlocal $BLK_SIZE

    secho "Creating VPLEX snapshot for source"
    run blocksnapshot create $PROJECT/$localVolume $localSnapshot
    blocksnapshot list $PROJECT/$localVolume
    blocksnapshot show $PROJECT/$localVolume/$localSnapshot

    secho "Creating VPLEX volume from snapshot"
    blocksnapshot expose $PROJECT/$localVolume/$localSnapshot

    secho "Creating host"
    hosts create $hostLbl $TENANT Windows $host --port 8111 --username user --password 'password' --osversion 1.0

    secho "Creating initiators"
    initiator create $hostLbl FC $PWWN1 --node $WWNN1
    initiator create $hostLbl FC $PWWN2 --node $WWNN2

    secho "Export VPLEX volume built from snapshot"
    run export_group create $PROJECT $hname-1$host $NH --volspec "$PROJECT/$localSnapshot" --inits "$hostLbl/$PWWN1","$hostLbl/$PWWN2"

    secho "Unexport VPLEX volume built from snapshot"
    run export_group delete $PROJECT/$hname-1$host

    secho "Deleting source VPLEX local volume"
    run volume delete $PROJECT/$localVolume --wait

    secho "Deleting Host"
    run hosts delete $hostLbl

    secho "Completed VPLEX snapshot export tests"
}

snapvx_vpool_setup()
{
    secho "Creating snapvx virtual pools"
    cos create block $COS_VMAX3BLOCK_FC true \
	--description 'Virtual-Pool-for-VMAX3-block-FC' \
                      --protocols FC \
                      --numpaths 2 \
                      --max_snapshots 10 \
	               --system_type vmax \
                      --provisionType 'Thin' \
			 --neighborhoods $NH \
                         --multiVolumeConsistency
}

snapvx_setup_once()
{
    # Do this only once
    secho "Checking for the existance of the VMAX3 provider"
    smisprovider show $VMAX3_SMIS_DEV &> /dev/null && return $?

    # Create provider for VMAX3 array.
    secho "Creating VMAX3 provider"
    smisprovider create $VMAX3_SMIS_DEV $VMAX3_SMIS_IP $VMAX3_SMIS_PORT $SMIS_USER "$SMIS_PASSWD" $VMAX3_SMIS_SSL

    # Discover the vmax3 array.
    secho "Discovering VMAX3 array"
    storagedevice discover_all --ignore_error

    # Set up virtual pools
    secho "Setting up virtual pools"
    snapvx_vpool_setup

    # Assign the storage pools of the VMAX3 array to our virtual array.
    secho "Assiging storage pools to virtual array"
    storagepool update $VMAX3_NATIVEGUID --nhadd $NH
 
    # Assign storage ports if necessary.     
    if [ $DISCOVER_SAN -eq 0 ]; then
        secho "Updating storage port networks"
        for porta in ${VMAX3_PORTS_A}
            do
	         storageport update $VMAX3_NATIVEGUID FC --tzone $FCTZ_A --group ${porta}
            done
    fi

    # Update virtual pool to use mathcing pools on the system.
    secho "Assiging system storage pools to virtual pool"
    cos update block $COS_VMAX3BLOCK_FC --storage $VMAX3_NATIVEGUID
}


snapvx_setup()
{
    secho "Executing snapvx setup"
    snapvx_setup_once
    cos allow $COS_VMAX3BLOCK_FC block $TENANT
}

snapvx_tests()
{
   snapvx_group_tests
   snapvx_single_tests
}

snapvx_group_tests()
{
    secho "Executing snapvx group tests"
    CG_NAME=SnapVxCG${RANDOM}
    VOLUME_NAME=SnapVxTest-CG-${RANDOM}
    SNAP_SESSION_NAME1=SnapSession-CG-${RANDOM}
    SNAP_SESSION_NAME2=SnapSession-CG-${RANDOM}
    SNAP_SESSION_NAME3=SnapSession-CG-${RANDOM}
    SNAP_SESSION_TGT_NAME1=SnapSessionTgt-CG-${RANDOM}
    SNAP_SESSION_TGT_NAME2=SnapSessionTgt-CG-${RANDOM}
    SNAP_SESSION_TGT_NAME3=SnapSessionTgt-CG-${RANDOM}

    run blockconsistencygroup create $PROJECT $CG_NAME

    # Create a VMAX3 volumes in CG to serve as the snapshot session sources.
    secho "Creating VMAX3 source volume"
    volume create $VOLUME_NAME-1 $PROJECT $NH $COS_VMAX3BLOCK_FC $BLK_SIZE --consistencyGroup $CG_NAME
    volume create $VOLUME_NAME-2 $PROJECT $NH $COS_VMAX3BLOCK_FC $BLK_SIZE --consistencyGroup $CG_NAME

    # Create snapvx sessions with no linked targets
    run blockconsistencygroup create_snapshot_session $CG_NAME $SNAP_SESSION_NAME1
    run blockconsistencygroup create_snapshot_session $CG_NAME $SNAP_SESSION_NAME2

    # Restore the snapvx session.
    secho "Restoring SnapVx session 1"
    run blockconsistencygroup restore_targets $CG_NAME/$SNAP_SESSION_NAME1

    # Link target to the snapvx session 1 in default nocopy mode.
    secho "Linking target to SnapVx session 1"
    run blockconsistencygroup link_targets $CG_NAME/$SNAP_SESSION_NAME1 1 $SNAP_SESSION_TGT_NAME1

    # Relink target to the snapvx session 2.
    secho "Relinking target to SnapVx session 2"
    run blockconsistencygroup relink_targets $CG_NAME/$SNAP_SESSION_NAME2 $CG_NAME/${SNAP_SESSION_TGT_NAME1}-1-1

    # Unlink target from the snapvx session and delete target volume.
    secho "unlinking target from SnapVx session 2"
    run blockconsistencygroup unlink_targets $CG_NAME/$SNAP_SESSION_NAME2 $CG_NAME/${SNAP_SESSION_TGT_NAME1}-1-1 --delete_target true

    # Delete the snapvx sessions.
    secho "Deleting SnapVx sessions 1 and 2"
    run blockconsistencygroup delete_snapshot_session $CG_NAME/$SNAP_SESSION_NAME1
    run blockconsistencygroup delete_snapshot_session $CG_NAME/$SNAP_SESSION_NAME2

    # Create a snapvx session with multiple linked targets in copy mode
    secho "Creating SnapVx session 3 with multiple linked targets"
    run blockconsistencygroup create_snapshot_session $CG_NAME $SNAP_SESSION_NAME3 --target_count 2 --target_name $SNAP_SESSION_TGT_NAME2 --target_copymode copy

    # Link another target to the snapvx session.
    secho "Linking another target to SnapVx session 3 in copy mode"
    run blockconsistencygroup link_targets $CG_NAME/$SNAP_SESSION_NAME3 1 $SNAP_SESSION_TGT_NAME3 --target_copymode copy

    # Restore the source from a linked target
    secho "Restoring source from linked target"
    run blockconsistencygroup restore_snapshot $CG_NAME $SNAP_SESSION_TGT_NAME3-1-1

    # Unlink target from the snapvx session and delete target volume.
    secho "Unlinking all targets from SnapVx session 3"
    run blockconsistencygroup unlink_targets $CG_NAME/$SNAP_SESSION_NAME3 $CG_NAME/$SNAP_SESSION_TGT_NAME2-1-1 --delete_target true
    run blockconsistencygroup unlink_targets $CG_NAME/$SNAP_SESSION_NAME3 $CG_NAME/$SNAP_SESSION_TGT_NAME2-2-1 --delete_target true
    run blockconsistencygroup unlink_targets $CG_NAME/$SNAP_SESSION_NAME3 $CG_NAME/$SNAP_SESSION_TGT_NAME3-1-1 --delete_target true

    # Create snapvx sessions with no linked targets
    run blockconsistencygroup delete_snapshot_session $CG_NAME/$SNAP_SESSION_NAME3

    # Delete the source volumes
    volumes=`blockconsistencygroup list_volume_ids $CG_NAME | tail -n+2`
    for volume in $volumes
    do
        secho "Deleting the VMAX3 source volume $volume"
        volume delete $volume --wait
    done

    secho "Deleting the VMAX3 CG"
    # Delete the snapshot session source consistency group.
    run blockconsistencygroup delete $CG_NAME
}

snapvx_single_tests()
{
    secho "Executing snapvx tests"
    VOLUME_NAME=SnapVxTest-${RANDOM}
    SNAP_SESSION_NAME1=SnapSession-${RANDOM}
    SNAP_SESSION_NAME2=SnapSession-${RANDOM}
    SNAP_SESSION_NAME3=SnapSession-${RANDOM}
    SNAP_SESSION_TGT_NAME1=SnapSessionTgt-${RANDOM}
    SNAP_SESSION_TGT_NAME2=SnapSessionTgt-${RANDOM}
    SNAP_SESSION_TGT_NAME3=SnapSessionTgt-${RANDOM}

    # Create a VMAX3 volume to serve as the snapshot session source.
    secho "Creating VMAX3 source volume"
    volume create $VOLUME_NAME $PROJECT $NH $COS_VMAX3BLOCK_FC $BLK_SIZE

    # Create snapvx sessions with no linked targets.
    secho "Creating SnapVx sessions 1 and 2 with no linked targets"
    snapshotsession create $PROJECT/$VOLUME_NAME $SNAP_SESSION_NAME1
    snapshotsession show $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME1
    snapshotsession create $PROJECT/$VOLUME_NAME $SNAP_SESSION_NAME2
    snapshotsession show $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME2

    # Restore the snapvx session.
    secho "Restoring SnapVx session 1"
    snapshotsession restore $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME1

    # Link target to the snapvx session 1 in default nocopy mode.
    secho "Linking target to SnapVx session 1"
    snapshotsession link_targets $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME1 1 $SNAP_SESSION_TGT_NAME1

    # Relink target to the snapvx session 2.
    secho "Relinking target to SnapVx session 2"
    snapshotsession relink_target $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME2 $PROJECT/$VOLUME_NAME/$SNAP_SESSION_TGT_NAME1-1

    # Delete the snapvx sessions.
    secho "Deleting SnapVx session 1"
    snapshotsession delete $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME1

    # delete the targetless session first before unlinking (OPT 498174)

    # Unlink target from the snapvx session and delete target volume.
    secho "unlinking target from SnapVx session 2"
    snapshotsession unlink_target $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME2 $PROJECT/$VOLUME_NAME/$SNAP_SESSION_TGT_NAME1-1 --delete_target true

    # Delete the snapvx sessions.
    secho "Deleting SnapVx session 2"
    snapshotsession delete $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME2

    # Create a snapvx session with multiple linked targets in copy mode
    secho "Creating SnapVx session 3 with multiple linked targets"
    snapshotsession create $PROJECT/$VOLUME_NAME $SNAP_SESSION_NAME3 --target_count 2 --target_name $SNAP_SESSION_TGT_NAME2 --target_copymode copy

    # Link another target to the snapvx session.
    secho "Linking another target to SnapVx session 3 in copy mode"
    snapshotsession link_targets $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME3 1 $SNAP_SESSION_TGT_NAME3 --target_copymode copy

    # Restore the source from a linked target
    secho "Restoring source from linked target"
    blocksnapshot restore $PROJECT/$VOLUME_NAME/$SNAP_SESSION_TGT_NAME3-1

    # Unlink target from the snapvx session and delete target volume.
    secho "Unlinking all targets from SnapVx session 3"
    snapshotsession unlink_target $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME3 $PROJECT/$VOLUME_NAME/$SNAP_SESSION_TGT_NAME2-1 --delete_target true
    snapshotsession unlink_target $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME3 $PROJECT/$VOLUME_NAME/$SNAP_SESSION_TGT_NAME2-2 --delete_target true
    snapshotsession unlink_target $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME3 $PROJECT/$VOLUME_NAME/$SNAP_SESSION_TGT_NAME3-1 --delete_target true

    # Delete the snapvx session.
    secho "Deleting SnapVx sessions 3"
    snapshotsession delete $PROJECT/$VOLUME_NAME/$SNAP_SESSION_NAME3

    # Delete the snapshot session source volume.
    secho "Deleting the VMAX3 source volume"
    volume delete $PROJECT/$VOLUME_NAME --wait
}

ceph_setup()
{
    secho "Ceph setup"
    storageprovider show $CEPH_PROVIDER  &> /dev/null && return $?
    run storageprovider create $CEPH_PROVIDER $CEPH_IP 22 $CEPH_USER "$CEPH_SECRET_KEY" ceph
    run storagedevice discover_all
    storagedevice list

    storage_device=`storagedevice list | awk '/ceph/ {print($2)}'`
    storage_port=`storageport list $storage_device | grep -o 'CEPH.*PORT+-'`
    run transportzone add $IP_ZONE "$storage_port"

    run cos create block $CEPH_COS true --description='Ceph-VPool' --protocols=RBD --provisionType='Thin' --max_snapshots=8 --neighborhoods="$NH" --expandable true --systemtype ceph
    run cos allow $CEPH_COS block $TENANT

    run hosts create "$CEPH_CLIENT_HOST_NAME" "$TENANT" Linux "$CEPH_CLIENT_HOST" --port 22 --username $CEPH_CLIENT_HOST_USER --password $CEPH_CLIENT_HOST_PASSWORD --discoverable true
    sleep 10
    port=`initiator list $CEPH_CLIENT_HOST_NAME | awk '/^rbd:/ {print($1)}'`
    run transportzone add $IP_ZONE "$port"

    secho "Ceph setup completed"
}

ceph_test_volume_capacity() {
    local volname=$1
    local expected_display_size=$2

    local requested_capacity_gb=`volume show $volname | awk '/"requested_capacity_gb"/ {print($2)}' | sed "s/[\"',]//g"`
    secho "Compare volume size $requested_capacity_gb with requested $display_size"
    test "$requested_capacity_gb" = "$expected_display_size"
}

ceph_test_volumes() {
    local size=$1
    local display_size=$2

    secho "${FUNCNAME} Begins: create/list/delete volumes"
    run volume create $CEPH_VOLNAME $PROJECT $NH $CEPH_COS $size
    local volname=$PROJECT/$CEPH_VOLNAME

    run volume show $volname
    ceph_test_volume_capacity $volname $display_size

    run volume delete $volname --wait
}

ceph_test_expand_volume() {
    local initial_size=$1
    local new_size=$2
    local display_new_size=$3

    secho "${FUNCNAME} Begins: Expand volume"
    run volume create $CEPH_VOLNAME $PROJECT $NH $CEPH_COS $initial_size
    local volname=$PROJECT/$CEPH_VOLNAME

    run volume expand $volname $new_size

    ceph_test_volume_capacity $volname $display_new_size

    run volume delete $volname --wait
}

ceph_test_full_copy() {
    local initial_size=$1
    local display_size=$2

    secho "${FUNCNAME} Begins: Full copy volume"
    run volume create $CEPH_VOLNAME $PROJECT $NH $CEPH_COS $initial_size
    local volname=$PROJECT/$CEPH_VOLNAME

    local fullcopy_name=FullCopy-"$CEPH_VOLNAME"
    run volume full_copy $fullcopy_name $volname
    local fcvolname=$PROJECT/$fullcopy_name

    run volume show $fcvolname
    ceph_test_volume_capacity $fcvolname $display_size

    run volume delete $volname --wait
    run volume delete $fcvolname --wait
}

ceph_test_snapshot() {
    secho "${FUNCNAME} Begins: create/list/delete snapshot"
    run volume create $CEPH_VOLNAME $PROJECT $NH $CEPH_COS $BLK_SIZE
    local volname=$PROJECT/$CEPH_VOLNAME

    local snap_name=Snap-"$VOLNAME"
    run blocksnapshot create $volname $snap_name
    local snapname=$volname/$snap_name

    run blocksnapshot show $snapname

    run blocksnapshot delete $snapname
    run volume delete $volname --wait
}

ceph_test_export() {
    secho "${FUNCNAME} Begins: Export/unexport volume"
    run volume create $CEPH_VOLNAME $PROJECT $NH $CEPH_COS $BLK_SIZE
    local volname=$PROJECT/$CEPH_VOLNAME
    local snap_name=Snap-"$CEPH_VOLNAME"
    run blocksnapshot create $volname $snap_name
    local snapname=$volname/$snap_name

    run export_group create $PROJECT $CEPH_EXPORT_GROUP_NAME $NH --type Host --volspec $volname --hosts "$CEPH_CLIENT_HOST_NAME"
    local egname=$PROJECT/$CEPH_EXPORT_GROUP_NAME
    run export_group update $egname --addVolspec $snapname

    run export_group delete $egname
    run blocksnapshot delete $snapname
    run volume delete $volname --wait
}

ceph_tests()
{
    secho "Ceph tests"
    ceph_test_volumes 1GB 1.00
    ceph_test_expand_volume 1GB 2GB 2.00
    ceph_test_full_copy 1GB 1.00
    ceph_test_snapshot
    ceph_test_export
}

unity_setup()
{
    networksystem show $BROCADE_NETWORK &> /dev/null && return $?
    networksystem create $BROCADE_NETWORK brocade --smisip $BROCADE_IP --smisport 5988 --smisuser $BROCADE_USER --smispw $BROCADE_PW --smisssl false
    secho "Starting storage system create"
    discoveredsystem create $UNITY_DEV unity $UNITY_IP $UNITY_PORT $UNITY_USER $UNITY_PW --serialno=$UNITY_SN
    storagedevice list

    project_setup

    secho "Setup ACLs on neighborhood for $TENANT"
    run neighborhood allow $NH $TENANT
    run neighborhood allow $NH $TENANT
    run transportzone assign ${SRDF_VMAXA_VSAN} $NH
    storagepool update $UNITY_NATIVEGUID --type block --volume_type THIN_AND_THICK
    run transportzone add ${SRDF_VMAXA_VSAN} $UNITY_INIT_PWWN1
    run transportzone add ${SRDF_VMAXA_VSAN} $UNITY_INIT_PWWN2
    unity_cos_setup
    run cos update block $COS_UNITYBLOCK_CG --storage $UNITY_NATIVEGUID

    storagepool update $UNITY_NATIVEGUID --type file
    storageport update $UNITY_NATIVEGUID IP --tzone nh/iptz
    run cos update file $COS_UNITY --storage $UNITY_NATIVEGUID
    run transportzone add $NH/$IP_ZONE $UNITY_IP_ENDPOINT1
    run storagedevice discover_all
}

unity_tests()
{
    FS_SIZEMB=$FS_UNITY_SIZE;
    FS_SIZE=$FS_UNITY_SIZE;
    FS_EXPAND_SIZE=$FS_UNITY_EXPAND_SIZE;
    file_tests $COS_UNITY default


    vol1=unity1-${RANDOM};
    vol2=unity-cg-${RANDOM};
    host=$PROJECT.lss.emc.com
    hostLbl=$PROJECT
    iqn1=iqn.1998-01.com.vmware:lgly6193-7ae20d76
    consistency_group=cg-${RANDOM}
    snap1_label=snap1-${RANDOM}
    snap2_label=snap2-${RANDOM}
    eg=eg-${RANDOM}
	    
	    run transportzone add $NH/$IP_ZONE $iqn1
    run volume create $vol1 $PROJECT $NH $COS_UNITYBLOCK_CG $BLK_SIZE
    sleep 60
    run volume expand $PROJECT/$vol1 $BLK_SIZE_EXPAND
	
    run hosts create $hostLbl $TENANT Windows $host --port 8111 --username user --password 'password' --osversion 1.0
    run initiator create $hostLbl FC $UNITY_INIT_PWWN1 --node $UNITY_INIT_NODE
    run initiator create $hostLbl FC $UNITY_INIT_PWWN2 --node $UNITY_INIT_NODE
	
    run export_group create $PROJECT $eg $NH --type Host --volspec $PROJECT/$vol1 --hosts $hostLbl
		
    run blocksnapshot create $PROJECT/$vol1 $snap1_label
    run blocksnapshot list $PROJECT/$vol1
    run blocksnapshot delete $PROJECT/$vol1/${snap1_label}
    run export_group delete $PROJECT/$eg
    run volume delete $PROJECT/$vol1 --wait
    run hosts delete $hostLbl
	    
    echo "unity consistency group tests"
	    
    run blockconsistencygroup create $PROJECT $consistency_group
    run volume create $vol2 $PROJECT $NH $COS_UNITYBLOCK_CG 1280000000 --consistencyGroup $consistency_group
    sleep 60
    run blocksnapshot create $PROJECT/$vol2 $snap2_label
    sleep 30
    run blocksnapshot delete $PROJECT/$vol2/${snap2_label}
    sleep 30
    run volume delete $PROJECT/$vol2 --wait
    run blockconsistencygroup delete $consistency_group
    echo "**** Done unity"
}

unity_cos_setup()
{
    secho "setting up Unity Virtual Pool"

    run cos create file $COS_UNITY                               \
         --description 'Virtual-Pool-for-unity-file' true \
                         --protocols NFS CIFS --max_snapshots 10    \
                         --provisionType 'Thin' \
                         --neighborhoods $NH
    ROOT_TENANT=`tenant root|tail -1`
    run cos allow $COS_UNITY file $ROOT_TENANT

    
    run cos create block $COS_UNITYBLOCK_CG                            \
	--description 'Virtual-Pool-for-unity-block-cg' true         \
                         --protocols FC                   \
                         --numpaths 2 \
                         --max_snapshots 10 \
                         --system_type unity \
                         --provisionType 'Thin' \
                         --expandable true \
                         --neighborhoods $NH \
                         --multiVolumeConsistency 
}

${SS}_setup
if [ $DO_SETUP_ONLY -eq 0 ]; then
    ${SS}_tests
fi

if [ $WS_SETUP_MODE -eq 1 ] ; then
    output_conf
fi

_success
